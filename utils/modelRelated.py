from typing import Dict, List, Optional, Any, TypedDict, Annotated
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage
import os
import time
import random
from pathlib import Path
import base64
from openai import RateLimitError
import requests

from utils.screen_shot import ExcelTableScreenshot


def _handle_rate_limit_with_backoff(func, max_retries: int = 6, base_delay: float = 1.0, max_delay: float = 60.0, silent_mode: bool = False):
    """
    Handle rate limit errors with exponential backoff retry logic.
    
    Args:
        func: Function to execute with retry logic
        max_retries: Maximum number of retry attempts
        base_delay: Base delay in seconds for exponential backoff
        max_delay: Maximum delay in seconds
        silent_mode: Whether to suppress logging output
        
    Returns:
        Function result on success
        
    Raises:
        Exception: Re-raises the last exception if all retries failed
    """
    last_exception = None
    
    for attempt in range(max_retries + 1):
        try:
            return func()
        except Exception as e:
            last_exception = e
            
            # Check if this is a rate limit error
            is_rate_limit_error = False
            retry_after = None
            
            # Handle different types of rate limit errors
            if hasattr(e, 'status_code') and e.status_code == 429:
                is_rate_limit_error = True
                # Try to extract retry-after header
                if hasattr(e, 'response') and hasattr(e.response, 'headers'):
                    retry_after = e.response.headers.get('retry-after')
            elif 'rate limit' in str(e).lower() or '429' in str(e) or 'too many requests' in str(e).lower():
                is_rate_limit_error = True
            elif isinstance(e, RateLimitError):
                is_rate_limit_error = True
                
            if not is_rate_limit_error or attempt >= max_retries:
                # Not a rate limit error or max retries reached
                break
                
            # Calculate delay with exponential backoff
            if retry_after:
                try:
                    delay = float(retry_after)
                    if not silent_mode:
                        print(f"â³ Rate limit hit, server requested {delay}s wait (attempt {attempt + 1}/{max_retries + 1})")
                except (ValueError, TypeError):
                    delay = min(base_delay * (2 ** attempt), max_delay)
            else:
                # Exponential backoff with jitter
                delay = min(base_delay * (2 ** attempt), max_delay)
                # Add jitter to prevent thundering herd
                delay += random.uniform(0, delay * 0.1)
                
            if not silent_mode:
                print(f"â³ Rate limit detected, waiting {delay:.1f}s before retry (attempt {attempt + 1}/{max_retries + 1})")
                
            time.sleep(delay)
    
    # All retries failed, raise the last exception
    if not silent_mode:
        print(f"âŒ All {max_retries + 1} attempts failed due to rate limiting")
    raise last_exception


def invoke_model(model_name : str, messages : List[BaseMessage], temperature: float = 0.2, silent_mode: bool = False) -> str:
    """è°ƒç”¨å¤§æ¨¡å‹ with automatic rate limit retry"""
    if not silent_mode:
        print(f"ğŸš€ å¼€å§‹è°ƒç”¨LLM: {model_name} (temperature={temperature})")
    
    def _make_api_call():
        start_time = time.time()
        
        if model_name.startswith("gpt-"):  # ChatGPT ç³»åˆ—æ¨¡å‹
            if not silent_mode:
                print("ğŸ” ä½¿ç”¨ OpenAI ChatGPT æ¨¡å‹")
            base_url = "https://api.openai.com/v1"
            api_key = os.getenv("OPENAI_API_KEY")
        else:  # å…¶ä»–æ¨¡å‹ï¼Œä¾‹å¦‚ deepseek, siliconflow...
            if not silent_mode:
                print("ğŸ” ä½¿ç”¨ SiliconFlow æ¨¡å‹")
            base_url = "https://api.siliconflow.cn/v1"
            api_key = os.getenv("SILICONFLOW_API_KEY")
        
        llm = ChatOpenAI(
            model = model_name,
            api_key=api_key, 
            base_url=base_url,
            streaming=not silent_mode,  # Disable streaming in silent mode
            temperature=temperature,
            timeout=200  # network timeout
        )

        full_response = ""
        total_tokens_used = {"input": 0, "output": 0, "total": 0}
        
        if silent_mode:
            # Silent mode: use invoke instead of stream
            response = llm.invoke(messages)
            full_response = response.content
            
            # Extract token usage from response
            if hasattr(response, 'usage_metadata') and response.usage_metadata:
                usage = response.usage_metadata
                total_tokens_used["input"] = usage.get('input_tokens', 0)
                total_tokens_used["output"] = usage.get('output_tokens', 0)
                total_tokens_used["total"] = usage.get('total_tokens', 0)
        else:
            # Normal mode: use streaming
            for chunk in llm.stream(messages):
                chunk_content = chunk.content
                print(chunk_content, end="", flush=True)
                full_response += chunk_content
                
                # Extract token usage if available in chunk
                if hasattr(chunk, 'usage_metadata') and chunk.usage_metadata:
                    usage = chunk.usage_metadata
                    total_tokens_used["input"] = usage.get('input_tokens', 0)
                    total_tokens_used["output"] = usage.get('output_tokens', 0)
                    total_tokens_used["total"] = usage.get('total_tokens', 0)
                
        end_time = time.time()
        execution_time = end_time - start_time
        
        # Print timing and token usage only if not in silent mode
        if not silent_mode:
            print(f"\nâ±ï¸ LLMè°ƒç”¨å®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
            if total_tokens_used["total"] > 0:
                print(f"ğŸ“Š Tokenä½¿ç”¨: è¾“å…¥={total_tokens_used['input']:,} | è¾“å‡º={total_tokens_used['output']:,} | æ€»è®¡={total_tokens_used['total']:,}")
        
        return full_response
    
    # Use rate limit retry wrapper
    try:
        return _handle_rate_limit_with_backoff(_make_api_call, silent_mode=silent_mode)
    except Exception as e:
        if not silent_mode:
            print(f"\nâŒ LLMè°ƒç”¨æœ€ç»ˆå¤±è´¥ï¼Œé”™è¯¯: {e}")
        raise


def invoke_model_with_tools(model_name : str, messages : List[BaseMessage], tools : List[str], temperature: float = 0.2) -> Any:
    """è°ƒç”¨å¤§æ¨¡å‹å¹¶ä½¿ç”¨å·¥å…· with automatic rate limit retry"""
    print(f"ğŸš€ å¼€å§‹è°ƒç”¨LLM(å¸¦å·¥å…·): {model_name} (temperature={temperature})")
    
    def _make_api_call_with_tools():
        start_time = time.time()
        
        if model_name.startswith("gpt-"):  # ChatGPT ç³»åˆ—æ¨¡å‹
            print("ğŸ” ä½¿ç”¨ OpenAI ChatGPT æ¨¡å‹")
            base_url = "https://api.openai.com/v1"
            api_key = os.getenv("OPENAI_API_KEY")
        else:  # å…¶ä»–æ¨¡å‹ï¼Œä¾‹å¦‚ deepseek, siliconflow...
            print("ğŸ” ä½¿ç”¨ SiliconFlow æ¨¡å‹")
            base_url = "https://api.siliconflow.cn/v1"
            api_key = os.getenv("SILICONFLOW_API_KEY")

        llm = ChatOpenAI(
            model=model_name,
            api_key=api_key,
            base_url=base_url,
            streaming=False,
            temperature=temperature,
            timeout=200
        )
        
        # ç»‘å®šå·¥å…·åˆ°æ¨¡å‹
        llm_with_tools = llm.bind_tools(tools)
        
        print("ğŸ“¤ æ­£åœ¨è°ƒç”¨LLM...")
        
        response = llm_with_tools.invoke(messages)
        
        print("ğŸ“¥ LLMå“åº”æ¥æ”¶å®Œæˆ")
        
        # æ‰“å°å“åº”å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
        if response.content:
            print(f"\nğŸ’¬ LLMå›å¤å†…å®¹:")
            print(response.content)
        
        # Extract token usage information
        token_usage = {"input": 0, "output": 0, "total": 0, "reasoning": 0}
        if hasattr(response, 'usage_metadata') and response.usage_metadata:
            usage = response.usage_metadata
            token_usage["input"] = usage.get('input_tokens', 0)
            token_usage["output"] = usage.get('output_tokens', 0)
            token_usage["total"] = usage.get('total_tokens', 0)
            
            # Check for reasoning tokens (for reasoning models like Qwen3-32B)
            if 'output_token_details' in usage and usage['output_token_details']:
                token_usage["reasoning"] = usage['output_token_details'].get('reasoning', 0)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        if hasattr(response, 'tool_calls') and response.tool_calls:
            print(f"\nğŸ”§ æ£€æµ‹åˆ° {len(response.tool_calls)} ä¸ªå·¥å…·è°ƒç”¨:")
            
            # æ‰“å°æ¯ä¸ªå·¥å…·è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯
            for i, tool_call in enumerate(response.tool_calls):
                print(f"\nğŸ“‹ å·¥å…·è°ƒç”¨ {i+1}:")
                print(f"   ğŸ”§ å·¥å…·åç§°: {tool_call.get('name', 'unknown')}")
                
                # æå–å·¥å…·å‚æ•°
                args = tool_call.get('args', {})
                print(f"   ğŸ“ å‚æ•°: {args}")
                
                # å¦‚æœæ˜¯ç”¨æˆ·äº¤äº’å·¥å…·ï¼Œç‰¹åˆ«æ˜¾ç¤ºé—®é¢˜
                if tool_call.get('name') == 'request_user_clarification':
                    question = args.get('question', '')
                    context = args.get('context', '')
                    if question:
                        print(f"\nğŸ’¬ â­ ç”¨æˆ·é—®é¢˜: {question}")
                        if context:
                            print(f"ğŸ“– ä¸Šä¸‹æ–‡: {context}")
                elif tool_call.get('name') == '_collect_user_input':
                    print(f"\nğŸ”„ å°†æ”¶é›†ç”¨æˆ·è¾“å…¥ä¿¡æ¯")
                    session_id = args.get('session_id', '')
                    if session_id:
                        print(f"ğŸ“‹ ä¼šè¯ID: {session_id}")
            
            end_time = time.time()
            execution_time = end_time - start_time
            print(f"\nâ±ï¸ LLMè°ƒç”¨å®Œæˆ(å¸¦å·¥å…·è°ƒç”¨)ï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
        else:
            end_time = time.time()
            execution_time = end_time - start_time
            print(f"\nâ±ï¸ LLMè°ƒç”¨å®Œæˆ(æ— å·¥å…·è°ƒç”¨)ï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
        
        # Print token usage information
        if token_usage["total"] > 0:
            print(f"ğŸ“Š Tokenä½¿ç”¨: è¾“å…¥={token_usage['input']:,} | è¾“å‡º={token_usage['output']:,} | æ€»è®¡={token_usage['total']:,}")
            if token_usage["reasoning"] > 0:
                print(f"ğŸ§  æ¨ç†Token: {token_usage['reasoning']:,} (å†…éƒ¨æ¨ç†è¿‡ç¨‹)")
                visible_output = token_usage["output"] - token_usage["reasoning"]
                print(f"ğŸ‘€ å¯è§è¾“å‡ºToken: {visible_output:,}")
        else:
            print("âš ï¸ æœªèƒ½è·å–Tokenä½¿ç”¨ä¿¡æ¯")
        
        # è¿”å›å®Œæ•´å“åº”ä»¥ä¾¿è°ƒç”¨è€…å¤„ç†
        return response
    
    # Use rate limit retry wrapper
    try:
        return _handle_rate_limit_with_backoff(_make_api_call_with_tools, silent_mode=False)
    except Exception as e:
        print(f"\nâŒ LLMè°ƒç”¨æœ€ç»ˆå¤±è´¥ï¼Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        raise


def invoke_model_with_screenshot(model_name : str, file_path : str, temperature: float = 0.2) -> Any:
    """è°ƒç”¨å¤§æ¨¡å‹å¹¶ä½¿ç”¨æˆªå›¾ with automatic rate limit retry"""
    print(f"ğŸš€ å¼€å§‹è°ƒç”¨LLM(å¸¦æˆªå›¾): {model_name} (temperature={temperature})")

    path = Path(file_path)
    screen_shot_path = path.with_suffix(".png")

    file_name = path.name

    excelTableScreenshot = ExcelTableScreenshot()
    excelTableScreenshot.take_screenshot(file_path, screen_shot_path)

    with open(screen_shot_path, "rb") as image_file:
        image_data = image_file.read()
        image_base64 = base64.b64encode(image_data).decode("utf-8")

    human_message = HumanMessage(content=[
    {
        "type": "text",
        "text": "è¯·è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—"
    },
    {
        "type": "image_url",
        "image_url": {
            "url": f"data:image/png;base64,{image_base64}"
        }
    }
    ])

    system_message = SystemMessage(content=f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è¡¨æ ¼ç»“æ„åˆ†æä¸“å®¶ï¼Œæ“…é•¿ä»å¤æ‚çš„ Excel æˆ– HTML è¡¨æ ¼ä¸­æå–å®Œæ•´çš„å¤šçº§è¡¨å¤´ç»“æ„ï¼Œå¹¶ç»“åˆæ•°æ®å†…å®¹è¾…åŠ©ç†è§£å­—æ®µå«ä¹‰ã€å±‚çº§å’Œåˆ†ç±»æ±‡æ€»å…³ç³»ã€‚

è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„è¡¨æ ¼ï¼Œå®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

ã€ä»»åŠ¡ç›®æ ‡ã€‘

1. æå–å®Œæ•´çš„è¡¨å¤´å±‚çº§ç»“æ„ï¼š
   - ä»è¡¨æ ¼ä¸»æ ‡é¢˜å¼€å§‹ï¼Œé€å±‚æå–æ‰€æœ‰åˆ†ç±»è¡¨å¤´ã€å­—æ®µè¡¨å¤´ï¼›
   - ç»“åˆå®é™…æ•°æ®ï¼Œè¾…åŠ©åˆ¤æ–­å­—æ®µçš„å«ä¹‰ã€åˆ†ç±»ã€å±‚çº§å½’å±ï¼Œç¡®ä¿ç»“æ„ç†è§£å‡†ç¡®ï¼›
   - ä½¿ç”¨åµŒå¥—çš„ key-value ç»“æ„ï¼Œè¡¨è¾¾è¡¨å¤´çš„å±‚çº§å…³ç³»ï¼›
   - æ¯ä¸€çº§è¡¨å¤´éƒ½åº”æ¸…æ™°åæ˜ å…¶å­çº§å­—æ®µæˆ–å­åˆ†ç±»ï¼Œé¿å…é—æ¼æˆ–è¯¯åˆ†ç±»ã€‚

2. é‡‡ç”¨ã€Œå€¼ / åˆ†è§£ / è§„åˆ™ã€ç»“æ„è¯†åˆ«åˆ†ç±»æ±‡æ€»å…³ç³»ï¼š
   - å¦‚æœæŸä¸ªçˆ¶çº§å­—æ®µ**è‡ªèº«æœ‰æ•°æ®**ï¼ŒåŒæ—¶åˆåŒ…å«å¤šä¸ªå­å­—æ®µï¼ˆå¦‚"ä¿éšœäººæ•°"ã€"é¢†å–é‡‘é¢"ï¼‰ï¼Œå¿…é¡»é‡‡ç”¨ä»¥ä¸‹æ ¼å¼ï¼š

   {{
     "å­—æ®µå": {{
       "å€¼": [],        // è¡¨ç¤ºè¯¥å­—æ®µè‡ªèº«çš„æ•°æ®ï¼ˆå³åŸè¡¨æ ¼ä¸­çš„å•å…ƒæ ¼æ•°æ®ï¼‰
       "åˆ†è§£": {{        // è¡¨ç¤ºè¯¥å­—æ®µä¸‹çš„å­åˆ†ç±»æˆ–å­å­—æ®µ
         "å­å­—æ®µ1": [],
         "å­å­—æ®µ2": []
       }},
       "è§„åˆ™": "å­å­—æ®µ1 + å­å­—æ®µ2"  // å¦‚æœè¯¥å­—æ®µçš„å€¼ç­‰äºå­å­—æ®µçš„åŠ æ€»æˆ–æœ‰å…¶ä»–è§„åˆ™ï¼Œè¯·è‡ªåŠ¨æ¨ç†å¹¶å†™æ˜è§„åˆ™ã€‚å¦‚æœæ— è§„å¾‹å¯å¾ªï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
     }}
   }}

   - å¦‚æœçˆ¶çº§å­—æ®µåªæ˜¯åˆ†ç±»ï¼ˆè‡ªèº«æ— æ•°æ®ï¼‰ï¼Œåªè¾“å‡º "åˆ†è§£"ï¼›
   - å¦‚æœæŸä¸ªå­—æ®µæ—¢æ²¡æœ‰å­åˆ†ç±»ï¼Œä¹Ÿæ²¡æœ‰å­å­—æ®µæ‹†åˆ†ï¼Œç›´æ¥è¾“å‡ºä¸ºï¼š

   {{
     "å­—æ®µå": []
   }}

3. è¾…åŠ©æ¨ç†å­—æ®µçš„è®¡ç®—è§„åˆ™ï¼š
   - æ£€æŸ¥çˆ¶çº§å­—æ®µçš„å€¼ä¸å…¶å­å­—æ®µæ•°æ®çš„å…³ç³»ï¼Œè‡ªåŠ¨æ¨ç†æ˜¯å¦æœ‰åŠ æ³•ã€å‡æ³•ã€æ¯”ä¾‹ã€ç‰¹æ®Šé€»è¾‘ç­‰ï¼›
   - å¦‚æœå‘ç°è§„å¾‹ï¼Œå®Œæ•´è¾“å‡ºå…¬å¼æè¿°ï¼ˆå¦‚ï¼š"å­å­—æ®µ1 + å­å­—æ®µ2" æˆ– "(å­å­—æ®µ1 + å­å­—æ®µ2) Ã— 1.2"ï¼‰ï¼›
   - å¦‚æœçˆ¶çº§å­—æ®µçš„å€¼å’Œå­å­—æ®µæ•°æ®æ— æ˜æ˜¾å…³ç³»ï¼Œè¾“å‡º "è§„åˆ™": ""ã€‚

4. è¾…åŠ©åˆ¤æ–­å­—æ®µå«ä¹‰ï¼š
   - ç»“åˆæ•°æ®å†…å®¹è¾…åŠ©åˆ¤æ–­å­—æ®µç”¨é€”ï¼Œé¿å…ä»…ä¾èµ–è¡¨å¤´æ–‡å­—è¡¨é¢æ‹†åˆ†ï¼›
   - è¯†åˆ«é‡å¤å­—æ®µã€å¹¶åˆ—å­—æ®µã€åˆå¹¶å•å…ƒæ ¼å¸¦æ¥çš„ç»“æ„å±‚çº§ï¼›
   - å¯¹äºå­˜åœ¨æ•°æ®ä½†è¡¨å¤´æè¿°æ¨¡ç³Šçš„æƒ…å†µï¼Œå°½é‡æ ¹æ®æ•°æ®åˆ¤æ–­å…¶çœŸå®åˆ†ç±»ã€‚

ã€è¾“å‡ºæ ¼å¼è¦æ±‚ã€‘

- ä¸¥æ ¼è¾“å‡ºä¸º**æ ‡å‡† JSON æ ¼å¼**ï¼Œä¸èƒ½æœ‰ markdownã€ä»£ç å—æ ‡è®°æˆ–å…¶ä»–å¤šä½™æ–‡å­—ï¼›
- åªè¾“å‡ºè¡¨å¤´ç»“æ„ï¼Œ**ä¸è¦è¾“å‡ºè¡¨æ ¼æ€»ç»“ã€æè¿°ã€ç”¨é€”è¯´æ˜æˆ–æ•°æ®æ ·ä¾‹**ï¼›
- JSON çš„æ ¹é”®åå¿…é¡»ä¸º {file_name}ï¼ˆä¸¥æ ¼ä¿ç•™ï¼Œä¸èƒ½æ›´æ”¹ï¼‰ï¼›
- æ¯ä¸€å±‚çº§éƒ½ä»¥å¯¹è±¡å½¢å¼æè¿°ï¼Œéµå¾ªä»¥ä¸‹ç»“æ„ï¼š

ã€è¾“å‡ºç¤ºä¾‹ã€‘

{{
  "{file_name}": {{
    "è¡¨æ ¼ç»“æ„": {{
      "åºå·": [],
      "æˆ·ä¸»å§“å": [],
      "ä½ä¿è¯å·": [],
      "èº«ä»½è¯å·ç ": [],
      "ä¿éšœäººæ•°": {{
        "å€¼": [],
        "åˆ†è§£": {{
          "é‡ç‚¹ä¿éšœäººæ•°": [],
          "æ®‹ç–¾äººæ•°": []
        }},
        "è§„åˆ™": "é‡ç‚¹ä¿éšœäººæ•° + æ®‹ç–¾äººæ•°"
      }},
      "é¢†å–é‡‘é¢": {{
        "å€¼": [],
        "åˆ†è§£": {{
          "å®¶åº­è¡¥å·®": [],
          "é‡ç‚¹æ•‘åŠ©60å…ƒ": [],
          "é‡ç‚¹æ•‘åŠ©100å…ƒ": [],
          "æ®‹ç–¾äººæ•‘åŠ©": []
        }},
        "è§„åˆ™": "å®¶åº­è¡¥å·® + é‡ç‚¹æ•‘åŠ©60å…ƒ + é‡ç‚¹æ•‘åŠ©100å…ƒ + æ®‹ç–¾äººæ•‘åŠ©"
      }},
      "é¢†æ¬¾äººç­¾å­—(ç« )": [],
      "é¢†æ¬¾æ—¶é—´": []
    }}
  }}
}}

ã€ç‰¹åˆ«æ³¨æ„ã€‘

- æ‰€æœ‰è¾“å‡ºå¿…é¡»ä¸ºä¸¥æ ¼çš„ JSON ç»“æ„ï¼Œä½†ä¸è¦å°†è¾“å‡ºåŒ…è£¹åœ¨ ```json å’Œ ``` ä¸­ï¼›
- ä¸å…è®¸è¾“å‡ºè¡¨æ ¼æ€»ç»“ã€æè¿°ã€å…ƒæ•°æ®æˆ–ç”¨é€”è¯´æ˜ï¼›
- ä¸å…è®¸æœ‰ markdownã€ä»£ç å—æ ‡è®°æˆ–å¤šä½™çš„æ ¼å¼ç¬¦å·ï¼›
- ä¿æŒå±‚çº§ç»“æ„æ¸…æ™°ï¼Œå®Œæ•´ä¿ç•™æ‰€æœ‰åˆ†ç±»è¡¨å¤´ã€å­è¡¨å¤´å’Œå­—æ®µè¡¨å¤´ï¼›
- **çˆ¶çº§å­—æ®µæœ‰æ•°æ®æ—¶ï¼Œå¿…é¡»é‡‡ç”¨ "å€¼ / åˆ†è§£ / è§„åˆ™" ç»“æ„ï¼Œç¡®ä¿æ•°æ®ä¸åˆ†ç±»ä¿¡æ¯ã€è®¡ç®—å…³ç³»éƒ½ä¿ç•™**ï¼›
- å¿…é¡»ç»“åˆæ•°æ®è¾…åŠ©åˆ¤æ–­å­—æ®µå«ä¹‰ï¼Œç¡®ä¿åˆ†ç±»ã€å±‚çº§å’Œæ±‡æ€»é€»è¾‘å‡†ç¡®ï¼›
- è§„åˆ™å­—æ®µéœ€è‡ªåŠ¨æ¨ç†å­å­—æ®µä¸çˆ¶å­—æ®µä¹‹é—´çš„è®¡ç®—å…³ç³»ï¼Œæ‰¾ä¸åˆ°è§„åˆ™æ—¶è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
""")


    messages = [system_message, human_message]

    # Use the rate limit retry invoke_model (which already has retry logic)
    response = invoke_model(model_name, messages)

    return response
    
    

def invoke_embedding_model(model_name: str, texts: List[str], silent_mode: bool = False) -> List[List[float]]:
    """è°ƒç”¨åµŒå…¥æ¨¡å‹ with automatic rate limit retry"""
    if not silent_mode:
        print(f"ğŸš€ å¼€å§‹è°ƒç”¨åµŒå…¥æ¨¡å‹: {model_name}")
    
    def _make_embedding_call():
        start_time = time.time()
        
        if not silent_mode:
            print("ğŸ” ä½¿ç”¨ SiliconFlow åŸç”ŸåµŒå…¥API")
        
        # SiliconFlow native API
        api_key = os.getenv("SILICONFLOW_API_KEY")
        url = "https://api.siliconflow.cn/v1/embeddings"
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        # Prepare payload
        payload = {
            "model": model_name,  # Use exact model name
            "input": texts,
            "encoding_format": "float"
        }
        
        # Make API call
        response = requests.post(url, headers=headers, json=payload, timeout=200)
        
        # Check for errors
        if response.status_code != 200:
            error_text = response.text
            print(f"âŒ API Error {response.status_code}: {error_text}")
            raise Exception(f"SiliconFlow API Error {response.status_code}: {error_text}")
        
        # Parse response
        result = response.json()
        embeddings = [item["embedding"] for item in result["data"]]
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        if not silent_mode:
            print(f"\nâ±ï¸ åµŒå…¥æ¨¡å‹è°ƒç”¨å®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
            if "usage" in result:
                print(f"ğŸ“Š Tokenä½¿ç”¨: æ€»è®¡={result['usage'].get('total_tokens', 'N/A'):,}")
            print(f"ğŸ”¢ åµŒå…¥ç»´åº¦: {len(embeddings[0])}")
            print(f"ğŸ“‹ å¤„ç†æ–‡æœ¬æ•°é‡: {len(embeddings)}")
        
        return embeddings
    
    # Use your existing rate limit retry wrapper
    try:
        return _handle_rate_limit_with_backoff(_make_embedding_call, silent_mode=silent_mode)
    except Exception as e:
        if not silent_mode:
            print(f"\nâŒ åµŒå…¥æ¨¡å‹è°ƒç”¨æœ€ç»ˆå¤±è´¥ï¼Œé”™è¯¯: {e}")
        raise
