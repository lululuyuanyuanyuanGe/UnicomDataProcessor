from __future__ import annotations
from bs4 import BeautifulSoup
from pathlib import Path
import re
import os
import json
from pathlib import Path
import subprocess
import chardet
from typing import Union, List, Dict
import pandas as pd
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

from utils.modelRelated import invoke_model

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage

def detect_and_process_file_paths(user_input: str) -> list:
    """æ£€æµ‹ç”¨æˆ·è¾“å…¥ä¸­çš„æ–‡ä»¶è·¯å¾„å¹¶éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œè¿”å›ç»“æœä¸ºç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„ç»„æˆçš„æ•°åˆ—"""
    file_paths = []
    processed_paths = set()  # Track already processed paths to avoid duplicates
    
    # æ”¹è¿›çš„æ–‡ä»¶è·¯å¾„æ£€æµ‹æ¨¡å¼ï¼Œæ”¯æŒä¸­æ–‡å­—ç¬¦
    # Windowsè·¯å¾„æ¨¡å¼ (C:\path\file.ext æˆ– D:\path\file.ext) - æ”¯æŒä¸­æ–‡å­—ç¬¦
    windows_pattern = r'[A-Za-z]:[\\\\/](?:[^\\\\/\s\n\r]+[\\\\/])*[^\\\\/\s\n\r]+\.\w+'
    # ç›¸å¯¹è·¯å¾„æ¨¡å¼ (./path/file.ext æˆ– ../path/file.ext) - æ”¯æŒä¸­æ–‡å­—ç¬¦
    relative_pattern = r'\.{1,2}[\\\\/](?:[^\\\\/\s\n\r]+[\\\\/])*[^\\\\/\s\n\r]+\.\w+'
    # ç®€å•æ–‡ä»¶åæ¨¡å¼ (filename.ext) - æ”¯æŒä¸­æ–‡å­—ç¬¦
    filename_pattern = r'\b[a-zA-Z0-9_\u4e00-\u9fff\-\(\)ï¼ˆï¼‰]+\.[a-zA-Z0-9]+\b'
    
    patterns = [windows_pattern, relative_pattern, filename_pattern]
    
    # Run the absolute path pattern first
    for match in re.findall(patterns[0], user_input):
        if match in processed_paths:
            continue
        processed_paths.add(match)
        _log_existence(match, file_paths)

    # Run the relative path pattern
    for match in re.findall(patterns[1], user_input):
        if match in processed_paths:
            continue
        processed_paths.add(match)
        _log_existence(match, file_paths)
        
    # Run the filename pattern if we didn't find any files
    if not file_paths:
        for match in re.findall(patterns[2], user_input):
            if match in processed_paths:
                continue
            processed_paths.add(match)
            _log_existence(match, file_paths)

    return file_paths


# -- å°å·¥å…·å‡½æ•° ------------------------------------------------------------
def _log_existence(path: str, container: list):
    if os.path.exists(path):
        container.append(path)
        print(f"âœ… æ£€æµ‹åˆ°æ–‡ä»¶: {path}")
    else:
        print(f"âš ï¸ æ–‡ä»¶è·¯å¾„æ— æ•ˆæˆ–æ–‡ä»¶ä¸å­˜åœ¨: {path}")


def process_file_to_text(file_path: str | Path) -> str | None:
    """
    Efficiently process a file to readable text content in memory.
    
    This function does: 1 read â†’ process in memory â†’ return text
    Instead of: read â†’ write temp file â†’ read temp file â†’ write final file
    
    Returns:
        str: The processed text content, or None if processing failed
    """
    source_path = Path(file_path)
    file_extension = source_path.suffix.lower()
    
    # Define file type categories
    spreadsheet_extensions = {'.xlsx', '.xls', '.xlsm', '.ods', '.csv'}
    text_extensions = {'.txt', '.md', '.json', '.xml', '.html', '.htm', '.py', '.js', '.css', '.sql', '.log'}
    document_extensions = {'.docx', '.doc', '.pptx', '.ppt'}
    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.webp', '.svg'}
    
    try:
        # Handle spreadsheet files
        if file_extension in spreadsheet_extensions:
            return _process_spreadsheet(source_path)
        
        # Handle document files (DOCX, DOC, etc.)
        elif file_extension in document_extensions:
            return _process_doc_file(source_path)
        
        # Handle plain text files
        elif file_extension in text_extensions:
            return _read_text_auto(source_path)
        
        # Handle image files - return metadata since we can't convert to text
        elif file_extension in image_extensions:
            return f"Image file: {source_path.name}\nFile size: {source_path.stat().st_size} bytes\nFormat: {file_extension}"
        
        # Handle other file types
        else:
            # Try to detect if it's a text file by MIME type
            import mimetypes
            mime_type, _ = mimetypes.guess_type(str(source_path))
            
            if mime_type and mime_type.startswith('text/'):
                return _read_text_auto(source_path)
            else:
                # For binary files, return metadata
                return f"Binary file: {source_path.name}\nFile size: {source_path.stat().st_size} bytes\nType: {mime_type or 'unknown'}"
                
    except Exception as e:
        print(f"âŒ Error processing file {file_path}: {e}")
        return None


# Global lock for LibreOffice operations to prevent concurrent access issues
import threading
_libreoffice_lock = threading.Lock()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ private helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def _read_text_auto(path: Path) -> str:
    """Best-effort text loader with encoding detection."""
    data = path.read_bytes()
    for enc in ("utf-8", "utf-8-sig", "gb18030", "gbk", "big5"):
        try:
            return data.decode(enc)
        except UnicodeDecodeError:
            continue
    if chardet:
        enc = chardet.detect(data).get("encoding")
        if enc:
            try:
                return data.decode(enc)
            except UnicodeDecodeError:
                pass
    return data.decode("utf-8", errors="replace")

def _process_spreadsheet():
    # This function should use pandas that convertts the excel to html or csv format, then save them inside the txt file with the same
    # file name but txt suffix under the temp folder
    pass

def _process_doc_file():
    # We will use mamoth library to extratct the content of the word files, and save it in the txt format under the temp folder
    pass

def _process_pdf_file():
    # We will support the PDF files in later versions, for now just safely ignore the pdf files
    pass

def move_template_files_safely(processed_template_file: str, original_files_list: list[str], dest_dir_name: str = "template_files") -> dict[str, str]:
    """
    Safely move both processed and original template files to the template_files directory.
    
    This function handles moving both the processed template file (.txt) and its corresponding
    original file to the template_files folder, with proper error handling and logging.
    
    Args:
        processed_template_file: Path to the processed template file (.txt)
        original_files_list: List of original file paths to search for the corresponding original file
        dest_dir_name: Name of the destination directory under conversations/files/user_uploaded_files/
        
    Returns:
        dict: {
            "processed_template_path": str,  # Path to moved processed template file
            "original_template_path": str    # Path to moved original template file (or empty if not found)
        }
    """
    import shutil
    
    try:
        # Create destination directories
        dest_dir = Path(f"conversations/files/user_uploaded_files/{dest_dir_name}")
        dest_dir.mkdir(parents=True, exist_ok=True)
        
        # Create original_file subdirectory within template_files
        original_dest_dir = dest_dir / "original_file"
        original_dest_dir.mkdir(parents=True, exist_ok=True)
        
        processed_template_path = Path(processed_template_file)
        result = {
            "processed_template_path": "",
            "original_template_path": ""
        }
        
        print(f"ğŸ“ æ­£åœ¨ç§»åŠ¨æ¨¡æ¿æ–‡ä»¶: {processed_template_path.name}")
        
        # Move the processed template file
        processed_target_path = dest_dir / processed_template_path.name
        
        # Handle existing processed file
        if processed_target_path.exists():
            print(f"âš ï¸ å¤„ç†æ¨¡æ¿æ–‡ä»¶å·²å­˜åœ¨: {processed_target_path.name}")
            try:
                processed_target_path.unlink()
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ—§çš„å¤„ç†æ¨¡æ¿æ–‡ä»¶: {processed_target_path.name}")
            except Exception as delete_error:
                print(f"âŒ åˆ é™¤æ—§çš„å¤„ç†æ¨¡æ¿æ–‡ä»¶å¤±è´¥: {delete_error}")
                result["processed_template_path"] = processed_template_file
                return result
        
        # Move processed template file
        try:
            shutil.move(str(processed_template_path), str(processed_target_path))
            result["processed_template_path"] = str(processed_target_path)
            print(f"âœ… å¤„ç†æ¨¡æ¿æ–‡ä»¶å·²ç§»åŠ¨åˆ°: {processed_target_path}")
        except Exception as move_error:
            print(f"âŒ ç§»åŠ¨å¤„ç†æ¨¡æ¿æ–‡ä»¶å¤±è´¥: {move_error}")
            result["processed_template_path"] = processed_template_file
            return result
        
        # Find and move the corresponding original file
        template_file_stem = processed_template_path.stem
        original_file_found = False
        
        print(f"ğŸ” æ­£åœ¨å¯»æ‰¾å¯¹åº”çš„åŸå§‹æ¨¡æ¿æ–‡ä»¶: {template_file_stem}")
        
        for original_file in original_files_list:
            original_file_path = Path(original_file)
            if original_file_path.stem == template_file_stem:
                print(f"ğŸ“‹ æ‰¾åˆ°å¯¹åº”çš„åŸå§‹æ–‡ä»¶: {original_file_path.name}")
                
                # Move the original file to the original_file subdirectory
                original_target_path = original_dest_dir / original_file_path.name
                
                # Handle existing original file
                if original_target_path.exists():
                    print(f"âš ï¸ åŸå§‹æ¨¡æ¿æ–‡ä»¶å·²å­˜åœ¨: {original_target_path.name}")
                    try:
                        original_target_path.unlink()
                        print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ—§çš„åŸå§‹æ¨¡æ¿æ–‡ä»¶: {original_target_path.name}")
                    except Exception as delete_error:
                        print(f"âŒ åˆ é™¤æ—§çš„åŸå§‹æ¨¡æ¿æ–‡ä»¶å¤±è´¥: {delete_error}")
                        # Continue with moving even if deletion failed
                
                # Move original file
                try:
                    shutil.move(str(original_file_path), str(original_target_path))
                    result["original_template_path"] = str(original_target_path)
                    print(f"âœ… åŸå§‹æ¨¡æ¿æ–‡ä»¶å·²ç§»åŠ¨åˆ°: {original_target_path}")
                    original_file_found = True
                    break
                except Exception as move_error:
                    print(f"âŒ ç§»åŠ¨åŸå§‹æ¨¡æ¿æ–‡ä»¶å¤±è´¥: {move_error}")
                    # Continue searching for other matching files
        
        if not original_file_found:
            print(f"âš ï¸ æœªæ‰¾åˆ°å¯¹åº”çš„åŸå§‹æ¨¡æ¿æ–‡ä»¶: {template_file_stem}")
            result["original_template_path"] = ""
        
        return result
        
    except Exception as e:
        print(f"âŒ ç§»åŠ¨æ¨¡æ¿æ–‡ä»¶è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return {
            "processed_template_path": processed_template_file,
            "original_template_path": ""
        }

def convert_html_to_excel(html_file_path: str, output_dir: str = None) -> str:
    """
    Convert HTML file to Excel format using session-specific output directory
    
    Args:
        html_file_path: Path to the HTML file to convert
        output_dir: Output directory path (should be session-specific)
    
    Returns:
        str: Path to the converted Excel file
    """
    # Function implementation placeholder
    pass

def delete_files_from_staging_area(file_paths: list[str]) -> dict[str, list[str]]:
    """Delete irrelevant files from staging area.
    
    Args:
        file_paths: List of file paths to delete
        
    Returns:
        dict: {
            "deleted_files": list[str],  # Successfully deleted files
            "failed_deletes": list[str]  # Files that failed to delete
        }
    """
    from pathlib import Path
    
    deleted_files = []
    failed_deletes = []
    
    for file_path in file_paths:
        try:
            file_to_delete = Path(file_path)
            if file_to_delete.exists():
                file_to_delete.unlink()
                deleted_files.append(str(file_to_delete))
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ— å…³æ–‡ä»¶: {file_to_delete.name}")
            else:
                print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ é™¤: {file_path}")
        except Exception as e:
            failed_deletes.append(file_path)
            print(f"âŒ åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
    
    print(f"ğŸ“Š åˆ é™¤ç»“æœ: æˆåŠŸ {len(deleted_files)} ä¸ªï¼Œå¤±è´¥ {len(failed_deletes)} ä¸ª")
    
    return {
        "deleted_files": deleted_files,
        "failed_deletes": failed_deletes
    }

def analyze_single_file(file_path: str) -> tuple[str, str, str]:
    """Analyze a single file and return (file_path, classification, file_name)"""   
    try:
        source_path = Path(file_path)
        print(f"ğŸ” æ­£åœ¨åˆ†ææ–‡ä»¶: {source_path.name}")
        
        if not source_path.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return file_path, "irrelevant", source_path.name
        
        # Read file content for analysis
        file_content = source_path.read_text(encoding='utf-8')
        # Truncate content for analysis (to avoid token limits)
        analysis_content = file_content[:5000] if len(file_content) > 2000 else file_content
        
        # Create individual analysis prompt for this file
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªè¡¨æ ¼ç”Ÿæˆæ™ºèƒ½ä½“ï¼Œéœ€è¦åˆ†æç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶å†…å®¹æ˜¯ä¸æ˜¯ä¸€ä¸ªåŒ…å«æœ‰æ•ˆæ•°æ®é›†çš„è¡¨æ ¼æ–‡ä»¶ï¼Œæœ€ç›´è§‚çš„æ˜¯ç”¨æˆ·ä¸Šä¼ äº†ä¸€ä¸ªexcelè¡¨æ ¼ï¼Œå¹¶ä¸”å¹¶éæ¨¡æ¿è¡¨æ ¼ï¼Œé‡Œé¢æœ‰å…·ä½“çš„æ•°æ®ï¼Œæ­¤æ—¶å°†æ–‡ä»¶

        ä»”ç»†æ£€æŸ¥ä¸è¦æŠŠè¡¥å……æ–‡ä»¶é”™è¯¯åˆ’åˆ†ä¸ºæ¨¡æ¿æ–‡ä»¶åä¹‹äº¦ç„¶ï¼Œè¡¥å……æ–‡ä»¶é‡Œé¢æ˜¯æœ‰æ•°æ®çš„ï¼Œæ¨¡æ¿æ–‡ä»¶é‡Œé¢æ˜¯ç©ºçš„ï¼Œæˆ–è€…åªæœ‰ä¸€ä¸¤ä¸ªä¾‹å­æ•°æ®
        æ³¨æ„ï¼šæ‰€æœ‰æ–‡ä»¶å·²è½¬æ¢ä¸ºtxtæ ¼å¼ï¼Œè¡¨æ ¼ä»¥HTMLä»£ç å½¢å¼å‘ˆç°ï¼Œè¯·æ ¹æ®å†…å®¹è€Œéæ–‡ä»¶åæˆ–åç¼€åˆ¤æ–­ã€‚

        å½“å‰åˆ†ææ–‡ä»¶:
        æ–‡ä»¶å: {source_path.name}
        æ–‡ä»¶è·¯å¾„: {file_path}
        æ–‡ä»¶å†…å®¹:
        {analysis_content}

        è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼å›å¤ï¼Œåªè¿”å›è¿™ä¸€ä¸ªæ–‡ä»¶çš„åˆ†ç±»ç»“æœï¼ˆä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ï¼‰ï¼Œä¸è¦å°†è¿”å›å†…å®¹åŒ…è£¹åœ¨```json```ä¸­ï¼š
        {{
            "classification": "irrelevant" | "table"
        }}"""
        
        # Get LLM analysis for this file
        print("ğŸ“¤ æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œæ–‡ä»¶åˆ†ç±»...")
        analysis_response = invoke_model(model_name="deepseek-ai/DeepSeek-V3", messages=[SystemMessage(content=system_prompt)])

        # Parse JSON response for this file
        try:
            # Extract JSON from response
            response_content = analysis_response.strip()
            print(f"ğŸ“¥ LLMåˆ†ç±»å“åº”: {response_content}")
            
            # Remove markdown code blocks if present
            if response_content.startswith('```'):
                response_content = response_content.split('\n', 1)[1]
                response_content = response_content.rsplit('\n', 1)[0]
            
            file_classification = json.loads(response_content)
            classification_type = file_classification.get("classification", "irrelevant")
            
            print(f"âœ… æ–‡ä»¶ {source_path.name} åˆ†ç±»ä¸º: {classification_type}")
            return file_path, classification_type, source_path.name
            
        except json.JSONDecodeError as e:
            print(f"âŒ æ–‡ä»¶ {source_path.name} JSONè§£æé”™è¯¯: {e}")
            print(f"LLMå“åº”: {analysis_response}")
            # Fallback: mark as irrelevant for safety
            return file_path, "irrelevant", source_path.name
        
    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶å‡ºé”™ {file_path}: {e}")
        # Return irrelevant on error
        return file_path, "irrelevant", Path(file_path).name