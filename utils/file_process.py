from __future__ import annotations
from bs4 import BeautifulSoup
from pathlib import Path
import re
import os
import json
from pathlib import Path
import subprocess
import chardet
from typing import Union, List, Dict
import pandas as pd
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

from utils.modelRelated import invoke_model

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage

def detect_and_process_file_paths(user_input: str) -> list:
    """æ£€æµ‹ç”¨æˆ·è¾“å…¥ä¸­çš„æ–‡ä»¶è·¯å¾„å¹¶éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œè¿”å›ç»“æœä¸ºç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„ç»„æˆçš„æ•°åˆ—"""
    file_paths = []
    processed_paths = set()  # Track already processed paths to avoid duplicates
    
    # æ”¹è¿›çš„æ–‡ä»¶è·¯å¾„æ£€æµ‹æ¨¡å¼ï¼Œæ”¯æŒä¸­æ–‡å­—ç¬¦
    # Windowsè·¯å¾„æ¨¡å¼ (C:\path\file.ext æˆ– D:\path\file.ext) - æ”¯æŒä¸­æ–‡å­—ç¬¦
    windows_pattern = r'[A-Za-z]:[\\\\/](?:[^\\\\/\s\n\r]+[\\\\/])*[^\\\\/\s\n\r]+\.\w+'
    # ç›¸å¯¹è·¯å¾„æ¨¡å¼ (./path/file.ext æˆ– ../path/file.ext) - æ”¯æŒä¸­æ–‡å­—ç¬¦
    relative_pattern = r'\.{1,2}[\\\\/](?:[^\\\\/\s\n\r]+[\\\\/])*[^\\\\/\s\n\r]+\.\w+'
    # ç®€å•æ–‡ä»¶åæ¨¡å¼ (filename.ext) - æ”¯æŒä¸­æ–‡å­—ç¬¦
    filename_pattern = r'\b[a-zA-Z0-9_\u4e00-\u9fff\-\(\)ï¼ˆï¼‰]+\.[a-zA-Z0-9]+\b'
    
    patterns = [windows_pattern, relative_pattern, filename_pattern]
    
    # Run the absolute path pattern first
    for match in re.findall(patterns[0], user_input):
        if match in processed_paths:
            continue
        processed_paths.add(match)
        _log_existence(match, file_paths)

    # Run the relative path pattern
    for match in re.findall(patterns[1], user_input):
        if match in processed_paths:
            continue
        processed_paths.add(match)
        _log_existence(match, file_paths)
        
    # Run the filename pattern if we didn't find any files
    if not file_paths:
        for match in re.findall(patterns[2], user_input):
            if match in processed_paths:
                continue
            processed_paths.add(match)
            _log_existence(match, file_paths)

    return file_paths


# -- å°å·¥å…·å‡½æ•° ------------------------------------------------------------
def _log_existence(path: str, container: list):
    if os.path.exists(path):
        container.append(path)
        print(f"âœ… æ£€æµ‹åˆ°æ–‡ä»¶: {path}")
    else:
        print(f"âš ï¸ æ–‡ä»¶è·¯å¾„æ— æ•ˆæˆ–æ–‡ä»¶ä¸å­˜åœ¨: {path}")


def convert_2_markdown(file_path: str) -> str:
    """å°†Excelæ–‡ä»¶è½¬æ¢ä¸ºMarkdownæ ¼å¼å¹¶ä¿å­˜ä¸º.mdæ–‡ä»¶"""

    # è¯»å–Excelæ–‡ä»¶
    df = pd.read_excel(file_path)
    markdown_content = df.to_markdown(index=False)

    # æ„é€ æ–°çš„Markdownæ–‡ä»¶å
    original_name = Path(file_path).stem  # ä¸å¸¦æ‰©å±•å
    markdown_file_name = f"{original_name}.md"

    # ç›®æ ‡ä¿å­˜ç›®å½•
    markdown_folder = Path(r"D:\asianInfo\ExcelAssist\conversations\files\user_uploaded_md")
    markdown_folder.mkdir(parents=True, exist_ok=True)  # å¦‚æœä¸å­˜åœ¨å°±åˆ›å»º

    # å®Œæ•´è·¯å¾„
    markdown_file_path = markdown_folder / markdown_file_name

    # å†™å…¥æ–‡ä»¶
    with open(markdown_file_path, "w", encoding="utf-8") as f:
        f.write(markdown_content)

    return str(markdown_file_path)  # è¿”å›ä¿å­˜è·¯å¾„ä»¥ä¾¿åç»­ä½¿ç”¨
    


def save_original_file(source_path: Path, original_files_dir: Path) -> str:
    """
    Save the original file to the original_file subfolder.
    
    Args:
        source_path: Path to the source file
        original_files_dir: Path to the original_file directory

    Returns:
        str: Path to the saved original file, empty string if failed
    """
    import shutil
    
    try:
        if not source_path.exists():
            print(f"âŒ Source file not found: {source_path}")
            return ""
            
        print(f"ğŸ“ æ­£åœ¨ä¿å­˜åŸå§‹æ–‡ä»¶: {source_path.name}")
        
        # Create target path for original file
        original_file_path = original_files_dir / source_path.name
        
        # Handle duplicate original files by updating content
        if original_file_path.exists():
            print(f"âš ï¸ åŸå§‹æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ­£åœ¨æ›´æ–°: {source_path.name}")
            try:
                # Try to remove existing file
                original_file_path.unlink()
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ—§çš„åŸå§‹æ–‡ä»¶: {source_path.name}")
            except Exception as e:
                print(f"âŒ åˆ é™¤æ—§åŸå§‹æ–‡ä»¶å¤±è´¥: {e}")
                # Check for permission errors
                if "WinError 5" in str(e) or "Access is denied" in str(e) or "Permission denied" in str(e):
                    print(f"ğŸ’¡ æ–‡ä»¶ '{source_path.name}' å¯èƒ½è¢«å…¶ä»–åº”ç”¨ç¨‹åºé”å®š")
                    print(f"ğŸ“ è¯·å…³é—­ç›¸å…³åº”ç”¨ç¨‹åºåé‡è¯•ï¼Œæˆ–ä½¿ç”¨ä¸åŒçš„æ–‡ä»¶å")
                    return ""
                else:
                    print(f"âš ï¸ å…¶ä»–é”™è¯¯: {e}")
                    return ""
        
        # Copy the original file to the original_file subfolder
        try:
            shutil.copy2(source_path, original_file_path)
            print(f"ğŸ’¾ åŸå§‹æ–‡ä»¶å·²ä¿å­˜: {original_file_path}")
            return str(original_file_path)
        except Exception as e:
            print(f"âŒ ä¿å­˜åŸå§‹æ–‡ä»¶å¤±è´¥: {e}")
            # Check for permission errors
            if "WinError 5" in str(e) or "Access is denied" in str(e) or "Permission denied" in str(e):
                print(f"ğŸ’¡ ç›®æ ‡æ–‡ä»¶ '{original_file_path}' å¯èƒ½è¢«å…¶ä»–åº”ç”¨ç¨‹åºé”å®š")
                print(f"ğŸ“ è¯·å…³é—­ç›¸å…³åº”ç”¨ç¨‹åºåé‡è¯•")
            else:
                print(f"âš ï¸ å…¶ä»–é”™è¯¯: {e}")
            return ""
            
    except Exception as e:
        print(f"âŒ ä¿å­˜åŸå§‹æ–‡ä»¶æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
        return ""

def convert_document_to_txt(file_path: str) -> str:
    """Convert document to txt file"""
    soffice = r"D:\LibreOffice\program\soffice.exe"
    subprocess.run(
        [soffice, "--headless", "--convert-to", "txt:Text (encoded):UTF8", file_path, "--outdir", "D:\asianInfo\ExcelAssist\agents\output"], check=True)
    return file_path

def retrieve_file_content(file_paths: list[str], session_id: str, output_dir: str = None) -> list[str]:
    """Process files and store them as .txt files in the staging area: conversations/session_id/user_uploaded_files
    This function only handles file processing, not original file saving.
    
    Args:
        file_paths: List of file paths to process
        session_id: Session identifier for folder structure
        output_dir: Optional output directory override
        
    Returns:
        list[str]: List of processed .txt file paths in staging area
    """
    
    from pathlib import Path
    
    if output_dir:
        staging_dir = Path(output_dir)
        staging_dir.mkdir(parents=True, exist_ok=True)
    else:
        # Create the staging area: conversations/session_id/user_uploaded_files
        project_root = Path.cwd()
        staging_dir = project_root / "conversations" / session_id / "user_uploaded_files"
        staging_dir.mkdir(parents=True, exist_ok=True)
        output_dir = project_root / "conversations" / session_id / "output"
        output_dir.mkdir(parents=True, exist_ok=True)
    
    processed_files = []
    
    for file_path in file_paths:
        try:
            source_path = Path(file_path)
            if not source_path.exists():
                print(f"âŒ File not found: {file_path}")
                continue
                
            print(f"ğŸ”„ Processing file: {source_path.name}")
            
            # Process the file content
            processed_content = process_file_to_text(source_path)
            
            if processed_content is not None:
                # Save processed content as .txt file in staging area
                txt_file_path = staging_dir / f"{source_path.stem}.txt"
                
                if txt_file_path.exists():
                    print(f"âš ï¸ å¤„ç†æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ­£åœ¨æ›´æ–°å†…å®¹: {txt_file_path.name}")
                
                txt_file_path.write_text(processed_content, encoding='utf-8')
                processed_files.append(str(txt_file_path))
                print(f"âœ… æ–‡ä»¶å¤„ç†å¹¶ä¿å­˜åˆ°æš‚å­˜åŒº: {txt_file_path}")
            else:
                print(f"âŒ æ–‡ä»¶å†…å®¹å¤„ç†å¤±è´¥: {source_path.name}")
                
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯ {file_path}: {e}")
            continue
    
    print(f"ğŸ‰ æˆåŠŸå¤„ç† {len(processed_files)} ä¸ªæ–‡ä»¶åˆ°æš‚å­˜åŒº")
    
    return processed_files


def process_file_to_text(file_path: str | Path) -> str | None:
    """
    Efficiently process a file to readable text content in memory.
    
    This function does: 1 read â†’ process in memory â†’ return text
    Instead of: read â†’ write temp file â†’ read temp file â†’ write final file
    
    Returns:
        str: The processed text content, or None if processing failed
    """
    source_path = Path(file_path)
    file_extension = source_path.suffix.lower()
    
    # Define file type categories
    spreadsheet_extensions = {'.xlsx', '.xls', '.xlsm', '.ods', '.csv'}
    text_extensions = {'.txt', '.md', '.json', '.xml', '.html', '.htm', '.py', '.js', '.css', '.sql', '.log'}
    document_extensions = {'.docx', '.doc', '.pptx', '.ppt'}
    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.webp', '.svg'}
    
    try:
        # Handle spreadsheet files
        if file_extension in spreadsheet_extensions:
            return _process_spreadsheet_in_memory(source_path)
        
        # Handle document files (DOCX, DOC, etc.)
        elif file_extension in document_extensions:
            return _process_document_in_memory(source_path)
        
        # Handle plain text files
        elif file_extension in text_extensions:
            return _read_text_auto(source_path)
        
        # Handle image files - return metadata since we can't convert to text
        elif file_extension in image_extensions:
            return f"Image file: {source_path.name}\nFile size: {source_path.stat().st_size} bytes\nFormat: {file_extension}"
        
        # Handle other file types
        else:
            # Try to detect if it's a text file by MIME type
            import mimetypes
            mime_type, _ = mimetypes.guess_type(str(source_path))
            
            if mime_type and mime_type.startswith('text/'):
                return _read_text_auto(source_path)
            else:
                # For binary files, return metadata
                return f"Binary file: {source_path.name}\nFile size: {source_path.stat().st_size} bytes\nType: {mime_type or 'unknown'}"
                
    except Exception as e:
        print(f"âŒ Error processing file {file_path}: {e}")
        return None


def _process_spreadsheet_in_memory(source_path: Path) -> str:
    """Process spreadsheet file in memory using LibreOffice"""
    import tempfile
    
    # Create a temporary directory for LibreOffice processing
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_dir_path = Path(temp_dir)
        
        # LibreOffice export to temp directory
        soffice = r"D:\LibreOffice\program\soffice.exe"
        subprocess.run(
            [soffice, "--headless", "--convert-to", "html", str(source_path),
             "--outdir", str(temp_dir_path)],
            check=True
        )
        
        # Read the generated HTML file
        raw_html_path = temp_dir_path / f"{source_path.stem}.html"
        if not raw_html_path.exists():
            raise FileNotFoundError(f"LibreOffice did not create {raw_html_path}")
        
        # Clean HTML in memory and return the result
        return _clean_html_in_memory(raw_html_path)


def _process_document_in_memory(source_path: Path) -> str:
    """Process document file in memory using LibreOffice to convert to txt"""
    import tempfile
    
    # Create a temporary directory for LibreOffice processing
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_dir_path = Path(temp_dir)
        
        # LibreOffice export to temp directory as txt
        soffice = r"D:\LibreOffice\program\soffice.exe"
        subprocess.run(
            [soffice, "--headless", "--convert-to", "txt:Text (encoded):UTF8", str(source_path),
             "--outdir", str(temp_dir_path)],
            check=True
        )
        
        # Read the generated TXT file
        raw_txt_path = temp_dir_path / f"{source_path.stem}.txt"
        if not raw_txt_path.exists():
            raise FileNotFoundError(f"LibreOffice did not create {raw_txt_path}")
        
        # Read and return the txt content directly
        return _read_text_auto(raw_txt_path)


def _clean_html_in_memory(raw_html_path: Path) -> str:
    """
    Clean HTML file in memory and return the clean HTML string.
    This function preserves both table structures and text content while
    removing unnecessary decorative HTML elements.
    """
    # Tags to keep for table structure
    TABLE_TAGS = {"table", "thead", "tbody", "tfoot", "tr", "td", "th", "col", "colgroup"}
    
    # Tags to keep for text content and basic formatting
    TEXT_TAGS = {"p", "div", "span", "h1", "h2", "h3", "h4", "h5", "h6", 
                 "br", "strong", "b", "em", "i", "u", "ul", "ol", "li", 
                 "blockquote", "pre", "code", "a"}
    
    # All tags we want to preserve
    KEEP = TABLE_TAGS | TEXT_TAGS
    
    # Attributes to always keep
    ATTR_ALWAYS = {"rowspan", "colspan"}
    
    # Additional attributes for specific tags
    ATTR_EXTRA = {
        "colgroup": {"span"},
        "a": {"href"},  # Keep links
    }

    html = _read_text_auto(raw_html_path)

    # Drop DOCTYPE / XML prologs
    html = re.sub(r'<!DOCTYPE[^>]*?>',           '', html, flags=re.I | re.S)
    html = re.sub(r'<\?xml[^>]*?\?>',            '', html, flags=re.I)
    html = re.sub(r'<\?mso-application[^>]*?\?>','', html, flags=re.I)

    soup = BeautifulSoup(html, "html.parser")

    # Remove styling and metadata tags
    for t in soup.find_all(["style", "meta", "link", "script", "noscript"]):
        t.decompose()

    # Clean up attributes and unwrap unwanted tags
    for t in soup.find_all(True):
        if t.name not in KEEP:
            # Unwrap tags we don't want to keep, but preserve their content
            t.unwrap()
            continue
        
        # For kept tags, clean up attributes
        allowed = ATTR_ALWAYS | ATTR_EXTRA.get(t.name, set())
        
        # Remove style attributes and other formatting attributes
        attrs_to_remove = []
        for attr_name in t.attrs.keys():
            if attr_name not in allowed:
                # Remove ALL styling and metadata attributes
                attrs_to_remove.append(attr_name)
        
        for attr in attrs_to_remove:
            del t.attrs[attr]

    # Build a clean document structure
    shell = BeautifulSoup("<html><body></body></html>", "html.parser")
    
    # Add all content from the body, preserving both text and tables
    if soup.body:
        for element in soup.body.children:
            if element.name or (hasattr(element, 'strip') and element.strip()):
                # Add both tag elements and non-empty text nodes
                shell.body.append(element)
    else:
        # If no body tag, add all content
        for element in soup.children:
            if element.name or (hasattr(element, 'strip') and element.strip()):
                shell.body.append(element)

    # Return HTML with preserved formatting (keep newlines for bs4 processing)
    return str(shell)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ private helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ #
def _read_text_auto(path: Path) -> str:
    """Best-effort text loader with encoding detection."""
    data = path.read_bytes()
    for enc in ("utf-8", "utf-8-sig", "gb18030", "gbk", "big5"):
        try:
            return data.decode(enc)
        except UnicodeDecodeError:
            continue
    if chardet:
        enc = chardet.detect(data).get("encoding")
        if enc:
            try:
                return data.decode(enc)
            except UnicodeDecodeError:
                pass
    return data.decode("utf-8", errors="replace")


def read_txt_file(file_path: Union[Path, str]) -> str:
    """Read the content of a txt file"""
    try:
        path = Path(file_path).expanduser().resolve()
        return path.read_text(encoding="utf-8")
    except Exception as e:
        return f"Error reading file {file_path}: {e}"


def read_processed_files_content(file_paths: list[str], separator: str = "\n\n--- File Separator ---\n\n") -> str:
    """
    Read the content of files returned by retrieve_file_content function and return as a combined string.
    
    Args:
        file_paths: List of file paths returned by retrieve_file_content
        separator: String to separate content from different files (optional)
    
    Returns:
        str: Combined content of all files as a single string
    """
    if not file_paths:
        return ""
    
    combined_content = []
    
    for file_path in file_paths:
        try:
            path = Path(file_path)
            if path.exists():
                content = _read_text_auto(path)
                # Add file header for clarity
                file_header = f"=== Content from: {path.name} ==="
                combined_content.append(f"{file_header}\n{content}")
                print(f"âœ… Read content from: {path.name}")
            else:
                error_msg = f"File not found: {file_path}"
                combined_content.append(f"âŒ {error_msg}")
                print(f"âš ï¸ {error_msg}")
        except Exception as e:
            error_msg = f"Error reading {file_path}: {e}"
            combined_content.append(f"âŒ {error_msg}")
            print(f"âŒ {error_msg}")
    
    return separator.join(combined_content)


def extract_filename(file_path: str) -> str:
    """
    Extract filename from a file path or URL.
    
    Handles various path formats:
    - Windows: d:\folder\file.txt
    - Linux/Mac: /folder/file.txt  
    - HTTP URLs: https://example.com/folder/file.txt
    
    Args:
        file_path: File path or URL string
        
    Returns:
        str: The filename with extension
        
    Examples:
        >>> extract_filename(r"d:\asianInfo\ExcelAssist\ç‡•äº‘æ‘case\æ­£æ–‡ç¨¿å…³äºå°å‘é€šçŸ¥.doc")
        'æ­£æ–‡ç¨¿å…³äºå°å‘é€šçŸ¥.doc'
        >>> extract_filename("/home/user/document.pdf")
        'document.pdf'
        >>> extract_filename("https://example.com/files/image.jpg")
        'image.jpg'
    """
    if not file_path:
        return ""
    
    # Replace backslashes with forward slashes for consistency
    normalized_path = file_path.replace('\\', '/')
    
    # Split by forward slash and take the last part
    filename = normalized_path.split('/')[-1]
    
    # Handle edge cases where path ends with slash
    if not filename:
        # If the path ends with a slash, try the second-to-last part
        parts = [part for part in normalized_path.split('/') if part]
        filename = parts[-1] if parts else file_path
    
    return filename


def fetch_related_files_content(related_files: dict[str], base_path: str = r"D:\asianInfo\ExcelAssist\files") -> dict[str, str]:
    """
    Wrapper function that receives a dictionary of classified related files, and invoke fetch_related_files_content
    to fetch the actual content
    """
    print("related_files:  aaaaaaaaaa", related_files)
    table_files = related_files["è¡¨æ ¼"]
    base_path = r"D:\asianInfo\ExcelAssist\files\table_files\html_content"
    table_files_content = fetch_files_content(table_files, base_path)

    # document_files = related_files["æ–‡æ¡£"]
    # base_path = r"D:\asianInfo\ExcelAssist\files\document_files\txt_content"
    # document_files_content = fetch_files_content(document_files, base_path)

    file_content = table_files_content

    return file_content



    

def fetch_files_content(related_files: List[str], base_path: str = r"D:\asianInfo\ExcelAssist\files") -> Dict[str, str]:
        """
        Fetch the content of related files from the specified directory
        
        Args:
            related_files: List of filenames to fetch
            base_path: Base directory path where files are stored
            
        Returns:
            Dictionary mapping filename to file content
        """
        files_content = {}
        base_directory = Path(base_path)
        
        for filename in related_files:
            # Handle both .txt and non-.txt filenames
            txt_filename = filename if filename.endswith('.txt') else f"{filename}.txt"
            file_path = base_directory / txt_filename
            
            try:
                if file_path.exists():
                    content = file_path.read_text(encoding='utf-8')
                    files_content[filename] = content
                    print(f"âœ… æˆåŠŸè¯»å–æ–‡ä»¶: {filename}")
                else:
                    print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                    files_content[filename] = ""
            except Exception as e:
                print(f"âŒ è¯»å–æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
                files_content[filename] = ""
        
        return files_content


def excel_to_csv(excel_file, csv_file, sheet_name=0):
    """Enhanced Excel to CSV conversion with proper date handling"""
    import re
    
    try:
        # Read Excel file
        df = pd.read_excel(excel_file, sheet_name=sheet_name)
        print(f"ğŸ“Š Processing {len(df.columns)} columns for date cleaning...")
        
        # Process each column to handle dates properly
        for col in df.columns:
            print(f"ğŸ” Processing column '{col}' with dtype: {df[col].dtype}")
            
            # Check if column contains datetime-like data
            if df[col].dtype == 'datetime64[ns]' or any(isinstance(x, pd.Timestamp) for x in df[col].dropna()):
                print(f"ğŸ“… Found datetime column: {col}")
                # Convert datetime columns to clean date format
                df[col] = df[col].apply(lambda x: x.strftime('%Y-%m-%d') if pd.notna(x) and hasattr(x, 'strftime') else x)
            
            else:
                # Apply aggressive date cleaning to ALL columns (not just object columns)
                df[col] = df[col].apply(lambda x: clean_date_string(x) if pd.notna(x) else x)
        
        # Convert to CSV
        df.to_csv(csv_file, index=False, encoding='utf-8')
        print(f"âœ… Successfully converted {excel_file} to {csv_file}")
        
        # Read back and verify cleaning worked
        with open(csv_file, 'r', encoding='utf-8') as f:
            sample_content = f.read()[:500]
            if " 00:00:00" in sample_content:
                print(f"âš ï¸ Warning: Still found '00:00:00' in output, applying post-processing...")
                # Apply additional cleaning to the entire CSV content
                with open(csv_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Clean up the content with regex
                content = re.sub(r' 00:00:00', '', content)
                content = re.sub(r'\.00\.00\.00', '', content)
                content = re.sub(r'\.0+(?=,|$)', '', content)
                
                # Write back the cleaned content
                with open(csv_file, 'w', encoding='utf-8') as f:
                    f.write(content)
                print(f"âœ… Applied post-processing date cleanup")
        
    except Exception as e:
        print(f"âŒ Error converting Excel to CSV: {e}")
        # Fallback to simple conversion with post-processing
        try:
            df = pd.read_excel(excel_file, sheet_name=sheet_name)
            df.to_csv(csv_file, index=False, encoding='utf-8')
            
            # Apply post-processing cleanup
            with open(csv_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            content = re.sub(r' 00:00:00', '', content)
            content = re.sub(r'\.00\.00\.00', '', content)
            
            with open(csv_file, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"âœ… Fallback conversion with post-processing completed")
            
        except Exception as fallback_error:
            print(f"âŒ Fallback conversion also failed: {fallback_error}")


def clean_date_string(value):
    """Clean up date strings and remove malformed time portions"""
    if not isinstance(value, str):
        return value
    
    # Remove malformed time formats like "00.00.00.00"
    if ".00.00.00" in str(value):
        value = str(value).replace(".00.00.00", "")
    
    # Remove "00:00:00" time portions from date strings
    if " 00:00:00" in str(value):
        value = str(value).replace(" 00:00:00", "")
    
    # Handle other common malformed patterns
    value = re.sub(r'\.00\.00\.00$', '', str(value))  # Remove trailing .00.00.00
    value = re.sub(r' 00:00:00\.0+$', '', str(value))  # Remove trailing 00:00:00.000...
    value = re.sub(r'\.0+$', '', str(value))  # Remove trailing .000...
    
    # Try to parse and reformat date strings
    try:
        # Common date patterns to try
        date_patterns = [
            '%Y-%m-%d %H:%M:%S',
            '%Y-%m-%d',
            '%Y/%m/%d',
            '%m/%d/%Y',
            '%d/%m/%Y',
            '%Y%m%d'
        ]
        
        for pattern in date_patterns:
            try:
                parsed_date = datetime.strptime(str(value), pattern)
                return parsed_date.strftime('%Y-%m-%d')  # Return clean date format
            except ValueError:
                continue
    except:
        pass
    
    return value  # Return original if no date pattern matched

def read_relative_files_from_data_json(data_json_path: str = "agents/data.json", headers_mapping: str = None) -> str:
    """
    Read the relative files from data.json file
    """
    with open(data_json_path, 'r', encoding='utf-8') as f:
        data_json = json.load(f)
        for key, value in data_json.items():
            if key in headers_mapping:
                return value
    return None
def find_largest_file(excel_file_paths: list[str]) -> str:
    """
    Find the largest file in the list of Excel file paths
    return a dictionary with the file path and the number of rows
    """
    file_row_counts = {}
    for file_path in excel_file_paths:
        try:
            df = pd.read_excel(file_path)
            # Count actual data rows (excluding header)
            data_rows = len(df.dropna(how='all'))  # Remove completely empty rows
            file_row_counts[file_path] = data_rows
            print(f"ğŸ“Š {Path(file_path).name}: {data_rows} data rows")
        except Exception as e:
            print(f"âŒ Error reading {file_path}: {e}")
            file_row_counts[file_path] = 0
    
    # Find file with most rows
    largest_file = max(file_row_counts, key=file_row_counts.get)
    largest_row_count = file_row_counts[largest_file]
    print(f"ğŸ¯ Largest file: {Path(largest_file).name} with {largest_row_count} rows")
    return {largest_file: largest_row_count}

def detect_csv_format(csv_lines: list[str]) -> tuple[bool, int]:
    """
    Detect if CSV has repeated headers before each data row.
    
    Returns:
        tuple: (is_repeated_header_format, data_record_count)
    """
    if len(csv_lines) >= 4:  # Need at least 4 lines to detect pattern
        first_line = csv_lines[0].strip()
        third_line = csv_lines[2].strip()
        
        # If first and third lines are identical, it's likely repeated headers
        if first_line == third_line and first_line.count(',') > 0:
            # Each data record consists of header + data line
            data_rows = len(csv_lines) // 2
            print(f"ğŸ” Detected repeated header format: {len(csv_lines)} lines = {data_rows} data records")
            return True, data_rows
        else:
            # Normal CSV format with single header
            data_rows = len(csv_lines) - 1 if csv_lines else 0  # Subtract 1 for header
            print(f"ğŸ” Standard CSV format: {len(csv_lines)} lines = {data_rows} data records")
            return False, data_rows
    else:
        # Too few lines, use standard counting
        data_rows = len(csv_lines) - 1 if csv_lines else 0  # Subtract 1 for header
        return False, data_rows

def extract_structure_info_for_file(file_path: str, table_structure_info: dict) -> str:
    """
    Extract structure information for a specific file.
    
    Returns:
        str: Formatted structure information
    """
    file_name = Path(file_path).stem + ".txt"
    structure_info = ""
    
    if file_name in table_structure_info:
        file_structure = table_structure_info[file_name]
        
        try:
            summary_content = file_structure.get("summary", "")
            if summary_content:
                summary_content = summary_content.strip()
                
                if summary_content.startswith('{') and summary_content.endswith('}'):
                    parsed_summary = json.loads(summary_content)
                    
                    if file_name in parsed_summary:
                        file_data = parsed_summary[file_name]
                        
                        # Extract è¡¨æ ¼ç»“æ„ (detailed column structure)
                        table_structure = file_data.get("è¡¨æ ¼ç»“æ„", {})
                        if table_structure:
                            structure_info += "=== è¡¨æ ¼ç»“æ„ ===\n"
                            structure_info += json.dumps(table_structure, ensure_ascii=False, indent=2) + "\n\n"
                        
                        # Extract è¡¨æ ¼æ€»ç»“ (summary)
                        table_summary = file_data.get("è¡¨æ ¼æ€»ç»“", "")
                        if table_summary:
                            structure_info += "=== è¡¨æ ¼æ€»ç»“ ===\n"
                            structure_info += table_summary + "\n\n"
                else:
                    structure_info += "=== æ–‡ä»¶åˆ†æ ===\n"
                    structure_info += summary_content + "\n\n"
                        
        except json.JSONDecodeError:
            structure_info += "=== æ–‡ä»¶åˆ†æ ===\n"
            structure_info += file_structure.get("summary", "") + "\n\n"
        
        # Also include the full summary if available and different from structured data
        if "summary" in file_structure and not structure_info:
            structure_info += "=== å®Œæ•´åˆ†æ ===\n"
            structure_info += file_structure["summary"] + "\n\n"
    
    return structure_info

def parse_header_data_pairs(csv_lines: list[str], is_repeated_header: bool) -> list[tuple[str, str]]:
    """
    Parse CSV lines into header+data pairs.
    
    Args:
        csv_lines: List of CSV lines
        is_repeated_header: Whether the CSV has repeated headers
        
    Returns:
        list: List of (header, data) tuples
    """
    pairs = []
    
    if is_repeated_header:
        # Group every 2 lines as header+data pairs
        for i in range(0, len(csv_lines), 2):
            if i + 1 < len(csv_lines):
                header = csv_lines[i].strip()
                data = csv_lines[i + 1].strip()
                if header and data:  # Only add if both header and data exist
                    pairs.append((header, data))
    else:
        # Standard CSV: first line is header, rest are data
        if csv_lines:
            header = csv_lines[0].strip()
            for data_line in csv_lines[1:]:
                data = data_line.strip()
                if data:
                    pairs.append((header, data))
    
    return pairs

def create_chunks_from_pairs(pairs: list[tuple[str, str]], chunk_nums: int) -> list[list[tuple[str, str]]]:
    """
    Split header+data pairs into chunks, keeping pairs intact.
    
    Args:
        pairs: List of (header, data) tuples
        chunk_nums: Desired number of chunks
        
    Returns:
        list: List of chunks, each containing pairs
    """
    if not pairs:
        return []
    
    total_pairs = len(pairs)
    actual_chunk_nums = min(chunk_nums, total_pairs)
    
    if actual_chunk_nums == 0:
        return []
    
    base_chunk_size = total_pairs // actual_chunk_nums
    remainder = total_pairs % actual_chunk_nums
    
    print(f"ğŸ“ Dividing {total_pairs} data pairs into {actual_chunk_nums} chunks (requested: {chunk_nums})")
    print(f"   Base chunk size: {base_chunk_size} pairs, Extra pairs to distribute: {remainder}")
    
    chunks = []
    current_idx = 0
    
    for chunk_index in range(actual_chunk_nums):
        # First 'remainder' chunks get one extra pair
        if chunk_index < remainder:
            chunk_size = base_chunk_size + 1
        else:
            chunk_size = base_chunk_size
        
        start_idx = current_idx
        end_idx = current_idx + chunk_size
        
        chunk_pairs = pairs[start_idx:end_idx]
        chunks.append(chunk_pairs)
        current_idx = end_idx
        
        print(f"âœ… Created chunk {chunk_index + 1}/{actual_chunk_nums} with {len(chunk_pairs)} data pairs")
    
    return chunks

def combine_chunk_content(chunk_pairs: list[tuple[str, str]], largest_structure_info: str, 
                         largest_filename: str, other_files_content: list[str], 
                         supplement_files_summary: str) -> str:
    """
    Combine chunk content with structure info and other files.
    
    Returns:
        str: Combined content for the chunk
    """
    chunk_combined = []
    
    # 1. Add largest file structure + data chunk as PRIMARY DATA SOURCE
    if chunk_pairs:
        largest_file_chunk_content = ""
        
        # Add structure information if available
        if largest_structure_info.strip():
            largest_file_chunk_content += largest_structure_info.strip() + "\n\n"
        
        # Add data header with clear main data source label
        largest_file_chunk_content += f"=== æ ¸å¿ƒæ•°æ®æºï¼š{largest_filename} ===\n"
        largest_file_chunk_content += "ã€è¯´æ˜ã€‘ä»¥ä¸‹ä¸ºä¸»è¦æ•°æ®æºï¼Œè¯·ä¼˜å…ˆåŸºäºæ­¤æ•°æ®è¿›è¡Œåˆæˆå’Œå¡«å……\n\n"
        
        # Reconstruct the alternating header+data format
        for header, data in chunk_pairs:
            largest_file_chunk_content += f"{header}\n{data}\n"
        
        chunk_combined.append(largest_file_chunk_content.rstrip())  # Remove trailing newline
    
    # 2. Add other files' complete content as REFERENCE DATA
    for other_content in other_files_content:
        if other_content.strip():
            # Add clear reference data label
            reference_content = other_content.replace("=== ", "=== å‚è€ƒæ•°æ®æºï¼š")
            reference_content = "ã€è¯´æ˜ã€‘ä»¥ä¸‹ä¸ºå‚è€ƒæ•°æ®æºï¼Œç”¨äºè¡¥å……å’ŒéªŒè¯æ ¸å¿ƒæ•°æ®æºä¸­çš„ä¿¡æ¯\n\n" + reference_content
            chunk_combined.append(reference_content)
    
    # 3. Add supplement information as ADDITIONAL CONTEXT
    if supplement_files_summary:
        supplement_content = ""
        if supplement_files_summary.strip().startswith("=== è¡¥å……æ–‡ä»¶å†…å®¹ ==="):
            supplement_content = supplement_files_summary.replace("=== è¡¥å……æ–‡ä»¶å†…å®¹ ===", "=== è¡¥å……ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ ===")
        else:
            supplement_content = f"=== è¡¥å……ä¿¡æ¯å’Œä¸Šä¸‹æ–‡ ===\n{supplement_files_summary}"
        
        supplement_content = "ã€è¯´æ˜ã€‘ä»¥ä¸‹ä¸ºè¡¥å……ä¿¡æ¯ï¼Œç”¨äºç†è§£ä¸šåŠ¡èƒŒæ™¯å’Œå¡«å……è§„åˆ™\n\n" + supplement_content
        chunk_combined.append(supplement_content)
    
    return "\n\n".join(chunk_combined)

def find_largest_file(csv_files: list[str], row_counts: list[int], largest_file: str = None) -> str:
    """
    Find the file with the most data rows.
    
    Returns:
        str: Path to the largest file
    """
    if largest_file is None:
        max_rows_idx = row_counts.index(max(row_counts))
        largest_file = csv_files[max_rows_idx]
        print(f"ğŸ“Š Largest file: {Path(largest_file).name} with {row_counts[max_rows_idx]} rows")
    else:
        # Validate the specified largest file exists in our CSV files
        if largest_file not in csv_files:
            print(f"âš ï¸ Specified largest file not found in CSV files, using automatic selection")
            max_rows_idx = row_counts.index(max(row_counts))
            largest_file = csv_files[max_rows_idx]
    
    return largest_file

def process_excel_files_for_integration(excel_file_paths: list[str], supplement_files_summary: str = "", 
                                      session_id: str = "1", chunk_nums: int = 5, largest_file: str = None,
                                      data_json_path: str = "agents/data.json", village_name: str = "") -> dict:
    """
    Process Excel files by reading their corresponding pre-generated CSV files,
    finding the one with most rows, chunking the largest file, and combining everything.
    
    Args:
        excel_file_paths: List of Excel file paths (used to find corresponding CSV files)
        supplement_files_summary: String containing supplement file content (not a list)
        session_id: Session identifier for folder structure
        chunk_nums: Number of chunks to create (default 5)
        largest_file: Optional pre-specified largest file path
        data_json_path: Path to data.json file containing structure information
        village_name: Name of the village to process
    Returns:
        dict: {
            "combined_chunks": List of strings, each containing combined content of one chunk with other files
            "largest_file_row_count": int, number of data rows in the largest file
        }
    """
    print(f"ğŸ”„ Processing {len(excel_file_paths)} Excel files...")
    if supplement_files_summary:
        print(f"ğŸ“„ Also processing supplement files content")
    
    # Map Excel file paths to corresponding CSV files in CSV_files directory
    csv_files = []
    row_counts = []
    csv_base_dir = Path(f"files/{village_name}/table_files/CSV_files")
    
    if not csv_base_dir.exists():
        print(f"âŒ CSV files directory not found: {csv_base_dir}")
        return {"combined_chunks": [], "largest_file_row_count": 0}
    
    # Step 1: Find corresponding CSV files and count rows
    file_contents = {}  # {original_excel_path: csv_content}
    
    for excel_path in excel_file_paths:
        excel_filename = Path(excel_path).stem  # Get filename without extension
        csv_file_path = csv_base_dir / f"{excel_filename}.csv"
        
        if csv_file_path.exists():
            try:
                # Read CSV content
                with open(csv_file_path, 'r', encoding='utf-8') as f:
                    csv_content = f.read()
                
                # Count data rows using helper function
                csv_lines = csv_content.strip().split('\n')
                is_repeated_header, data_rows = detect_csv_format(csv_lines)
                
                csv_files.append(excel_path)  # Keep original Excel path as key
                row_counts.append(data_rows)
                
                # Store content with proper headers
                combined_content = f"=== {Path(excel_path).name} çš„è¡¨æ ¼æ•°æ® ===\n{csv_content}"
                file_contents[excel_path] = combined_content
                
                print(f"âœ… Found CSV for {Path(excel_path).name}: {data_rows} data rows")
                
            except Exception as e:
                print(f"âŒ Error reading CSV for {Path(excel_path).name}: {e}")
                file_contents[excel_path] = f"Error reading CSV file: {e}"
                csv_files.append(excel_path)
                row_counts.append(0)
        else:
            print(f"âš ï¸ No corresponding CSV found for {Path(excel_path).name}, skipping...")
    
    if not csv_files:
        print("âŒ No CSV files found for processing")
        return {"combined_chunks": [], "largest_file_row_count": 0}
    
    # Step 2: Load structure information from data.json
    table_structure_info = {}
    if Path(data_json_path).exists():
        try:
            with open(data_json_path, 'r', encoding='utf-8') as f:
                data_content = json.load(f)
                # Get table structure info from all locations
                for location_key, location_data in data_content.items():
                    if isinstance(location_data, dict) and "è¡¨æ ¼" in location_data:
                        table_structure_info.update(location_data["è¡¨æ ¼"])
                print(f"ğŸ“‹ Loaded structure info for {len(table_structure_info)} tables")
        except Exception as e:
            print(f"âš ï¸ Failed to load structure info: {e}")
    
    # Step 3: Add structure information to file contents
    for excel_path in list(file_contents.keys()):
        structure_info = extract_structure_info_for_file(excel_path, table_structure_info)
        
        # Update file content to include structure information
        if structure_info:
            original_content = file_contents[excel_path]
            filename = Path(excel_path).name
            new_content = f"=== {filename} çš„è¡¨æ ¼ç»“æ„ ===\n{structure_info}=== {filename} çš„è¡¨æ ¼æ•°æ® ===\n"
            # Extract the CSV data part
            csv_data = original_content.split(f"=== {filename} çš„è¡¨æ ¼æ•°æ® ===\n", 1)[1] if f"=== {filename} çš„è¡¨æ ¼æ•°æ® ===" in original_content else original_content
            file_contents[excel_path] = new_content + csv_data
            print(f"âœ… Added structure info for {filename}")
    
    # Step 4: Find the largest file by row count
    largest_file = find_largest_file(csv_files, row_counts, largest_file)
    
    # Step 5: Handle the largest file - divide into chunks while preserving header+data pairs
    largest_file_content = file_contents[largest_file]
    other_files_content = [content for path, content in file_contents.items() if path != largest_file]
    
    # Extract data from the largest file content
    largest_file_lines = largest_file_content.split('\n')
    largest_filename = Path(largest_file).name
    data_header_pattern = f"=== {largest_filename} çš„è¡¨æ ¼æ•°æ® ==="
    
    # Find where the actual CSV data starts and extract structure info
    data_section_start = -1
    for i, line in enumerate(largest_file_lines):
        if line.strip() == data_header_pattern:
            data_section_start = i
            break
    
    if data_section_start == -1:
        print(f"âš ï¸ Could not find data section separator '{data_header_pattern}', using full content")
        largest_structure_info = ""
        largest_data_lines = largest_file_lines
    else:
        # Structure is everything before the data header
        largest_structure_info = '\n'.join(largest_file_lines[:data_section_start])
        # Data is everything after the data header (excluding the header itself)
        largest_data_lines = largest_file_lines[data_section_start + 1:]
    
    # Remove empty lines at the beginning and end of data
    while largest_data_lines and not largest_data_lines[0].strip():
        largest_data_lines.pop(0)
    while largest_data_lines and not largest_data_lines[-1].strip():
        largest_data_lines.pop()
    
    # Detect format and parse into header+data pairs
    is_repeated_header, _ = detect_csv_format(largest_data_lines)
    header_data_pairs = parse_header_data_pairs(largest_data_lines, is_repeated_header)
    
    if not header_data_pairs:
        print("âš ï¸ No valid header+data pairs found")
        return {"combined_chunks": [], "largest_file_row_count": 0}
    
    # Create chunks from pairs (preserving header+data integrity)
    pair_chunks = create_chunks_from_pairs(header_data_pairs, chunk_nums)
    
    if not pair_chunks:
        print("âš ï¸ No chunks created")
        return {"combined_chunks": [], "largest_file_row_count": 0}
    
    # Step 6: Combine chunks with other content
    combined_chunks = []
    for chunk_index, chunk_pairs in enumerate(pair_chunks):
        combined_content = combine_chunk_content(
            chunk_pairs, largest_structure_info, largest_filename, 
            other_files_content, supplement_files_summary
        )
        combined_chunks.append(combined_content)
    
    print(f"ğŸ‰ Successfully created {len(combined_chunks)} combined chunks")
    
    # Return both chunks and largest file row count
    largest_file_row_count = row_counts[csv_files.index(largest_file)] if largest_file in csv_files else 0
    
    return {
        "combined_chunks": combined_chunks,
        "largest_file_row_count": largest_file_row_count
    }


def process_excel_files_for_merge(excel_file_paths: list[str], session_id: str = "1", 
                                      village_name: str = "", chunk_nums: int = 5) -> dict:
        """
        å¤„ç†Excelæ–‡ä»¶è¿›è¡Œåˆå¹¶ - å°†æ‰€æœ‰æ–‡ä»¶ä½œä¸ºæ ¸å¿ƒæ•°æ®è¿›è¡Œåˆå¹¶è€Œä¸æ˜¯åˆ†ä¸ºæ ¸å¿ƒå’Œå‚è€ƒæ•°æ®
        
        Args:
            excel_file_paths: Excelæ–‡ä»¶è·¯å¾„åˆ—è¡¨
            session_id: ä¼šè¯ID
            village_name: æ‘åº„åç§°
            chunk_nums: åˆ†å—æ•°é‡
        Returns:
            dict: {
                "combined_chunks": åˆå¹¶åçš„æ•°æ®å—åˆ—è¡¨
                "total_row_count": æ€»è¡Œæ•°
            }
        """
        print(f"ğŸ”„ åˆå¹¶å¤„ç† {len(excel_file_paths)} ä¸ªExcelæ–‡ä»¶...")
        
        # Map Excel file paths to corresponding CSV files in CSV_files directory
        csv_files = []
        row_counts = []
        csv_base_dir = Path(f"files/{village_name}/table_files/CSV_files")
        
        if not csv_base_dir.exists():
            print(f"âŒ CSV files directory not found: {csv_base_dir}")
            return {"combined_chunks": [], "total_row_count": 0}
        
        # Step 1: Load all CSV files and their content
        file_contents = {}  # {original_excel_path: csv_content}
        all_data_rows = []  # å­˜å‚¨æ‰€æœ‰æ•°æ®è¡Œç”¨äºåˆå¹¶
        
        for excel_path in excel_file_paths:
            excel_filename = Path(excel_path).stem
            csv_file_path = csv_base_dir / f"{excel_filename}.csv"
            
            if csv_file_path.exists():
                try:
                    # Read CSV content
                    with open(csv_file_path, 'r', encoding='utf-8') as f:
                        csv_content = f.read()
                    
                    # Count data rows using helper function from file_process.py
                    csv_lines = csv_content.strip().split('\n')
                    # We need to import the helper functions
                    from utils.file_process import detect_csv_format, parse_header_data_pairs
                    
                    is_repeated_header, data_rows = detect_csv_format(csv_lines)
                    
                    csv_files.append(excel_path)
                    row_counts.append(data_rows)
                    
                    # Store content with proper headers
                    combined_content = f"=== {Path(excel_path).name} çš„æ ¸å¿ƒæ•°æ® ===\n{csv_content}"
                    file_contents[excel_path] = combined_content
                    
                    # Parse header+data pairs for merging
                    header_data_pairs = parse_header_data_pairs(csv_lines, is_repeated_header)
                    
                    # Add all data pairs to the combined list with file identifier
                    for header, data in header_data_pairs:
                        all_data_rows.append({
                            'source_file': Path(excel_path).name,
                            'header': header,
                            'data': data,
                            'combined_entry': f"æ–‡ä»¶æ¥æº: {Path(excel_path).name}\nè¡¨å¤´: {header}\næ•°æ®: {data}"
                        })
                    
                    print(f"âœ… åŠ è½½CSVæ–‡ä»¶ {Path(excel_path).name}: {data_rows} æ•°æ®è¡Œ")
                    
                except Exception as e:
                    print(f"âŒ è¯»å–CSVæ–‡ä»¶é”™è¯¯ {Path(excel_path).name}: {e}")
                    file_contents[excel_path] = f"Error reading CSV file: {e}"
                    csv_files.append(excel_path)
                    row_counts.append(0)
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°å¯¹åº”çš„CSVæ–‡ä»¶ {Path(excel_path).name}")
        
        if not all_data_rows:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯åˆå¹¶çš„æ•°æ®")
            return {"combined_chunks": [], "total_row_count": 0}
        
        print(f"ğŸ“Š æ€»å…±æ”¶é›†åˆ° {len(all_data_rows)} è¡Œæ•°æ®ç”¨äºåˆå¹¶")
        
        # Step 2: Create chunks from all merged data
        total_rows = len(all_data_rows)
        chunk_size = max(1, total_rows // chunk_nums)  # ç¡®ä¿æ¯ä¸ªchunkè‡³å°‘æœ‰1è¡Œ
        
        combined_chunks = []
        for i in range(0, total_rows, chunk_size):
            chunk_end = min(i + chunk_size, total_rows)
            chunk_data = all_data_rows[i:chunk_end]
            
            # Build chunk content
            chunk_content = f"=== åˆå¹¶æ•°æ®å— {len(combined_chunks) + 1} ===\n"
            chunk_content += f"åŒ…å« {len(chunk_data)} è¡Œæ¥è‡ª {len(set([row['source_file'] for row in chunk_data]))} ä¸ªæ–‡ä»¶çš„æ•°æ®\n\n"
            
            # Add all data entries in this chunk
            for idx, row_data in enumerate(chunk_data):
                chunk_content += f"--- æ•°æ®æ¡ç›® {idx + 1} ---\n"
                chunk_content += row_data['combined_entry'] + "\n\n"
            
            combined_chunks.append(chunk_content)
        
        print(f"ğŸ‰ æˆåŠŸåˆ›å»º {len(combined_chunks)} ä¸ªåˆå¹¶æ•°æ®å—")
        
        return {
            "combined_chunks": combined_chunks,
            "total_row_count": total_rows
        }


def extract_file_from_recall(response: str) -> list:
    """è¿”å›æ–‡ä»¶åæ•°ç»„"""

    # Parse the response to extract the file list
    print(f"ğŸ” å¼€å§‹è§£æå“åº”å†…å®¹: {response[:200]}...")
    
    try:
        # Try to parse as JSON array first
        related_files = json.loads(response)
        if isinstance(related_files, list):
            print(f"âœ… æˆåŠŸè§£æJSONæ•°ç»„: {related_files}")
            return related_files
    except:
        print("âŒ JSONè§£æå¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•")
        pass
    
    try:
        # Look for patterns like ["file1", "file2"] or ['file1', 'file2']
        match = re.search(r'\[.*?\]', response)
        if match:
            related_files = json.loads(match.group())
            print(f"âœ… æ­£åˆ™åŒ¹é…æˆåŠŸ: {related_files}")
            return related_files
    except:
        print("âŒ æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å¤±è´¥")
        pass
    
    # Check if response contains file names with .txt, .xlsx, .docx extensions
    file_pattern = r'["""]([^"""]*?\.(txt|xlsx|docx|csv|pdf))["""]'
    file_matches = re.findall(file_pattern, response)
    if file_matches:
        related_files = [match[0] for match in file_matches]
        print(f"âœ… æ–‡ä»¶åæ¨¡å¼åŒ¹é…æˆåŠŸ: {related_files}")
        return related_files
    
    # Final fallback: split by lines and filter
    related_files = [line.strip().strip('"\'') for line in response.split('\n') 
                    if line.strip() and not line.strip().startswith('#') and 
                    any(ext in line.lower() for ext in ['.txt', '.xlsx', '.docx', '.csv', '.pdf'])]
    
    print(f"ğŸ“ è§£æå‡ºçš„ç›¸å…³æ–‡ä»¶: {related_files}")
    return related_files

def _clean_csv_data(csv_data: str) -> str:
    """
    Clean up the CSV data by removing the thinking part and only keeping the actual data
    under the "===æœ€ç»ˆç­”æ¡ˆ===" section
    
    Args:
        csv_data: Raw CSV data string containing both reasoning and final answer
        
    Returns:
        str: Cleaned CSV data with only the actual data rows
    """
    csv_data_lines = csv_data.split('\n')
    
    # Find the start of the final answer section
    final_answer_started = False
    cleaned_lines = []
    
    for line in csv_data_lines:
        line = line.strip()
        
        # Check if we've reached the final answer section
        if "=== æœ€ç»ˆç­”æ¡ˆ ===" in line:
            final_answer_started = True
            continue
        
        # If we encounter a new reasoning section, stop collecting
        if final_answer_started and "=== æ¨ç†è¿‡ç¨‹ ===" in line:
            final_answer_started = False
            continue
        
        # If we're in the final answer section, collect the data lines
        if final_answer_started:
            # Skip empty lines and lines that look like section headers
            if line and not line.startswith("==="):
                cleaned_lines.append(line)
    
    # Join the cleaned lines and apply LLM error message cleaning
    initial_cleaned = '\n'.join(cleaned_lines)
    
    # Apply comprehensive error message cleaning
    final_cleaned = clean_llm_error_messages(initial_cleaned)
    
    return final_cleaned

def save_csv_to_output(csv_data_list: list[str], session_id: str = "1") -> str:
    """
    Save CSV data to session-specific CSV_files folder
    
    Args:
        csv_data_list: List of CSV strings from concurrent processing
        session_id: Session identifier for folder structure
    
    Returns:
        str: Full path to the saved CSV file
    """
    import os
    from datetime import datetime
    from pathlib import Path
    
    # Create output directory
    output_dir = Path(f"D:\\asianInfo\\ExcelAssist\\conversations\\{session_id}\\CSV_files")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Generate filename with timestamp
    actual_filename_with_thinking = "synthesized_table_with_thinking"
    actual_filename_with_only_data = "synthesized_table_with_only_data"
    filename_with_thinking = f"{actual_filename_with_thinking}.csv"
    filename_with_only_data = f"{actual_filename_with_only_data}.csv"
    filepath_with_thinking = output_dir / filename_with_thinking
    filepath_with_only_data = output_dir / filename_with_only_data
    
    # Combine all CSV data and clean up
    combined_csv = '\n'.join(csv_data_list)
    
    # Apply comprehensive error message cleaning first
    # combined_csv = clean_llm_error_messages(combined_csv)
    
    # Remove unnecessary newlines and clean up the CSV content
    lines = combined_csv.split('\n')
    cleaned_lines = []
    
    for line in lines:
        line = line.strip()
        if line:  # Only keep non-empty lines
            cleaned_lines.append(line)
    
    # Join with single newlines
    final_csv = '\n'.join(cleaned_lines)

    # Write to file with thinking process
    with open(filepath_with_thinking, 'w', encoding='utf-8', newline='') as f:
        f.write(final_csv)
    
    # Write to file with only cleaned data (using helper function)
    with open(filepath_with_only_data, 'w', encoding='utf-8', newline='') as f:
        cleaned_data = _clean_csv_data(final_csv)
        f.write(cleaned_data)
            
    print(f"ğŸ’¾ CSVæ•°æ®å·²ä¿å­˜åˆ°: {filepath_with_thinking}")
    print(f"ğŸ“„ CSVæ•°æ®å·²ä¿å­˜åˆ°: {filepath_with_only_data}")
    print(f"ğŸ“Š æ¸…ç†ååŒ…å« {len(cleaned_lines)} è¡Œæ•°æ®")
    return str(filepath_with_thinking), str(filepath_with_only_data)



def get_available_locations(data: dict) -> list[str]:
        """
        ä»data.jsonä¸­è·å–å¯ç”¨çš„æ‘/é•‡ä½ç½®åˆ—è¡¨
        
        Args:
            data: data.jsonçš„æ•°æ®ç»“æ„
            
        Returns:
            list[str]: å¯ç”¨çš„ä½ç½®åˆ—è¡¨
        """
        locations = []
        for key in data.keys():
            if isinstance(data[key], dict) and "è¡¨æ ¼" in data[key] and "æ–‡æ¡£" in data[key]:
                locations.append(key)
        return locations

def determine_location_from_content(file_content: str, file_name: str, user_input: str, available_locations: list[str]) -> str:
    """
    æ ¹æ®æ–‡ä»¶å†…å®¹ã€æ–‡ä»¶åå’Œç”¨æˆ·è¾“å…¥ç¡®å®šæ–‡ä»¶æ‰€å±çš„æ‘/é•‡
    
    Args:
        file_content: æ–‡ä»¶å†…å®¹
        file_name: æ–‡ä»¶å
        user_input: ç”¨æˆ·è¾“å…¥
        available_locations: å¯ç”¨çš„ä½ç½®åˆ—è¡¨ï¼ˆä»data.jsonè¯»å–ï¼‰
        
    Returns:
        location: ç¡®å®šçš„ä½ç½®ï¼Œå¦‚æœæ— æ³•ç¡®å®šåˆ™è¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨ä½ç½®
    """
    if not available_locations:
        print("âš ï¸ æ²¡æœ‰å¯ç”¨çš„ä½ç½®ï¼Œåˆ›å»ºé»˜è®¤ä½ç½®")
        return "é»˜è®¤ä½ç½®"
    
    # é¦–å…ˆæ£€æŸ¥æ–‡ä»¶åä¸­æ˜¯å¦åŒ…å«ä½ç½®ä¿¡æ¯
    for location in available_locations:
        if location in file_name:
            print(f"ğŸ“ ä»æ–‡ä»¶åç¡®å®šä½ç½®: {location}")
            return location
    
    # æ£€æŸ¥æ–‡ä»¶å†…å®¹ä¸­æ˜¯å¦åŒ…å«ä½ç½®ä¿¡æ¯
    content_to_check = file_content[:1000]  # åªæ£€æŸ¥å‰1000ä¸ªå­—ç¬¦
    for location in available_locations:
        if location in content_to_check:
            print(f"ğŸ“ ä»æ–‡ä»¶å†…å®¹ç¡®å®šä½ç½®: {location}")
            return location
    
    # æ£€æŸ¥ç”¨æˆ·è¾“å…¥ä¸­æ˜¯å¦åŒ…å«ä½ç½®ä¿¡æ¯
    for location in available_locations:
        if location in user_input:
            print(f"ğŸ“ ä»ç”¨æˆ·è¾“å…¥ç¡®å®šä½ç½®: {location}")
            return location
    
    # å¦‚æœæ— æ³•ç¡®å®šï¼Œä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½åˆ†æ
    try:
        analysis_prompt = f"""
        è¯·åˆ†æä»¥ä¸‹ä¿¡æ¯ï¼Œåˆ¤æ–­æ–‡ä»¶å±äºå“ªä¸ªæ‘/é•‡ï¼š
        
        å¯é€‰ä½ç½®ï¼š{', '.join(available_locations)}
        
        æ–‡ä»¶åï¼š{file_name}
        ç”¨æˆ·è¾“å…¥ï¼š{user_input}
        æ–‡ä»¶å†…å®¹ç‰‡æ®µï¼š{content_to_check}
        
        è¯·åªå›å¤ç¡®å®šçš„ä½ç½®åç§°ï¼Œå¦‚æœæ— æ³•ç¡®å®šï¼Œè¯·å›å¤"{available_locations[0]}"ã€‚
        """
        
        analysis_result = invoke_model(model_name="Qwen/Qwen3-32B", 
                                        messages=[SystemMessage(content=analysis_prompt)])
        
        for location in available_locations:
            if location in analysis_result:
                print(f"ğŸ“ é€šè¿‡LLMåˆ†æç¡®å®šä½ç½®: {location}")
                return location
                
    except Exception as e:
        print(f"âŒ LLMä½ç½®åˆ†æå¤±è´¥: {e}")
    
    # é»˜è®¤è¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨ä½ç½®
    default_location = available_locations[0]
    print(f"ğŸ“ ä½¿ç”¨é»˜è®¤ä½ç½®: {default_location}")
    return default_location

def ensure_location_structure(data: dict, location: str) -> dict:
    """
    ç¡®ä¿æŒ‡å®šä½ç½®çš„æ•°æ®ç»“æ„å­˜åœ¨
    
    Args:
        data: å½“å‰çš„æ•°æ®ç»“æ„
        location: éœ€è¦ç¡®ä¿å­˜åœ¨çš„ä½ç½®
        
    Returns:
        dict: æ›´æ–°åçš„æ•°æ®ç»“æ„
    """
    if location not in data:
        data[location] = {"è¡¨æ ¼": {}, "æ–‡æ¡£": {}}
        print(f"ğŸ“ åˆ›å»ºæ–°ä½ç½®ç»“æ„: {location}")
    elif not isinstance(data[location], dict):
        data[location] = {"è¡¨æ ¼": {}, "æ–‡æ¡£": {}}
        print(f"ğŸ“ ä¿®å¤ä½ç½®ç»“æ„: {location}")
    else:
        if "è¡¨æ ¼" not in data[location]:
            data[location]["è¡¨æ ¼"] = {}
        if "æ–‡æ¡£" not in data[location]:
            data[location]["æ–‡æ¡£"] = {}
    
    return data

def check_file_exists_in_data(data: dict, file_name: str) -> bool:
    """
    æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨äºdata.jsonä¸­
    
    Args:
        data: data.jsonçš„æ•°æ®ç»“æ„
        file_name: æ–‡ä»¶å
        
    Returns:
        bool: æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    """
    for location in data.keys():
        if isinstance(data[location], dict):
            if file_name in data[location].get("è¡¨æ ¼", {}) or file_name in data[location].get("æ–‡æ¡£", {}):
                return True
    return False



def move_template_file_safely(source_file: str, dest_dir_name: str = "template_files") -> str:
        """
        Safely move a template file to the destination directory, handling existing files.
        
        This function ensures robust file handling by:
        - Creating the destination directory if it doesn't exist
        - Generating unique filenames when target files already exist (adds _1, _2, etc.)
        - Gracefully handling move errors and maintaining original path as fallback
        - Providing detailed logging of the process
        
        Args:
            source_file: Path to the source file to be moved
            dest_dir_name: Name of the destination directory under conversations/files/user_uploaded_files/
            
        Returns:
            str: Path to the final file location (new path if moved successfully, original path if failed)
        """
        try:
            dest_dir = Path(f"conversations/files/user_uploaded_files/{dest_dir_name}")
            dest_dir.mkdir(parents=True, exist_ok=True)
            
            source_file_path = Path(source_file)
            target_file_path = dest_dir / source_file_path.name
            
            # Handle existing file case by deleting old file
            if target_file_path.exists():
                print(f"âš ï¸ ç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨: {target_file_path.name}")
                try:
                    target_file_path.unlink()  # Delete the existing file
                    print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ—§æ–‡ä»¶: {target_file_path.name}")
                except Exception as delete_error:
                    print(f"âŒ åˆ é™¤æ—§æ–‡ä»¶å¤±è´¥: {delete_error}")
                    # If we can't delete the old file, we can't proceed
                    return source_file
            
            # Move the file
            source_file_path.rename(target_file_path)
            print(f"âœ… æ¨¡æ¿æ–‡ä»¶å·²ç§»åŠ¨åˆ°: {target_file_path}")
            return str(target_file_path)
            
        except Exception as move_error:
            print(f"âŒ ç§»åŠ¨æ¨¡æ¿æ–‡ä»¶å¤±è´¥: {move_error}")
            print(f"âš ï¸ ä¿æŒåŸå§‹æ–‡ä»¶è·¯å¾„: {source_file}")
            return source_file


def move_template_files_safely(processed_template_file: str, original_files_list: list[str], dest_dir_name: str = "template_files") -> dict[str, str]:
    """
    Safely move both processed and original template files to the template_files directory.
    
    This function handles moving both the processed template file (.txt) and its corresponding
    original file to the template_files folder, with proper error handling and logging.
    
    Args:
        processed_template_file: Path to the processed template file (.txt)
        original_files_list: List of original file paths to search for the corresponding original file
        dest_dir_name: Name of the destination directory under conversations/files/user_uploaded_files/
        
    Returns:
        dict: {
            "processed_template_path": str,  # Path to moved processed template file
            "original_template_path": str    # Path to moved original template file (or empty if not found)
        }
    """
    import shutil
    
    try:
        # Create destination directories
        dest_dir = Path(f"conversations/files/user_uploaded_files/{dest_dir_name}")
        dest_dir.mkdir(parents=True, exist_ok=True)
        
        # Create original_file subdirectory within template_files
        original_dest_dir = dest_dir / "original_file"
        original_dest_dir.mkdir(parents=True, exist_ok=True)
        
        processed_template_path = Path(processed_template_file)
        result = {
            "processed_template_path": "",
            "original_template_path": ""
        }
        
        print(f"ğŸ“ æ­£åœ¨ç§»åŠ¨æ¨¡æ¿æ–‡ä»¶: {processed_template_path.name}")
        
        # Move the processed template file
        processed_target_path = dest_dir / processed_template_path.name
        
        # Handle existing processed file
        if processed_target_path.exists():
            print(f"âš ï¸ å¤„ç†æ¨¡æ¿æ–‡ä»¶å·²å­˜åœ¨: {processed_target_path.name}")
            try:
                processed_target_path.unlink()
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ—§çš„å¤„ç†æ¨¡æ¿æ–‡ä»¶: {processed_target_path.name}")
            except Exception as delete_error:
                print(f"âŒ åˆ é™¤æ—§çš„å¤„ç†æ¨¡æ¿æ–‡ä»¶å¤±è´¥: {delete_error}")
                result["processed_template_path"] = processed_template_file
                return result
        
        # Move processed template file
        try:
            shutil.move(str(processed_template_path), str(processed_target_path))
            result["processed_template_path"] = str(processed_target_path)
            print(f"âœ… å¤„ç†æ¨¡æ¿æ–‡ä»¶å·²ç§»åŠ¨åˆ°: {processed_target_path}")
        except Exception as move_error:
            print(f"âŒ ç§»åŠ¨å¤„ç†æ¨¡æ¿æ–‡ä»¶å¤±è´¥: {move_error}")
            result["processed_template_path"] = processed_template_file
            return result
        
        # Find and move the corresponding original file
        template_file_stem = processed_template_path.stem
        original_file_found = False
        
        print(f"ğŸ” æ­£åœ¨å¯»æ‰¾å¯¹åº”çš„åŸå§‹æ¨¡æ¿æ–‡ä»¶: {template_file_stem}")
        
        for original_file in original_files_list:
            original_file_path = Path(original_file)
            if original_file_path.stem == template_file_stem:
                print(f"ğŸ“‹ æ‰¾åˆ°å¯¹åº”çš„åŸå§‹æ–‡ä»¶: {original_file_path.name}")
                
                # Move the original file to the original_file subdirectory
                original_target_path = original_dest_dir / original_file_path.name
                
                # Handle existing original file
                if original_target_path.exists():
                    print(f"âš ï¸ åŸå§‹æ¨¡æ¿æ–‡ä»¶å·²å­˜åœ¨: {original_target_path.name}")
                    try:
                        original_target_path.unlink()
                        print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ—§çš„åŸå§‹æ¨¡æ¿æ–‡ä»¶: {original_target_path.name}")
                    except Exception as delete_error:
                        print(f"âŒ åˆ é™¤æ—§çš„åŸå§‹æ¨¡æ¿æ–‡ä»¶å¤±è´¥: {delete_error}")
                        # Continue with moving even if deletion failed
                
                # Move original file
                try:
                    shutil.move(str(original_file_path), str(original_target_path))
                    result["original_template_path"] = str(original_target_path)
                    print(f"âœ… åŸå§‹æ¨¡æ¿æ–‡ä»¶å·²ç§»åŠ¨åˆ°: {original_target_path}")
                    original_file_found = True
                    break
                except Exception as move_error:
                    print(f"âŒ ç§»åŠ¨åŸå§‹æ¨¡æ¿æ–‡ä»¶å¤±è´¥: {move_error}")
                    # Continue searching for other matching files
        
        if not original_file_found:
            print(f"âš ï¸ æœªæ‰¾åˆ°å¯¹åº”çš„åŸå§‹æ¨¡æ¿æ–‡ä»¶: {template_file_stem}")
            result["original_template_path"] = ""
        
        return result
        
    except Exception as e:
        print(f"âŒ ç§»åŠ¨æ¨¡æ¿æ–‡ä»¶è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return {
            "processed_template_path": processed_template_file,
            "original_template_path": ""
        }

def convert_html_to_excel(html_file_path: str, output_dir: str = None) -> str:
    """
    Convert HTML file to Excel format using session-specific output directory
    
    Args:
        html_file_path: Path to the HTML file to convert
        output_dir: Output directory path (should be session-specific)
    
    Returns:
        str: Path to the converted Excel file
    """
    # Function implementation placeholder
    pass

def move_template_files_to_final_destination(processed_file_path: str, original_file_path: str, session_id: str) -> dict[str, str]:
    """Move template files from staging area to final destination.
    
    Destination: conversations/session_id/user_uploaded_files/template/
    
    Args:
        processed_file_path: Path to processed template file in staging area
        original_file_path: Path to original template file in staging area
        session_id: Session identifier
        
    Returns:
        dict: {
            "processed_template_path": str,  # Final path of processed template
            "original_template_path": str    # Final path of original template
        }
    """
    import shutil
    from pathlib import Path
    
    try:
        # Create destination directory
        project_root = Path.cwd()
        dest_dir = project_root / "conversations" / session_id / "user_uploaded_files" / "template"
        dest_dir.mkdir(parents=True, exist_ok=True)
        
        result = {
            "processed_template_path": "",
            "original_template_path": ""
        }
        
        # Move processed template file
        if processed_file_path and Path(processed_file_path).exists():
            processed_source = Path(processed_file_path)
            processed_target = dest_dir / processed_source.name
            
            # Handle existing file
            if processed_target.exists():
                print(f"âš ï¸ æ¨¡æ¿æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ­£åœ¨æ›´æ–°: {processed_target.name}")
                processed_target.unlink()
            
            shutil.move(str(processed_source), str(processed_target))
            result["processed_template_path"] = str(processed_target)
            print(f"âœ… æ¨¡æ¿æ–‡ä»¶å·²ç§»åŠ¨åˆ°: {processed_target}")
        
        # Move original template file
        if original_file_path and Path(original_file_path).exists():
            original_source = Path(original_file_path)
            original_target = dest_dir / original_source.name
            
            # Handle existing file
            if original_target.exists():
                print(f"âš ï¸ åŸå§‹æ¨¡æ¿æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ­£åœ¨æ›´æ–°: {original_target.name}")
                original_target.unlink()
            
            shutil.move(str(original_source), str(original_target))
            result["original_template_path"] = str(original_target)
            print(f"âœ… åŸå§‹æ¨¡æ¿æ–‡ä»¶å·²ç§»åŠ¨åˆ°: {original_target}")
        
        return result
        
    except Exception as e:
        print(f"âŒ ç§»åŠ¨æ¨¡æ¿æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return {
            "processed_template_path": processed_file_path,
            "original_template_path": original_file_path
        }



def move_supplement_files_to_final_destination(processed_file_path: str, original_file_path: str, file_type: str,
                                               village_name: str) -> dict[str, str]:
    """Move supplement files from staging area to final destination with simple override strategy.
    
    Destinations:
    - Table files: conversations/files/table_files/html_content/ and conversations/files/table_files/original/
    - Document files: conversations/files/document_files/txt_content/ and conversations/files/document_files/original/
    
    Args:
        processed_file_path: Path to processed supplement file in staging area
        original_file_path: Path to original supplement file in staging area
        file_type: Either "table" or "document"
        village_name: Name of the village
    Returns:
        dict: {
            "processed_supplement_path": str,  # Final path of processed supplement
            "original_supplement_path": str    # Final path of original supplement
        }
    """
    import shutil
    import stat
    from pathlib import Path
    
    def remove_readonly_and_delete(target_path: Path):
        """Remove read-only attribute and delete file"""
        try:
            if target_path.exists():
                # Remove read-only attribute
                target_path.chmod(stat.S_IWRITE | stat.S_IREAD)
                target_path.unlink()
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤åªè¯»æ–‡ä»¶: {target_path.name}")
        except Exception as e:
            print(f"âš ï¸ åˆ é™¤æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            raise
    
    try:
        # Determine destination based on file type
        project_root = Path.cwd()
        
        if file_type == "table":
            processed_content_dir = project_root / "files" / village_name / "table_files" / "html_content"
            original_dir = project_root / "files" / village_name / "table_files" / "original"
            screen_shot_dir = project_root / "files" / village_name / "table_files" / "screen_shot"
        elif file_type == "document":
            processed_content_dir = project_root / "files" / village_name / "document_files" / "txt_content"
            original_dir = project_root / "files" / village_name / "document_files" / "original"
        else:
            print(f"âŒ æ— æ•ˆçš„æ–‡ä»¶ç±»å‹: {file_type}")
            return {
                "processed_supplement_path": processed_file_path,
                "original_supplement_path": original_file_path
            }
        
        # Create destination directories
        processed_content_dir.mkdir(parents=True, exist_ok=True)
        original_dir.mkdir(parents=True, exist_ok=True)
        if file_type == "table":
            screen_shot_dir.mkdir(parents=True, exist_ok=True)

        result = {
            "processed_supplement_path": "",
            "original_supplement_path": ""
        }
        
        # Move processed supplement file
        if processed_file_path and Path(processed_file_path).exists():
            processed_source = Path(processed_file_path)
            processed_target = processed_content_dir / processed_source.name
            
            # Handle existing file with read-only attribute
            if processed_target.exists():
                print(f"âš ï¸ è¡¥å……æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ­£åœ¨è¦†ç›–: {processed_target.name}")
                remove_readonly_and_delete(processed_target)
            
            shutil.move(str(processed_source), str(processed_target))
            result["processed_supplement_path"] = str(processed_target)
            print(f"âœ… è¡¥å……æ–‡ä»¶å·²ç§»åŠ¨åˆ°: {processed_target}")
        
        # Move original supplement file
        if original_file_path and Path(original_file_path).exists():
            original_source = Path(original_file_path)
            original_target = original_dir / original_source.name
            
            # Handle existing file with read-only attribute
            if original_target.exists():
                print(f"âš ï¸ åŸå§‹è¡¥å……æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ­£åœ¨è¦†ç›–: {original_target.name}")
                remove_readonly_and_delete(original_target)
            
            shutil.move(str(original_source), str(original_target))
            result["original_supplement_path"] = str(original_target)
            print(f"âœ… åŸå§‹è¡¥å……æ–‡ä»¶å·²ç§»åŠ¨åˆ°: {original_target}")
        
        # Move screen shot file
        if file_type == "table" and screen_shot_dir and Path(screen_shot_dir).exists():
            screen_shot_source = original_source.with_suffix(".png")

            screen_shot_target = screen_shot_dir / screen_shot_source.name
            
            shutil.move(str(screen_shot_source), str(screen_shot_target))
            result["screen_shot_path"] = str(screen_shot_target)
            print(f"âœ… å±å¹•æˆªå›¾å·²ç§»åŠ¨åˆ°: {screen_shot_target}")

        return result
        
    except Exception as e:
        print(f"âŒ ç§»åŠ¨è¡¥å……æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return {
            "processed_supplement_path": processed_file_path,
            "original_supplement_path": original_file_path
        }


def delete_files_from_staging_area(file_paths: list[str]) -> dict[str, list[str]]:
    """Delete irrelevant files from staging area.
    
    Args:
        file_paths: List of file paths to delete
        
    Returns:
        dict: {
            "deleted_files": list[str],  # Successfully deleted files
            "failed_deletes": list[str]  # Files that failed to delete
        }
    """
    from pathlib import Path
    
    deleted_files = []
    failed_deletes = []
    
    for file_path in file_paths:
        try:
            file_to_delete = Path(file_path)
            if file_to_delete.exists():
                file_to_delete.unlink()
                deleted_files.append(str(file_to_delete))
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ— å…³æ–‡ä»¶: {file_to_delete.name}")
            else:
                print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ é™¤: {file_path}")
        except Exception as e:
            failed_deletes.append(file_path)
            print(f"âŒ åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
    
    print(f"ğŸ“Š åˆ é™¤ç»“æœ: æˆåŠŸ {len(deleted_files)} ä¸ªï¼Œå¤±è´¥ {len(failed_deletes)} ä¸ª")
    
    return {
        "deleted_files": deleted_files,
        "failed_deletes": failed_deletes
    }




def reconstruct_csv_with_headers(analysis_response: str, original_filename: str, 
                                 original_excel_file_path: str = None, village_name: str = None) -> str:
    """
    Reconstruct CSV file with headers using the analyzed table structure.
    
    Args:
        table_file_path: Path to the processed table file (.txt with HTML content)
        analysis_response: JSON response from LLM containing table structure
        original_filename: Original filename for the output CSV
        original_excel_file_path: Path to the original Excel file for CSV conversion
        village_name: Name of the village
    Returns:
        str: Path to the reconstructed CSV file
    """
    try:
        # Create output directory
        project_root = Path.cwd()
        csv_output_dir = project_root / "files" / village_name / "table_files" / "CSV_files"
        csv_output_dir.mkdir(parents=True, exist_ok=True)
        
        # Parse the analysis response to extract table structure
        try:
            if analysis_response.startswith('{') and analysis_response.endswith('}'):
                structure_data = json.loads(analysis_response)
            else:
                # Try to find JSON within the response
                import re
                json_match = re.search(r'\{.*\}', analysis_response, re.DOTALL)
                if json_match:
                    structure_data = json.loads(json_match.group())
                else:
                    raise ValueError("No valid JSON found in analysis response")
        except json.JSONDecodeError as e:
            print(f"âŒ è§£æè¡¨æ ¼ç»“æ„JSONå¤±è´¥: {e}")
            return ""
        
        # Extract the table structure from the first key (should be filename)
        table_key = list(structure_data.keys())[0]
        table_structure = structure_data[table_key].get("è¡¨æ ¼ç»“æ„", {})
        
        # Determine the Excel file path to use
        if original_excel_file_path and Path(original_excel_file_path).exists():
            excel_file_path = Path(original_excel_file_path)
        else:
            print("âŒ æœªæä¾›åŸå§‹Excelæ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶ä¸å­˜åœ¨")
            return ""
        
        # Convert the original Excel file to CSV using helper function
        temp_csv_path = csv_output_dir / f"temp_{excel_file_path.stem}.csv"
        
        # Import the helper function
        from utils.file_process import excel_to_csv
        
        try:
            # Use the existing helper function to convert Excel to CSV
            excel_to_csv(str(excel_file_path), str(temp_csv_path))
            print(f"ğŸ“Š Excelæ–‡ä»¶å·²è½¬æ¢ä¸ºCSV: {temp_csv_path}")
        except Exception as e:
            print(f"âŒ Excelè½¬CSVå¤±è´¥: {e}")
            return ""
        
        # Read the CSV data (skip header row)
        try:
            with open(temp_csv_path, 'r', encoding='utf-8') as f:
                csv_lines = f.readlines()
            
            # Skip the header row and get data rows
            print(f"è¿™æ˜¯æˆ‘ä»¬CSV_linesçš„å†…å®¹ï¼š\n{csv_lines}")
            data_rows = [line.strip() for line in csv_lines[2:] if line.strip()]
            print(f"è¿™æ˜¯æˆ‘ä»¬CSVçš„å†…å®¹strip è¡¨å¤´ï¼š\n{data_rows}")
            
            if not data_rows:
                print("âŒ CSVæ–‡ä»¶ä¸­æœªæ‰¾åˆ°æ•°æ®è¡Œ")
                return ""
            
            # Clean up temporary CSV file
            temp_csv_path.unlink()
            
        except Exception as e:
            print(f"âŒ è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
            return ""
        
        print(f"ğŸ“Š æå–åˆ° {len(data_rows)} è¡Œæ•°æ®")
        
        # Dynamically adjust chunking based on data size
        max_chunks = 15  # Maximum number of chunks we want to create
        total_rows = len(data_rows)
        
        if total_rows <= max_chunks:
            # If we have fewer rows than max chunks, create one chunk per row
            chunks = [[row] for row in data_rows]
            print(f"ğŸ“¦ æ•°æ®è¡Œæ•°({total_rows})å°äºç­‰äºæœ€å¤§åˆ†å—æ•°({max_chunks})ï¼Œåˆ›å»º {len(chunks)} ä¸ªå•è¡Œåˆ†å—")
        else:
            # If we have more rows than max chunks, distribute evenly
            chunk_size = max(1, total_rows // max_chunks)
            chunks = [data_rows[i:i + chunk_size] for i in range(0, total_rows, chunk_size)]
            print(f"ğŸ“¦ æ•°æ®è¡Œæ•°({total_rows})å¤§äºæœ€å¤§åˆ†å—æ•°({max_chunks})ï¼Œåˆ›å»º {len(chunks)} ä¸ªåˆ†å—ï¼Œæ¯å—çº¦ {chunk_size} è¡Œ")
        
        print(f"ğŸ“ æ•°æ®åˆ†ä¸º {len(chunks)} ä¸ªå—è¿›è¡Œå¤„ç†")
        
        # Process chunks with multi-threading
        def process_chunk(chunk_data: list, chunk_index: int) -> tuple[int, str]:
            """Process a single chunk with LLM"""
            try:
                # Validate chunk data - skip if empty or invalid
                valid_data = [row for row in chunk_data if row.strip() and ',' in row]
                if not valid_data:
                    print(f"âš ï¸ è·³è¿‡å— {chunk_index + 1} - æ— æœ‰æ•ˆæ•°æ®")
                    return chunk_index, ""
                
                print(f"ğŸ” å— {chunk_index + 1} åŒ…å«æœ‰æ•ˆæ•°æ®: {len(valid_data)} è¡Œ")
                
                system_prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è¡¨æ ¼ç»“æ„åˆ†æä¸æ•°æ®é‡æ„ä¸“å®¶ã€‚

ã€ä»»åŠ¡è¯´æ˜ã€‘
æˆ‘å°†ä¾æ¬¡æä¾›ä»¥ä¸‹ä¸¤éƒ¨åˆ†å†…å®¹ï¼š
1. è¡¨æ ¼çš„**ç»“æ„åŒ–è¡¨å¤´ä¿¡æ¯**ï¼Œé‡‡ç”¨å¦‚ä¸‹ JSON ç»“æ„æ ¼å¼ï¼š
{{
    "å­—æ®µå": {{
        "å€¼": [],      // è¯¥å­—æ®µæœ¬èº«åœ¨è¡¨æ ¼ä¸­å¯¹åº”çš„å•å…ƒæ ¼æ•°æ®ï¼Œå¯èƒ½ä¸ºç©ºæ•°ç»„
        "åˆ†è§£": {{      // è¯¥å­—æ®µä¸‹çš„å­åˆ†ç±»æˆ–å­å­—æ®µï¼Œå¯èƒ½ä¸ºç©ºå¯¹è±¡
            "å­å­—æ®µ1": [],
            "å­å­—æ®µ2": []
        }},
        "è§„åˆ™": "å­å­—æ®µ1 + å­å­—æ®µ2"  // å¦‚æœè¯¥å­—æ®µçš„å€¼ç­‰äºå­å­—æ®µçš„åŠ æ€»æˆ–å…¶ä»–è§„åˆ™ï¼Œè¯·è‡ªåŠ¨æ¨ç†å¹¶å†™æ˜è§„åˆ™ï¼›æ— è§„å¾‹åˆ™ä¸ºç©ºå­—ç¬¦ä¸²ã€‚
    }}
}}

è¯´æ˜ï¼š
- **é‡è¦**ï¼šæ‰€æœ‰åœ¨ JSON ç»“æ„ä¸­å‡ºç°çš„å­—æ®µåï¼ˆæ— è®ºæ˜¯çˆ¶å­—æ®µè¿˜æ˜¯å­å­—æ®µï¼‰ï¼Œéƒ½åº”è¯¥ä½œä¸º CSV çš„åˆ—è¾“å‡ºï¼›
- "å€¼"å­—æ®µä»…ç”¨äºç†è§£å­—æ®µå«ä¹‰å’Œè®¡ç®—å…³ç³»ï¼Œ**ä¸ç”¨äºåˆ¤æ–­è¯¥å­—æ®µæ˜¯å¦åº”è¯¥å‡ºç°åœ¨ CSV ä¸­**ï¼›
- æœ€ç»ˆ CSV è¡¨å¤´ç”Ÿæˆè§„åˆ™ï¼š
  1) **æ‰€æœ‰çˆ¶å­—æ®µ**ï¼šå¦‚æœæœ‰"åˆ†è§£"ï¼Œçˆ¶å­—æ®µæœ¬èº«ä¹Ÿè¦ä½œä¸ºä¸€åˆ—ï¼ˆå¦‚"ä¿éšœäººæ•°"ï¼‰
  2) **æ‰€æœ‰å­å­—æ®µ**ï¼šåœ¨"åˆ†è§£"ä¸­çš„æ‰€æœ‰å­—æ®µéƒ½è¦ä½œä¸ºåˆ—ï¼ˆå¦‚"é‡ç‚¹ä¿éšœäººæ•°"ã€"æ®‹ç–¾äººæ•°"ï¼‰
  3) **å•ç‹¬å­—æ®µ**ï¼šæ²¡æœ‰"åˆ†è§£"çš„å­—æ®µç›´æ¥ä½œä¸ºåˆ—ï¼ˆå¦‚"åºå·"ã€"æˆ·ä¸»å§“å"ï¼‰
- è¡¨å¤´é¡ºåºæŒ‰ JSON ä¸­çš„è‡ªç„¶æ’åˆ—é¡ºåºï¼Œå…ˆçˆ¶å­—æ®µï¼Œå†å…¶å­å­—æ®µï¼›
- å¿½ç•¥æ‰€æœ‰éæ­£å¸¸æ•°æ®ï¼ˆå¦‚åªæœ‰è¡¨å¤´æ— æ•°æ®ã€è¡¨å°¾åˆè®¡è¡Œã€æ•°æ®ç¼ºå¤±æˆ–æ˜æ˜¾è¡¥å…¨ç­‰ï¼‰ï¼›
- ä¸¥æ ¼åªè¾“å‡ºæœ‰æ•ˆä¸”å®Œæ•´çš„æ•°æ®åŠå…¶å¯¹åº”çš„è¡¨å¤´ã€‚

2. ä¸€ç»„å¯¹åº”è¡¨å¤´çš„**CSVæ•°æ®è¡Œ**ã€‚

ã€å¤„ç†æ€è·¯ - Chain of Thoughtã€‘
- è¾“å…¥çš„ CSV æ–‡ä»¶å¯èƒ½åŒ…å« Excel è½¬æ¢æ—¶è‡ªå¸¦çš„è¡¨å¤´è¡Œï¼Œè¯¥è¡¨å¤´è¡Œåº”**ä¸¥æ ¼å¿½ç•¥ï¼Œä¸ä½œä¸ºæ•°æ®å¤„ç†å¯¹è±¡**ï¼›
- é¦–å…ˆï¼Œæ£€æŸ¥æ•°æ®å®Œæ•´æ€§å’Œæœ‰æ•ˆæ€§ï¼Œæ’é™¤ç©ºè¡Œã€ä»…è¡¨å¤´è¡Œï¼ˆå« Excel å¯¼å…¥çš„è¡¨å¤´ï¼‰ã€é‡å¤è¡¨å¤´ã€åˆè®¡åŠè¡¥å…¨æ•°æ®ç­‰æ— æ•ˆå†…å®¹ï¼›
- è§£æè¡¨å¤´ JSONï¼Œæå–æ‰€æœ‰å­—æ®µåï¼ˆåŒ…æ‹¬çˆ¶å­—æ®µå’Œå­å­—æ®µï¼‰ï¼Œå½¢æˆæœ€ç»ˆæœ€å®Œæ•´çš„è¡¨å¤´å­—æ®µåˆ—è¡¨ï¼›
- å¯¹æ¯è¡Œæ•°æ®ï¼Œä¸¥æ ¼åˆ¤æ–­åˆ—æ•°ä¸è¡¨å¤´å­—æ®µæ•°åŒ¹é…ä¸”åŒ…å«æœ‰æ•ˆä¸šåŠ¡ä¿¡æ¯ï¼ˆéç©ºéåˆè®¡ç­‰ï¼‰ï¼›
- é‡ç‚¹ï¼šæ¯æ¡æ•°æ®è¡Œ**å‰å‡å¿…é¡»è¾“å‡ºå¯¹åº”å®Œæ•´è¡¨å¤´è¡Œ**ï¼Œä¸å…è®¸åªç”Ÿæˆä¸€æ¬¡è¡¨å¤´è¡Œï¼›
- å¦‚é‡ä»»ä½•ä¸æ»¡è¶³æ¡ä»¶çš„æƒ…å†µï¼ˆæ— æ•ˆæ•°æ®ã€ç¼ºå¤±å­—æ®µã€è¡Œæ•°ä¸è¶³ç­‰ï¼‰ï¼Œ**ä¸¥æ ¼è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œä¸è¾“å‡ºä»»ä½•å†…å®¹**ã€‚

ã€ğŸš¨ å…³é”®è§„åˆ™ - å¿…é¡»ä¸¥æ ¼éµå®ˆã€‘
é‡ä»¥ä¸‹æƒ…å†µæ—¶ç»å¯¹ä¸è¾“å‡ºä»»ä½•å†…å®¹ï¼ˆåŒ…æ‹¬è§£é‡Šã€æ³¨é‡Šç­‰ï¼‰ï¼š
- æ•°æ®å—æ— å®é™…æ•°æ®ï¼Œä»…æ˜¯è¡¨å¤´è¡Œæˆ–è€…ç©ºè¡Œï¼ˆåŒ…æ‹¬ Excel å¯¼å…¥çš„è¡¨å¤´è¡Œï¼‰ï¼›
- æ•°æ®æ˜æ˜¾è¡¥å…¨ã€ä¸å®Œæ•´ã€æ ¼å¼é”™è¯¯ï¼›
- æ•°æ®åŒ…å«åˆè®¡ã€æ€»è®¡ã€ç»Ÿè®¡ä¹‹ç±»éä¸šåŠ¡æ­£å¸¸æ•°æ®è¡Œï¼›
- æ•°æ®è¡Œæ•°å°‘äºæœ€ç»ˆè¡¨å¤´å­—æ®µæ•°ï¼›
- æ•°æ®æ ¼å¼ä¸ç¬¦åˆ CSV æ ‡å‡†ã€‚

âš ï¸ ä¸¥ç¦è¾“å‡ºä»»ä½•è§£é‡Šæ€§æ–‡å­—ï¼Œç›´æ¥ç©ºç™½è¿”å›ã€‚

âœ… ä»…å½“æ»¡è¶³å®Œæ•´ä¸”æœ‰æ•ˆæ•°æ®æ—¶ï¼Œæ‰è¾“å‡ºæ ¼å¼åŒ–çš„â€œè¡¨å¤´è¡Œ+æ•°æ®è¡Œâ€å¯¹ï¼Œä¸”æ­¤å¯¹é¡»ä¸ºæ¯æ¡æ•°æ®é‡å¤è¾“å‡ºã€‚

ã€è¾“å‡ºè¦æ±‚ã€‘
- æ¯æ¡æ•°æ®è¡Œä¹‹å‰å¿…é¡»æœ‰å¯¹åº”çš„å®Œæ•´è¡¨å¤´è¡Œï¼›
- è¡¨å¤´é¡ºåºä¸¥æ ¼å¯¹åº” JSON ä¸­æå–çš„æœ€åº•å±‚åŠæœ‰â€œå€¼â€å­—æ®µé¡ºåºï¼›
- çº¯å‡€ CSV æ ¼å¼ï¼Œé€—å·åˆ†å‰²ï¼Œæ¯è¡Œä»¥æ¢è¡Œç¬¦ç»“å°¾ï¼›
- ä¸å¾—æ·»åŠ ä»»ä½•é CSV å†…å®¹ã€‚

ã€åˆ¤æ–­æœ‰æ•ˆæ•°æ®è¡Œæ ‡å‡†ã€‘
æœ‰æ•ˆæ•°æ®è¡Œå¿…é¡»åŒ…å«å®é™…ä¸šåŠ¡æ•°æ®ï¼ˆå¦‚å§“åã€æ•°å­—ã€æ—¥æœŸç­‰ï¼‰ï¼Œä¸”æ•°æ®å®Œæ•´ä¸”æ­£ç¡®ï¼›
æ— æ•ˆå¦‚ç©ºè¡Œã€åˆè®¡è¡Œã€ä»…å­—æ®µåã€é‡å¤è¡¨å¤´ï¼ˆåŒ…å« Excel å¯¼å…¥è¡¨å¤´ï¼‰ç­‰å¿…é¡»å¿½ç•¥ã€‚

ã€ç¤ºä¾‹ã€‘
{{  
    "é›†å›¢": {{
        "å€¼": [],           // çˆ¶å­—æ®µå€¼ä¸ºç©ºï¼Œä½†ä»ä½œä¸ºä¸€åˆ—è¾“å‡º
        "åˆ†è§£": {{
            "äº§å“": [],      // å­å­—æ®µä½œä¸ºåˆ—è¾“å‡º
            "é”€å”®é¢": []     // å­å­—æ®µä½œä¸ºåˆ—è¾“å‡º
        }},
        "è§„åˆ™": ""
    }},
    "å¤‡æ³¨": []              // å•ç‹¬å­—æ®µç›´æ¥ä½œä¸ºåˆ—è¾“å‡º
}}
åˆ™æœ€ç»ˆè¡¨å¤´ä¸ºï¼š
é›†å›¢,äº§å“,é”€å”®é¢,å¤‡æ³¨

ã€ğŸ”¥ æœ€é‡è¦çš„è¦æ±‚ã€‘
æ— æ•ˆæƒ…å†µç»å¯¹é™é»˜ä¸è¾“å‡ºï¼›
åªåœ¨æ•°æ®å®Œæ•´æœ‰æ•ˆæ—¶è¾“å‡º CSV æ ¼å¼ç»“æœï¼›
æ¯æ¡æ•°æ®å‡é¡»å¸¦å…¶å®Œæ•´è¡¨å¤´è¡Œä½œä¸ºå‰ä¸€è¡Œï¼›
æ— ä»»ä½•è§£é‡Šã€æ³¨é‡Šã€æˆ–é CSV å†…å®¹ã€‚

è¯·ä¸¥æ ¼æŒ‰æ­¤æ€è·¯å’Œè§„åˆ™æ‰§è¡Œï¼Œä»”ç»†æ€è€ƒå¹¶éµå®ˆ chain of thoughtï¼Œç¡®ä¿ç»“æœå®Œå…¨ç¬¦åˆè¦æ±‚ã€‚

"""
                
                # Prepare input for this chunk using validated data
                chunk_input = f"""
=== è¡¨æ ¼ç»“æ„ ===
{json.dumps(structure_data, ensure_ascii=False, indent=2)}

=== CSVæ•°æ® ===
{chr(10).join(valid_data)}
"""
                
                print(f"ğŸ“¤ å¤„ç†å— {chunk_index + 1} (åŸå§‹: {len(chunk_data)} è¡Œ, æœ‰æ•ˆ: {len(valid_data)} è¡Œ)")
                print(f"ğŸ” é‡æ„CSVè¾“å…¥æ•°æ®å—å†…å®¹\n: {chunk_input}") 
                # Call LLM
                response = invoke_model(
                    model_name="Pro/deepseek-ai/DeepSeek-V3",
                    messages=[SystemMessage(content=system_prompt), HumanMessage(content=chunk_input)],
                    temperature=0.2,
                    silent_mode=True
                )
                
                print(f"ğŸ“¥ å— {chunk_index + 1} å¤„ç†å®Œæˆ")
                return chunk_index, response
                
            except Exception as e:
                print(f"âŒ å¤„ç†å— {chunk_index + 1} å¤±è´¥: {e}")
                return chunk_index, ""
        
        # Process all chunks in parallel
        chunk_results = {}
        max_workers = min(len(chunks), 15)  # Dynamically adjust workers based on actual chunk count
        print(f"ğŸ‘¥ ä½¿ç”¨ {max_workers} ä¸ªå¹¶å‘å·¥ä½œè€…å¤„ç† {len(chunks)} ä¸ªæ•°æ®å—")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_chunk = {
                executor.submit(process_chunk, chunk, i): i 
                for i, chunk in enumerate(chunks)
            }
            
            for future in as_completed(future_to_chunk):
                chunk_index = future_to_chunk[future]
                try:
                    idx, result = future.result()
                    chunk_results[idx] = result
                except Exception as e:
                    print(f"âŒ å— {chunk_index} å¤„ç†å‡ºé”™: {e}")
                    chunk_results[chunk_index] = ""
        
        # Combine results in order, filtering out empty results
        combined_csv = []
        for i in range(len(chunks)):
            if i in chunk_results and chunk_results[i] and chunk_results[i].strip():
                combined_csv.append(chunk_results[i])
                print(f"âœ… æ·»åŠ å— {i + 1} çš„ç»“æœåˆ°æœ€ç»ˆCSV")
        
        # Join all chunks
        final_csv_content = '\n'.join(combined_csv)
        
        # Apply comprehensive error message cleaning
        final_csv_content = clean_llm_error_messages(final_csv_content)
        
        # Save to CSV file
        csv_filename = Path(original_filename).stem + ".csv"
        csv_output_path = csv_output_dir / csv_filename
        print("è¿™æ˜¯æˆ‘ä»¬CSVçš„å†…å®¹ï¼š\n", final_csv_content)
        with open(csv_output_path, 'w', encoding='utf-8', newline='') as f:
            f.write(final_csv_content)
        
        print(f"ğŸ’¾ é‡æ„çš„CSVæ–‡ä»¶å·²ä¿å­˜: {csv_output_path}")
        return str(csv_output_path)
        
    except Exception as e:
        print(f"âŒ CSVé‡æ„è¿‡ç¨‹å‡ºé”™: {e}")
        return ""
    

def extract_summary_for_each_file(file_content: dict) -> str:
            """æå–æ–‡ä»¶å†…å®¹çš„æ‘˜è¦ä¿¡æ¯"""
            summary = ""
            
            # æå–è¡¨æ ¼summary
            if "è¡¨æ ¼" in file_content and file_content["è¡¨æ ¼"]:
                summary += "è¡¨æ ¼: \n"
                tables = file_content["è¡¨æ ¼"]
                for table_name in tables:
                    if isinstance(tables[table_name], dict) and "summary" in tables[table_name]:
                        summary += f"  {tables[table_name]['summary']}\n"
                    else:
                        summary += f"  {table_name}: [æ— æ‘˜è¦ä¿¡æ¯]\n"
            
            # æå–æ–‡æ¡£summary
            if "æ–‡æ¡£" in file_content and file_content["æ–‡æ¡£"]:
                summary += "\næ–‡æ¡£: \n"
                documents = file_content["æ–‡æ¡£"]
                for doc_name in documents:
                    if isinstance(documents[doc_name], dict) and "summary" in documents[doc_name]:
                        summary += f"  {documents[doc_name]['summary']}\n"
                    else:
                        summary += f"  {doc_name}: [æ— æ‘˜è¦ä¿¡æ¯]\n"
            
            return summary

def clean_llm_error_messages(csv_content: str) -> str:
    """
    Clean LLM error messages and artifacts from CSV content.
    
    This function removes common LLM error messages, thinking process artifacts,
    and other non-CSV content that might contaminate the output.
    
    Args:
        csv_content: Raw CSV content string that may contain error messages
        
    Returns:
        str: Cleaned CSV content with error messages removed
    """
    if not csv_content or not isinstance(csv_content, str):
        return ""
    
    # Common LLM error messages and artifacts to remove
    error_patterns = [
        # Chinese error messages
        r'ï¼ˆä»€ä¹ˆéƒ½ä¸è¾“å‡ºï¼Œå®Œå…¨ç©ºç™½ï¼‰',
        r'ï¼ˆä»€ä¹ˆéƒ½ä¸è¾“å‡ºï¼Œå®Œå…¨ç©ºç™½ï¼‰',
        r'å®Œå…¨é™é»˜',
        r'ä»€ä¹ˆéƒ½ä¸è¾“å‡º',
        r'æ ¹æ®è§„åˆ™.*ä¸è¾“å‡º',
        r'æ•°æ®ä¸å®Œæ•´.*è·³è¿‡',
        r'æ— æœ‰æ•ˆæ•°æ®.*è·³è¿‡',
        r'æ ¹æ®.*è§„åˆ™.*é™é»˜',
        r'æ²¡æœ‰.*æ•°æ®.*è¾“å‡º',
        r'æ•°æ®æ ¼å¼.*é”™è¯¯',
        r'æ— æ³•.*å¤„ç†.*è·³è¿‡',
        r'é‡åˆ°.*æƒ…å†µ.*é™é»˜',
        r'æŒ‰ç…§.*è¦æ±‚.*ä¸è¾“å‡º',
        
        # English error messages
        r'(?i)no output',
        r'(?i)silent mode',
        r'(?i)skip.*empty.*data',
        r'(?i)invalid.*data.*format',
        r'(?i)incomplete.*data.*skip',
        r'(?i)according.*rules.*silent',
        r'(?i)data.*incomplete.*skip',
        r'(?i)no.*valid.*data',
        r'(?i)error.*processing.*skip',
        r'(?i)cannot.*process.*skip',
        
        # Thinking process artifacts
        r'=== æ¨ç†è¿‡ç¨‹ ===',
        r'=== æ€è€ƒè¿‡ç¨‹ ===',
        r'=== åˆ†æè¿‡ç¨‹ ===',
        r'=== å¤„ç†è¿‡ç¨‹ ===',
        r'=== æœ€ç»ˆç­”æ¡ˆ ===',
        r'=== ç»“æœ ===',
        r'=== THINKING ===',
        r'=== ANALYSIS ===',
        r'=== RESULT ===',
        r'=== FINAL ANSWER ===',
        
        # Processing status messages
        r'æ­£åœ¨å¤„ç†.*',
        r'å¤„ç†å®Œæˆ.*',
        r'å¼€å§‹å¤„ç†.*',
        r'è·³è¿‡.*è¡Œ',
        r'æ·»åŠ .*ç»“æœ',
        r'ç”Ÿæˆ.*æ•°æ®',
        r'Processing.*',
        r'Completed.*',
        r'Starting.*',
        r'Skipping.*',
        r'Adding.*result',
        r'Generated.*data',
        
        # Markdown artifacts
        r'```csv',
        r'```',
        r'```.*',
        
        # Other common artifacts
        r'æ•°æ®å—.*å¤„ç†.*å¼‚å¸¸',
        r'Error.*processing.*chunk',
        r'Failed.*to.*process',
        r'å¤„ç†å¤±è´¥.*',
        r'å¼‚å¸¸.*å¤„ç†',
        r'é”™è¯¯.*è·³è¿‡',
        r'Warning.*skip',
        r'âš ï¸.*',
        r'âŒ.*',
        r'âœ….*',
        r'ğŸ”.*',
        r'ğŸ“Š.*',
        r'ğŸ‰.*',
        r'ğŸ’¾.*',
        r'ğŸ“„.*',
        r'ğŸš€.*',
        r'ğŸ”„.*',
        r'âš¡.*',
        r'ğŸ“‹.*',
        r'ğŸ“¤.*',
        r'ğŸ“¥.*',
        r'ğŸ”§.*',
        r'ğŸ› ï¸.*',
        r'ğŸ”¬.*',
        r'ğŸ¯.*',
        r'ğŸ’¡.*',
        r'â­.*',
        r'ğŸª.*',
        r'ğŸ¨.*',
        r'ğŸ­.*',
        r'ğŸŒŸ.*',
        r'ğŸ”¥.*',
        r'ğŸ’ª.*',
        r'ğŸš¨.*',
        r'âš ï¸.*',
        r'â—.*',
        r'â€¼ï¸.*',
        r'ğŸ’¯.*',
        r'ğŸŠ.*',
        r'ğŸˆ.*',
        r'ğŸ.*',
        r'ğŸ€.*',
        r'ğŸ‚.*',
        r'ğŸ°.*',
        r'ğŸƒ.*',
        r'ğŸ„.*',
        r'ğŸ†.*',
        r'ğŸ‡.*',
        r'ğŸ§¨.*',
        r'âœ¨.*',
        r'ğŸ‰.*',
        r'ğŸŠ.*',
        r'ğŸˆ.*',
        r'ğŸ.*',
        r'ğŸ€.*',
        r'ğŸ‚.*',
        r'ğŸ°.*',
        r'ğŸƒ.*',
        r'ğŸ„.*',
        r'ğŸ†.*',
        r'ğŸ‡.*',
        r'ğŸ§¨.*',
        r'âœ¨.*',
    ]
    
    # Split content into lines for processing
    lines = csv_content.split('\n')
    cleaned_lines = []
    
    for line in lines:
        line = line.strip()
        
        # Skip empty lines
        if not line:
            continue
        
        # Check if line matches any error pattern
        is_error_line = False
        for pattern in error_patterns:
            if re.search(pattern, line):
                is_error_line = True
                break
        
        # Skip error lines
        if is_error_line:
            continue
        
        # Additional checks for valid CSV lines
        if is_valid_csv_line(line):
            cleaned_lines.append(line)
    
    # Join cleaned lines
    cleaned_content = '\n'.join(cleaned_lines)
    
    # Additional cleanup for any remaining artifacts
    cleaned_content = re.sub(r'\n\s*\n', '\n', cleaned_content)  # Remove extra blank lines
    cleaned_content = cleaned_content.strip()
    
    return cleaned_content

def is_valid_csv_line(line: str) -> bool:
    """
    Check if a line is a valid CSV line.
    
    Args:
        line: Line to check
        
    Returns:
        bool: True if line appears to be valid CSV data
    """
    if not line or not isinstance(line, str):
        return False
    
    line = line.strip()
    
    # Skip obviously invalid lines
    if not line:
        return False
    
    # Skip lines that are pure symbols or decorations
    if re.match(r'^[=\-\*\+\s]+$', line):
        return False
    
    # Skip lines that are just section headers
    if line.startswith('===') and line.endswith('==='):
        return False
    
    # Skip lines that are just markdown
    if line.startswith('```') or line == '```':
        return False
    
    # Skip lines that are just comments or explanations
    if line.startswith('#') or line.startswith('//'):
        return False
    
    # Skip lines that are clearly status messages
    if any(keyword in line.lower() for keyword in ['processing', 'å¤„ç†', 'error', 'é”™è¯¯', 'skip', 'è·³è¿‡', 'warning', 'è­¦å‘Š']):
        return False
    
    # Check if line contains commas (basic CSV structure)
    if ',' not in line:
        return False
    
    # Check if line has reasonable structure
    parts = line.split(',')
    if len(parts) < 2:  # Should have at least 2 columns
        return False
    
    # Check if all parts are just empty or whitespace
    if all(not part.strip() for part in parts):
        return False
    
    return True


