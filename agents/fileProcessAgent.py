import sys
from pathlib import Path

# Add root project directory to sys.path
sys.path.append(str(Path(__file__).resolve().parent.parent))

from dotenv import load_dotenv
load_dotenv()


from typing import Dict, List, Optional, Any, TypedDict, Annotated
from datetime import datetime
from utils.modelRelated import invoke_model, invoke_model_with_screenshot
from utils.file_process import (retrieve_file_content, save_original_file,
                                    extract_filename, 
                                    ensure_location_structure, check_file_exists_in_data,
                                    get_available_locations, move_template_files_to_final_destination,
                                    move_supplement_files_to_final_destination, delete_files_from_staging_area,
                                    reconstruct_csv_with_headers, detect_and_process_file_paths)

import json

from langgraph.graph import StateGraph, END, START
from langgraph.constants import Send
# from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage
from langchain_core.tools import tool


class FileProcessState(TypedDict):
    session_id: str
    upload_files_path: list[str] # Store all uploaded files
    new_upload_files_path: list[str] # Track the new uploaded files in this round
    new_upload_files_processed_path: list[str] # Store the processed new uploaded files
    original_files_path: list[str] # Store the original files in original_file subfolder
    table_files_path: list[str]
    irrelevant_files_path: list[str]
    irrelevant_original_files_path: list[str] # Track original files to be deleted with irrelevant files
    all_files_irrelevant: bool  # Flag to indicate all files are irrelevant
    village_name: str


class FileProcessAgent:


    def __init__(self):
        self.memory = MemorySaver()
        self.graph = self._build_graph().compile(checkpointer=self.memory)

    def _build_graph(self):
        graph = StateGraph(FileProcessState)

        graph.add_node("file_upload", self._file_upload)
        graph.add_node("analyze_uploaded_files", self._analyze_uploaded_files)
        graph.add_node("route_after_analyze_uploaded_files", self._route_after_analyze_uploaded_files)
        graph.add_node("process_table", self._process_table)
        graph.add_node("process_irrelevant", self._process_irrelevant)
        graph.add_node("summary_file_upload", self._summary_file_upload)

        graph.add_edge(START, "file_upload")
        graph.add_edge("file_upload", "analyze_uploaded_files")
        graph.add_conditional_edges("analyze_uploaded_files", self._route_after_analyze_uploaded_files)
        graph.add_edge("process_table", "summary_file_upload")
        graph.add_edge("process_irrelevant", "summary_file_upload")
        graph.add_edge("summary_file_upload", END)

        return graph

    def _create_initial_state(self, session_id: str = "1", upload_files_path: list[str] = [], village_name: str = "") -> FileProcessState:
        return {
            "session_id": session_id,
            "upload_files_path": upload_files_path,
            "new_upload_files_path": [],
            "new_upload_files_processed_path": [],
            "original_files_path": [],
            "uploaded_template_files_path": [],
            "table_files_path": [],
            "irrelevant_files_path": [],
            "irrelevant_original_files_path": [],
            "all_files_irrelevant": False,
            "template_complexity": "",
            "village_name": village_name
        }


    def _file_upload(self, state: FileProcessState) -> FileProcessState:
            """This node will upload user's file to our system"""
            print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _file_upload")
            print("=" * 50)
            
            print("ğŸ“ æ­£åœ¨æ£€æµ‹ç”¨æˆ·è¾“å…¥ä¸­çš„æ–‡ä»¶è·¯å¾„...")
            detected_files = state["upload_files_path"]
            print(f"ğŸ“‹ æ£€æµ‹åˆ° {len(detected_files)} ä¸ªæ–‡ä»¶")
            
            # Load data.json with error handling
            data_file = Path("agents/data.json")
            try:
                with open(data_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
            except (FileNotFoundError, json.JSONDecodeError) as e:
                print(f"âš ï¸ data.jsonæ–‡ä»¶å‡ºé”™: {e}")
                # Initialize empty structure if file is missing or corrupted
                data = {}
            
            print("ğŸ” æ­£åœ¨æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨...")
            files_to_remove = []
            for file in detected_files:
                file_name = Path(file).name
                if check_file_exists_in_data(data, file_name):
                    files_to_remove.append(file)
                    print(f"âš ï¸ æ–‡ä»¶ {file} å·²å­˜åœ¨")
            
            # Remove existing files from detected_files
            for file in files_to_remove:
                detected_files.remove(file)
            
            if not detected_files:
                print("âš ï¸ æ²¡æœ‰æ–°æ–‡ä»¶éœ€è¦ä¸Šä¼ ")
                print("âœ… _file_upload æ‰§è¡Œå®Œæˆ")
                print("=" * 50)
                return {
                    "new_upload_files_path": [],
                    "new_upload_files_processed_path": []
                }
            
            print(f"ğŸ”„ æ­£åœ¨å¤„ç† {len(detected_files)} ä¸ªæ–°æ–‡ä»¶...")
            
            # Create staging area for original files
            project_root = Path.cwd()
            staging_dir = project_root / "conversations" / state["session_id"] / "user_uploaded_files"
            staging_dir.mkdir(parents=True, exist_ok=True)
            
            # Process the files to get .txt versions
            processed_files = retrieve_file_content(detected_files, state["session_id"])
            
            # Save original files separately
            original_files = []
            for file_path in detected_files:
                try:
                    source_path = Path(file_path)
                    original_file_saved_path = save_original_file(source_path, staging_dir)
                    if original_file_saved_path:
                        original_files.append(original_file_saved_path)
                        print(f"ğŸ’¾ åŸå§‹æ–‡ä»¶å·²ä¿å­˜: {Path(original_file_saved_path).name}")
                    else:
                        print(f"âš ï¸ åŸå§‹æ–‡ä»¶ä¿å­˜å¤±è´¥: {source_path.name}")
                except Exception as e:
                    print(f"âŒ ä¿å­˜åŸå§‹æ–‡ä»¶æ—¶å‡ºé”™ {file_path}: {e}")
            
            print(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {len(processed_files)} ä¸ªå¤„ç†æ–‡ä»¶, {len(original_files)} ä¸ªåŸå§‹æ–‡ä»¶")
            print("âœ… _file_upload æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            
            # Update state with new files
            # Safely handle the case where upload_files_path might not exist in state
            existing_files = state.get("upload_files_path", [])
            existing_original_files = state.get("original_files_path", [])
            print("detected_files ç±»å‹: ", type(detected_files))
            print("existing_files ç±»å‹: ", type(existing_files))
            print("existing_original_files ç±»å‹: ", type(existing_original_files))
            print("processed_files ç±»å‹: ", type(processed_files))
            print("original_files ç±»å‹: ", type(original_files))
            return {
                "new_upload_files_path": detected_files,
                "upload_files_path": existing_files + detected_files,
                "new_upload_files_processed_path": processed_files,
                "original_files_path": existing_original_files + original_files
            }
    


    def _analyze_uploaded_files(self, state: FileProcessState) -> FileProcessState:
        """This node will analyze the user's uploaded files, it need to classify the file into template
        supplement, or irrelevant. If all files are irrelevant, it will flag for text analysis instead."""
        
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _analyze_uploaded_files")
        print("=" * 50)
        
        import json
        from pathlib import Path
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        # Initialize classification results
        classification_results = {
            "tables": [],
            "irrelevant": []
        }
        
        # Process files one by one for better accuracy
        processed_files = []
        # Safely handle the case where new_upload_files_processed_path might not exist in state
        new_files_to_process = state.get("new_upload_files_processed_path", [])
        
        print(f"ğŸ“ éœ€è¦åˆ†æçš„æ–‡ä»¶æ•°é‡: {len(new_files_to_process)}")
        
        if not new_files_to_process:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶")
            print("âœ… _analyze_uploaded_files æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            return {
                "table_files_path": [],
                "irrelevant_files_path": [],
                "all_files_irrelevant": True  # Flag for routing to text analysis
            }
        
        def analyze_single_file(file_path: str) -> tuple[str, str, str]:
            """Analyze a single file and return (file_path, classification, file_name)"""   
            try:
                source_path = Path(file_path)
                print(f"ğŸ” æ­£åœ¨åˆ†ææ–‡ä»¶: {source_path.name}")
                
                if not source_path.exists():
                    print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                    return file_path, "irrelevant", source_path.name
                
                # Read file content for analysis
                file_content = source_path.read_text(encoding='utf-8')
                # Truncate content for analysis (to avoid token limits)
                analysis_content = file_content[:5000] if len(file_content) > 2000 else file_content
                
                # Create individual analysis prompt for this file
                system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªè¡¨æ ¼ç”Ÿæˆæ™ºèƒ½ä½“ï¼Œéœ€è¦åˆ†æç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶å†…å®¹æ˜¯ä¸æ˜¯ä¸€ä¸ªåŒ…å«æœ‰æ•ˆæ•°æ®é›†çš„è¡¨æ ¼æ–‡ä»¶ï¼Œæœ€ç›´è§‚çš„æ˜¯ç”¨æˆ·ä¸Šä¼ äº†ä¸€ä¸ªexcelè¡¨æ ¼ï¼Œå¹¶ä¸”å¹¶éæ¨¡æ¿è¡¨æ ¼ï¼Œé‡Œé¢æœ‰å…·ä½“çš„æ•°æ®ï¼Œæ­¤æ—¶å°†æ–‡ä»¶

                ä»”ç»†æ£€æŸ¥ä¸è¦æŠŠè¡¥å……æ–‡ä»¶é”™è¯¯åˆ’åˆ†ä¸ºæ¨¡æ¿æ–‡ä»¶åä¹‹äº¦ç„¶ï¼Œè¡¥å……æ–‡ä»¶é‡Œé¢æ˜¯æœ‰æ•°æ®çš„ï¼Œæ¨¡æ¿æ–‡ä»¶é‡Œé¢æ˜¯ç©ºçš„ï¼Œæˆ–è€…åªæœ‰ä¸€ä¸¤ä¸ªä¾‹å­æ•°æ®
                æ³¨æ„ï¼šæ‰€æœ‰æ–‡ä»¶å·²è½¬æ¢ä¸ºtxtæ ¼å¼ï¼Œè¡¨æ ¼ä»¥HTMLä»£ç å½¢å¼å‘ˆç°ï¼Œè¯·æ ¹æ®å†…å®¹è€Œéæ–‡ä»¶åæˆ–åç¼€åˆ¤æ–­ã€‚

                å½“å‰åˆ†ææ–‡ä»¶:
                æ–‡ä»¶å: {source_path.name}
                æ–‡ä»¶è·¯å¾„: {file_path}
                æ–‡ä»¶å†…å®¹:
                {analysis_content}

                è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼å›å¤ï¼Œåªè¿”å›è¿™ä¸€ä¸ªæ–‡ä»¶çš„åˆ†ç±»ç»“æœï¼ˆä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ï¼‰ï¼Œä¸è¦å°†è¿”å›å†…å®¹åŒ…è£¹åœ¨```json```ä¸­ï¼š
                {{
                    "classification": "irrelevant" | "table"
                }}"""
                
                # Get LLM analysis for this file
                print("ğŸ“¤ æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œæ–‡ä»¶åˆ†ç±»...")
                analysis_response = invoke_model(model_name="deepseek-ai/DeepSeek-V3", messages=[SystemMessage(content=system_prompt)])

                # Parse JSON response for this file
                try:
                    # Extract JSON from response
                    response_content = analysis_response.strip()
                    print(f"ğŸ“¥ LLMåˆ†ç±»å“åº”: {response_content}")
                    
                    # Remove markdown code blocks if present
                    if response_content.startswith('```'):
                        response_content = response_content.split('\n', 1)[1]
                        response_content = response_content.rsplit('\n', 1)[0]
                    
                    file_classification = json.loads(response_content)
                    classification_type = file_classification.get("classification", "irrelevant")
                    
                    print(f"âœ… æ–‡ä»¶ {source_path.name} åˆ†ç±»ä¸º: {classification_type}")
                    return file_path, classification_type, source_path.name
                    
                except json.JSONDecodeError as e:
                    print(f"âŒ æ–‡ä»¶ {source_path.name} JSONè§£æé”™è¯¯: {e}")
                    print(f"LLMå“åº”: {analysis_response}")
                    # Fallback: mark as irrelevant for safety
                    return file_path, "irrelevant", source_path.name
                
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶å‡ºé”™ {file_path}: {e}")
                # Return irrelevant on error
                return file_path, "irrelevant", Path(file_path).name
        
        # Use ThreadPoolExecutor for parallel processing
        max_workers = min(len(new_files_to_process), 5)  # Limit to 5 concurrent requests
        print(f"ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç†æ–‡ä»¶ï¼Œä½¿ç”¨ {max_workers} ä¸ªå·¥ä½œçº¿ç¨‹")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all file analysis tasks
            future_to_file = {
                executor.submit(analyze_single_file, file_path): file_path 
                for file_path in new_files_to_process
            }
            
            # Process completed tasks as they finish
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    file_path_result, classification_type, file_name = future.result()
                    
                    # Add to appropriate category
                    if classification_type == "table":
                        classification_results["tables"].append(file_path_result)
                    else:  # irrelevant or unknown
                        classification_results["irrelevant"].append(file_path_result)
                    
                    processed_files.append(file_name)
                    
                except Exception as e:
                    print(f"âŒ å¹¶è¡Œå¤„ç†æ–‡ä»¶ä»»åŠ¡å¤±è´¥ {file_path}: {e}")
                    # Add to irrelevant on error
                    classification_results["irrelevant"].append(file_path)
        
        print(f"ğŸ‰ å¹¶è¡Œæ–‡ä»¶åˆ†æå®Œæˆ:")
        print(f"  - è¡¨æ ¼æ–‡ä»¶: {len(classification_results['tables'])} ä¸ª")
        print(f"  - æ— å…³æ–‡ä»¶: {len(classification_results['irrelevant'])} ä¸ª")
        print(f"  - æˆåŠŸå¤„ç†: {len(processed_files)} ä¸ªæ–‡ä»¶")
        
        if not processed_files and not classification_results["irrelevant"]:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶")
            print("âœ… _analyze_uploaded_files æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            return {
                "table_files_path": [],
                "irrelevant_files_path": [],
                "all_files_irrelevant": True  # Flag for routing to text analysis
            }
        
        # Update state with classification results
        table_files = classification_results.get("tables", [])
        irrelevant_files = classification_results.get("irrelevant", [])
        
        # Create mapping of processed files to original files to track irrelevant originals
        irrelevant_original_files = []
        if irrelevant_files:
            original_files = state.get("original_files_path", [])
            processed_files = state.get("new_upload_files_processed_path", [])
            
            print("ğŸ” æ­£åœ¨æ˜ å°„æ— å…³æ–‡ä»¶å¯¹åº”çš„åŸå§‹æ–‡ä»¶...")
            
            # Create mapping based on filename (stem)
            for irrelevant_file in irrelevant_files:
                irrelevant_file_stem = Path(irrelevant_file).stem
                # Find the corresponding original file
                for original_file in original_files:
                    original_file_stem = Path(original_file).stem
                    if irrelevant_file_stem == original_file_stem:
                        irrelevant_original_files.append(original_file)
                        print(f"ğŸ“‹ æ˜ å°„æ— å…³æ–‡ä»¶: {Path(irrelevant_file).name} -> {Path(original_file).name}")
                        break
        
        # Check if all files are irrelevant
        # Safely handle the case where new_upload_files_processed_path might not exist in state
        new_files_processed_count = len(state.get("new_upload_files_processed_path", []))
        all_files_irrelevant = len(irrelevant_files) == new_files_processed_count
        
        if all_files_irrelevant:
            print("âš ï¸ æ‰€æœ‰æ–‡ä»¶éƒ½è¢«åˆ†ç±»ä¸ºæ— å…³æ–‡ä»¶")
            print("âœ… _analyze_uploaded_files æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            return {
                "table_files_path": [],
                "irrelevant_files_path": irrelevant_files,
                "irrelevant_original_files_path": irrelevant_original_files,
                "all_files_irrelevant": True  # Flag for routing
            }
        else:
            # Some files are relevant, proceed with normal flow
            print("âœ… æ–‡ä»¶åˆ†æå®Œæˆï¼Œå­˜åœ¨æœ‰æ•ˆæ–‡ä»¶")
            print(f"  - è¡¨æ ¼æ–‡ä»¶: {len(table_files)} ä¸ª")
            print(f"  - æ— å…³æ–‡ä»¶: {len(irrelevant_files)} ä¸ª")
            print("âœ… _analyze_uploaded_files æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            
            return {
                "table_files_path": table_files,
                "irrelevant_files_path": irrelevant_files,
                "irrelevant_original_files_path": irrelevant_original_files,
                "all_files_irrelevant": False  # Flag for routing
            }
                
    def _route_after_analyze_uploaded_files(self, state: FileProcessState):
        """Route after analyzing uploaded files. Uses Send objects for all routing."""
        print("Debug: route_after_analyze_uploaded_files")
        
        # Check if all files are irrelevant - route to cleanup
        if state.get("all_files_irrelevant", False):
            sends = []
            if state.get("irrelevant_files_path"):
                sends.append(Send("process_irrelevant", state))
            return sends if sends else [Send("summary_file_upload", state)]
        
        # Some files are relevant - process them in parallel
        sends = []
        if state.get("table_files_path"):
            sends.append(Send("process_table", state))
        if state.get("irrelevant_files_path"):
            sends.append(Send("process_irrelevant", state))

        # The parallel nodes will automatically converge, then continue to summary
        return sends if sends else [Send("summary_file_upload", state)]  # Fallback
    
    def _process_table(self, state: FileProcessState) -> FileProcessState:
        """This node will process the table files and extract headers from them"""
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _process_table")
        print("=" * 50)
        
        table_files = state["table_files_path"]
        
        print(f"ğŸ“Š éœ€è¦å¤„ç†çš„è¡¨æ ¼æ–‡ä»¶: {len(table_files)} ä¸ª")
        
        if not table_files:
            print("âš ï¸ æ²¡æœ‰è¡¨æ ¼æ–‡ä»¶éœ€è¦å¤„ç†")
            print("âœ… _process_table æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            return {}
        
        # Store all extracted headers for summary
        all_extracted_headers = {}
        
        def extract_headers_from_response(response: str) -> list[str]:
            """Extract headers from LLM response"""
            try:
                # Try to parse JSON response first
                import re
                # Look for table structure in the response
                if 'è¡¨æ ¼ç»“æ„' in response:
                    # Extract field names from JSON structure
                    json_match = re.search(r'\{.*\}', response, re.DOTALL)
                    if json_match:
                        json_data = json.loads(json_match.group())
                        # Navigate through the JSON to find field names
                        table_structure = None
                        for key, value in json_data.items():
                            if isinstance(value, dict) and 'è¡¨æ ¼ç»“æ„' in value:
                                table_structure = value['è¡¨æ ¼ç»“æ„']
                                break
                            elif key == 'è¡¨æ ¼ç»“æ„' and isinstance(value, dict):
                                table_structure = value
                                break
                        
                        if table_structure:
                            headers = list(table_structure.keys())
                            return headers
                
                # Fallback: extract from plain text
                lines = response.split('\n')
                headers = []
                for line in lines:
                    line = line.strip()
                    if ':' in line and not line.startswith('{') and not line.startswith('}'):
                        header = line.split(':')[0].strip('" ')
                        if header and len(header) < 50:  # Reasonable header length
                            headers.append(header)
                
                return headers[:20]  # Limit to first 20 headers
                
            except Exception as e:
                print(f"æå–è¡¨å¤´å¤±è´¥: {e}")
                return []
        
        def extract_headers_from_txt_content(content: str, filename: str) -> list[str]:
            """Extract headers from HTML content in txt file"""
            try:
                import re
                headers = []
                
                # Look for table headers in HTML
                th_matches = re.findall(r'<th[^>]*>(.*?)</th>', content, re.IGNORECASE | re.DOTALL)
                if th_matches:
                    for th in th_matches:
                        # Clean HTML tags
                        clean_header = re.sub(r'<[^>]+>', '', th).strip()
                        if clean_header and len(clean_header) < 50:
                            headers.append(clean_header)
                
                # If no th tags, look for first row cells
                if not headers:
                    td_matches = re.findall(r'<td[^>]*>(.*?)</td>', content, re.IGNORECASE | re.DOTALL)
                    if td_matches:
                        # Take first few cells as potential headers
                        for i, td in enumerate(td_matches[:10]):
                            clean_header = re.sub(r'<[^>]+>', '', td).strip()
                            if clean_header and len(clean_header) < 50:
                                headers.append(clean_header)
                
                return headers[:15]  # Limit to 15 headers
                
            except Exception as e:
                print(f"ä»æ–‡æœ¬å†…å®¹æå–è¡¨å¤´å¤±è´¥: {e}")
                return []
        
        # Process each table file
        for table_file in table_files:
            try:
                source_path = Path(table_file)
                print(f"ğŸ” æ­£åœ¨å¤„ç†è¡¨æ ¼æ–‡ä»¶: {source_path.name}")
                
                # Find corresponding original Excel file
                table_file_stem = Path(table_file).stem
                original_files = state.get("original_files_path", [])
                original_excel_file = None
                
                for original_file in original_files:
                    if Path(original_file).stem == table_file_stem:
                        original_excel_file = Path(original_file)
                        break
                
                headers = []
                
                try:
                    if original_excel_file and original_excel_file.exists():
                        print(f"ğŸ” æ‰¾åˆ°åŸå§‹Excelæ–‡ä»¶: {original_excel_file}")
                        # Use screenshot-based analysis to extract headers
                        print("ğŸ“¤ æ­£åœ¨è°ƒç”¨LLMæå–è¡¨æ ¼è¡¨å¤´...")
                        analysis_response = invoke_model_with_screenshot(
                            model_name="Qwen/Qwen2.5-VL-72B-Instruct", 
                            file_path=str(original_excel_file)
                        )
                        print("ğŸ“¥ è¡¨å¤´æå–å“åº”æ¥æ”¶æˆåŠŸ")
                        
                        # Extract headers from the response
                        headers = extract_headers_from_response(analysis_response)
                        
                    else:
                        print(f"âš ï¸ æœªæ‰¾åˆ°å¯¹åº”çš„åŸå§‹Excelæ–‡ä»¶: {table_file_stem}")
                        # Fallback: try to extract from txt content
                        file_content = source_path.read_text(encoding='utf-8')
                        headers = extract_headers_from_txt_content(file_content, source_path.name)
                        
                except Exception as llm_error:
                    print(f"âŒ è¡¨å¤´æå–å¤±è´¥: {llm_error}")
                    # Fallback: try to extract from txt content
                    try:
                        file_content = source_path.read_text(encoding='utf-8')
                        headers = extract_headers_from_txt_content(file_content, source_path.name)
                    except Exception as e:
                        print(f"âŒ æ–‡æœ¬å†…å®¹æå–ä¹Ÿå¤±è´¥: {e}")
                        headers = []
                
                # Store extracted headers
                all_extracted_headers[source_path.name] = headers
                print(f"âœ… è¡¨æ ¼æ–‡ä»¶å·²å¤„ç†: {source_path.name} (æå–åˆ° {len(headers)} ä¸ªè¡¨å¤´)")
                
                if headers:
                    print(f"ğŸ“‹ è¡¨å¤´åˆ—è¡¨: {', '.join(headers[:5])}{'...' if len(headers) > 5 else ''}")
                
            except Exception as e:
                print(f"âŒ å¤„ç†è¡¨æ ¼æ–‡ä»¶å‡ºé”™ {table_file}: {e}")
                all_extracted_headers[Path(table_file).name] = []
        
        # Print summary of extracted headers
        print(f"\nğŸ“Š è¡¨å¤´æå–æ€»ç»“:")
        total_headers = 0
        for filename, headers in all_extracted_headers.items():
            print(f"  - {filename}: {len(headers)} ä¸ªè¡¨å¤´")
            total_headers += len(headers)
        print(f"  - æ€»è®¡: {total_headers} ä¸ªè¡¨å¤´ä» {len(table_files)} ä¸ªæ–‡ä»¶ä¸­æå–")
        
        print("âœ… _process_table æ‰§è¡Œå®Œæˆ")
        print("=" * 50)
        
        return {"extracted_headers": all_extracted_headers}
    
        
    def _process_irrelevant(self, state: FileProcessState) -> FileProcessState:
        """This node will process the irrelevant files, it will delete the irrelevant files (both processed and original) from the staging area"""
        
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _process_irrelevant")
        print("=" * 50)
        
        irrelevant_files = state["irrelevant_files_path"]
        irrelevant_original_files = state.get("irrelevant_original_files_path", [])
        
        print(f"ğŸ—‘ï¸ éœ€è¦åˆ é™¤çš„æ— å…³å¤„ç†æ–‡ä»¶æ•°é‡: {len(irrelevant_files)}")
        print(f"ğŸ—‘ï¸ éœ€è¦åˆ é™¤çš„æ— å…³åŸå§‹æ–‡ä»¶æ•°é‡: {len(irrelevant_original_files)}")
        
        # Combine all files to delete
        all_files_to_delete = irrelevant_files + irrelevant_original_files
        
        if all_files_to_delete:
            delete_result = delete_files_from_staging_area(all_files_to_delete)
            
            deleted_count = len(delete_result["deleted_files"])
            failed_count = len(delete_result["failed_deletes"])
            
            print(f"ğŸ“Š åˆ é™¤ç»“æœ: æˆåŠŸ {deleted_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª (æ€»è®¡ {len(all_files_to_delete)} ä¸ªæ–‡ä»¶)")
            
            if delete_result["failed_deletes"]:
                print("âŒ åˆ é™¤å¤±è´¥çš„æ–‡ä»¶:")
                for failed_file in delete_result["failed_deletes"]:
                    print(f"  - {failed_file}")
        else:
            print("âš ï¸ æ²¡æœ‰æ— å…³æ–‡ä»¶éœ€è¦åˆ é™¤")
        
        print("âœ… _process_irrelevant æ‰§è¡Œå®Œæˆ")
        print("=" * 50)
        
        return {}  # Return empty dict since this node doesn't need to update any state keys

    
    def _summary_file_upload(self, state: FileProcessState) -> FileProcessState:
        """Summary node for file upload process"""
        
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _summary_file_upload")
        print("=" * 50)
        
        # Log the final state summary
        print("ğŸ“Š æ–‡ä»¶å¤„ç†æ€»ç»“:")
        print(f"  - ä¸Šä¼ æ–‡ä»¶æ€»æ•°: {len(state.get('upload_files_path', []))}")
        print(f"  - æ–°ä¸Šä¼ æ–‡ä»¶æ•°: {len(state.get('new_upload_files_path', []))}")
        print(f"  - è¡¨æ ¼æ–‡ä»¶æ•°: {len(state.get('table_files_path', []))}")
        print(f"  - æ— å…³æ–‡ä»¶æ•°: {len(state.get('irrelevant_files_path', []))}")
        
        # Show extracted headers summary if available
        extracted_headers = state.get('extracted_headers', {})
        if extracted_headers:
            print(f"  - æå–è¡¨å¤´æ–‡ä»¶æ•°: {len(extracted_headers)}")
            total_headers = sum(len(headers) for headers in extracted_headers.values())
            print(f"  - æ€»è¡¨å¤´æ•°: {total_headers}")
        
        print("âœ… _summary_file_upload æ‰§è¡Œå®Œæˆ")
        print("=" * 50)
        
        return {}

    def run_file_process_agent(self, session_id: str = "1", upload_files_path: list[str] = [], village_name: str = "ChatBI") -> FileProcessState:
        """Driver to run the process file agent"""
        print("\nğŸš€ å¼€å§‹è¿è¡Œ FileProcessAgent")
        print("=" * 60)

        initial_state = self._create_initial_state(session_id = session_id, upload_files_path = upload_files_path, village_name = village_name)
        config = {"configurable": {"thread_id": session_id}}

        print(f"ğŸ“‹ ä¼šè¯ID: {session_id}")
        print(f"ğŸ“ åˆå§‹çŠ¶æ€å·²åˆ›å»º")
        print("ğŸ”„ æ­£åœ¨æ‰§è¡Œæ–‡ä»¶å¤„ç†å·¥ä½œæµ...")

        try:
            final_state = self.graph.invoke(initial_state, config=config)

            print("\nğŸ‰ FileProcessAgent æ‰§è¡Œå®Œæˆï¼")
            print("=" * 60)
            print("ğŸ“Š æœ€ç»ˆç»“æœ:")
            print(f"- ä¸Šä¼ æ–‡ä»¶æ•°é‡: {len(final_state.get('upload_files_path', []))}")
            print(f"- æ–°ä¸Šä¼ æ–‡ä»¶æ•°é‡: {len(final_state.get('new_upload_files_path', []))}")
            print(f"- æ–°ä¸Šä¼ æ–‡ä»¶å·²å¤„ç†æ•°é‡: {len(final_state.get('new_upload_files_processed_path', []))}")
            print(f"- åŸå§‹æ–‡ä»¶æ•°é‡: {len(final_state.get('original_files_path', []))}")
            print(f"- è¡¨æ ¼æ–‡ä»¶æ•°é‡: {len(final_state.get('table_files_path', []))}")
            print(f"- æ— å…³æ–‡ä»¶æ•°é‡: {len(final_state.get('irrelevant_files_path', []))}")

            return final_state
        
        except Exception as e:
            print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return initial_state
if __name__ == "__main__":
    upload_files_path = input("è¯·è¾“å…¥ä¸Šä¼ æ–‡ä»¶è·¯å¾„: ")
    upload_files_path = detect_and_process_file_paths(upload_files_path)
    agent = FileProcessAgent()
    agent.run_file_process_agent(upload_files_path = upload_files_path)
