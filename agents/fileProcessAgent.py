import sys
from pathlib import Path

# Add root project directory to sys.path
sys.path.append(str(Path(__file__).resolve().parent.parent))



from typing import Dict, List, Optional, Any, TypedDict, Annotated
from datetime import datetime
from utils.modelRelated import invoke_model, invoke_model_with_screenshot
from utils.file_process import (retrieve_file_content, save_original_file,
                                    extract_filename, 
                                    ensure_location_structure, check_file_exists_in_data,
                                    get_available_locations, move_template_files_to_final_destination,
                                    move_supplement_files_to_final_destination, delete_files_from_staging_area,
                                    reconstruct_csv_with_headers)

import json

from langgraph.graph import StateGraph, END, START
from langgraph.constants import Send
from langgraph.graph.message import add_messages
# from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command, interrupt
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI


class FileProcessState(TypedDict):
    session_id: str
    upload_files_path: list[str] # Store all uploaded files
    new_upload_files_path: list[str] # Track the new uploaded files in this round
    new_upload_files_processed_path: list[str] # Store the processed new uploaded files
    original_files_path: list[str] # Store the original files in original_file subfolder
    uploaded_template_files_path: list[str]
    supplement_files_path: dict[str, list[str]]
    irrelevant_files_path: list[str]
    irrelevant_original_files_path: list[str] # Track original files to be deleted with irrelevant files
    all_files_irrelevant: bool  # Flag to indicate all files are irrelevant
    template_complexity: str
    village_name: str


class FileProcessAgent:

    @tool
    def request_user_clarification(question: str, context: str = "") -> str:
        """
        è¯¢é—®ç”¨æˆ·æ¾„æ¸…ï¼Œå’Œç”¨æˆ·ç¡®è®¤ï¼Œæˆ–è€…è¯¢é—®ç”¨æˆ·è¡¥å……ä¿¡æ¯ï¼Œå½“ä½ ä¸ç¡®å®šçš„æ—¶å€™è¯·è¯¢é—®ç”¨æˆ·

        å‚æ•°ï¼š
            question: é—®é¢˜
            context: å¯é€‰è¡¥å……å†…å®¹ï¼Œè§£é‡Šä¸ºç”šæ¶é­”ä½ éœ€è¦ä¸€ä¸‹ä¿¡æ¯
        """
        print("\n" + "="*60)
        print("ğŸ¤” éœ€è¦æ‚¨çš„ç¡®è®¤")
        print("="*60)
        print(f"ğŸ“‹ {question}")
        if context:
            print(f"ğŸ’¡ {context}")
        print("="*60)
        
        user_response = input("ğŸ‘¤ è¯·è¾“å…¥æ‚¨çš„é€‰æ‹©: ").strip()
        
        print(f"âœ… æ‚¨çš„é€‰æ‹©: {user_response}")
        print("="*60 + "\n")
        
        return user_response
    
    tools = [request_user_clarification]


    def __init__(self):
        self.memory = MemorySaver()
        self.graph = self._build_graph().compile(checkpointer=self.memory)

    def _build_graph(self):
        graph = StateGraph(FileProcessState)

        graph.add_node("file_upload", self._file_upload)
        graph.add_node("analyze_uploaded_files", self._analyze_uploaded_files)
        graph.add_node("route_after_analyze_uploaded_files", self._route_after_analyze_uploaded_files)
        graph.add_node("process_supplement", self._process_supplement)
        graph.add_node("process_irrelevant", self._process_irrelevant)
        graph.add_node("process_template", self._process_template)
        graph.add_node("summary_file_upload", self._summary_file_upload)

        graph.add_edge(START, "file_upload")
        graph.add_edge("file_upload", "analyze_uploaded_files")
        graph.add_conditional_edges("analyze_uploaded_files", self._route_after_analyze_uploaded_files)
        graph.add_edge("process_template", "summary_file_upload")
        graph.add_edge("process_supplement", "summary_file_upload")
        graph.add_edge("process_irrelevant", "summary_file_upload")
        graph.add_edge("summary_file_upload", END)

        return graph

    def _create_initial_state(self, session_id: str = "1", upload_files_path: list[str] = [], village_name: str = "") -> FileProcessState:
        return {
            "session_id": session_id,
            "upload_files_path": upload_files_path,
            "new_upload_files_path": [],
            "new_upload_files_processed_path": [],
            "original_files_path": [],
            "uploaded_template_files_path": [],
            "supplement_files_path": {"è¡¨æ ¼": [], "æ–‡æ¡£": []},
            "irrelevant_files_path": [],
            "irrelevant_original_files_path": [],
            "all_files_irrelevant": False,
            "template_complexity": "",
            "village_name": village_name
        }


    def _file_upload(self, state: FileProcessState) -> FileProcessState:
            """This node will upload user's file to our system"""
            print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _file_upload")
            print("=" * 50)
            
            print("ğŸ“ æ­£åœ¨æ£€æµ‹ç”¨æˆ·è¾“å…¥ä¸­çš„æ–‡ä»¶è·¯å¾„...")
            detected_files = state["upload_files_path"]
            print(f"ğŸ“‹ æ£€æµ‹åˆ° {len(detected_files)} ä¸ªæ–‡ä»¶")
            
            # Load data.json with error handling
            data_file = Path("agents/data.json")
            try:
                with open(data_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
            except (FileNotFoundError, json.JSONDecodeError) as e:
                print(f"âš ï¸ data.jsonæ–‡ä»¶å‡ºé”™: {e}")
                # Initialize empty structure if file is missing or corrupted
                data = {}
            
            print("ğŸ” æ­£åœ¨æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨...")
            files_to_remove = []
            for file in detected_files:
                file_name = Path(file).name
                if check_file_exists_in_data(data, file_name):
                    files_to_remove.append(file)
                    print(f"âš ï¸ æ–‡ä»¶ {file} å·²å­˜åœ¨")
            
            # Remove existing files from detected_files
            for file in files_to_remove:
                detected_files.remove(file)
            
            if not detected_files:
                print("âš ï¸ æ²¡æœ‰æ–°æ–‡ä»¶éœ€è¦ä¸Šä¼ ")
                print("âœ… _file_upload æ‰§è¡Œå®Œæˆ")
                print("=" * 50)
                return {
                    "new_upload_files_path": [],
                    "new_upload_files_processed_path": []
                }
            
            print(f"ğŸ”„ æ­£åœ¨å¤„ç† {len(detected_files)} ä¸ªæ–°æ–‡ä»¶...")
            
            # Create staging area for original files
            project_root = Path.cwd()
            staging_dir = project_root / "conversations" / state["session_id"] / "user_uploaded_files"
            staging_dir.mkdir(parents=True, exist_ok=True)
            
            # Process the files to get .txt versions
            processed_files = retrieve_file_content(detected_files, state["session_id"])
            
            # Save original files separately
            original_files = []
            for file_path in detected_files:
                try:
                    source_path = Path(file_path)
                    original_file_saved_path = save_original_file(source_path, staging_dir)
                    if original_file_saved_path:
                        original_files.append(original_file_saved_path)
                        print(f"ğŸ’¾ åŸå§‹æ–‡ä»¶å·²ä¿å­˜: {Path(original_file_saved_path).name}")
                    else:
                        print(f"âš ï¸ åŸå§‹æ–‡ä»¶ä¿å­˜å¤±è´¥: {source_path.name}")
                except Exception as e:
                    print(f"âŒ ä¿å­˜åŸå§‹æ–‡ä»¶æ—¶å‡ºé”™ {file_path}: {e}")
            
            print(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {len(processed_files)} ä¸ªå¤„ç†æ–‡ä»¶, {len(original_files)} ä¸ªåŸå§‹æ–‡ä»¶")
            print("âœ… _file_upload æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            
            # Update state with new files
            # Safely handle the case where upload_files_path might not exist in state
            existing_files = state.get("upload_files_path", [])
            existing_original_files = state.get("original_files_path", [])
            return {
                "new_upload_files_path": detected_files,
                "upload_files_path": existing_files + detected_files,
                "new_upload_files_processed_path": processed_files,
                "original_files_path": existing_original_files + original_files
            }
    


    def _analyze_uploaded_files(self, state: FileProcessState) -> FileProcessState:
        """This node will analyze the user's uploaded files, it need to classify the file into template
        supplement, or irrelevant. If all files are irrelevant, it will flag for text analysis instead."""
        
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _analyze_uploaded_files")
        print("=" * 50)
        
        import json
        from pathlib import Path
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        # Initialize classification results
        classification_results = {
            "template": [],
            "supplement": {"è¡¨æ ¼": [], "æ–‡æ¡£": []},
            "irrelevant": []
        }
        
        # Process files one by one for better accuracy
        processed_files = []
        # Safely handle the case where new_upload_files_processed_path might not exist in state
        new_files_to_process = state.get("new_upload_files_processed_path", [])
        
        print(f"ğŸ“ éœ€è¦åˆ†æçš„æ–‡ä»¶æ•°é‡: {len(new_files_to_process)}")
        
        if not new_files_to_process:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶")
            print("âœ… _analyze_uploaded_files æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            return {
                "uploaded_template_files_path": [],
                "supplement_files_path": {"è¡¨æ ¼": [], "æ–‡æ¡£": []},
                "irrelevant_files_path": [],
                "all_files_irrelevant": True  # Flag for routing to text analysis
            }
        
        def analyze_single_file(file_path: str) -> tuple[str, str, str]:
            """Analyze a single file and return (file_path, classification, file_name)"""
            try:
                source_path = Path(file_path)
                print(f"ğŸ” æ­£åœ¨åˆ†ææ–‡ä»¶: {source_path.name}")
                
                if not source_path.exists():
                    print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                    return file_path, "irrelevant", source_path.name
                
                # Read file content for analysis
                file_content = source_path.read_text(encoding='utf-8')
                # Truncate content for analysis (to avoid token limits)
                analysis_content = file_content[:5000] if len(file_content) > 2000 else file_content
                
                # Create individual analysis prompt for this file
                system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªè¡¨æ ¼ç”Ÿæˆæ™ºèƒ½ä½“ï¼Œéœ€è¦åˆ†æç”¨æˆ·ä¸Šä¼ çš„æ–‡ä»¶å†…å®¹å¹¶è¿›è¡Œåˆ†ç±»ã€‚å…±æœ‰å››ç§ç±»å‹ï¼š

                1. **æ¨¡æ¿ç±»å‹ (template)**: ç©ºç™½è¡¨æ ¼æ¨¡æ¿ï¼Œåªæœ‰è¡¨å¤´æ²¡æœ‰å…·ä½“æ•°æ®
                2. **è¡¥å……è¡¨æ ¼ (supplement-è¡¨æ ¼)**: å·²å¡«å†™çš„å®Œæ•´è¡¨æ ¼ï¼Œç”¨äºè¡¥å……æ•°æ®åº“
                3. **è¡¥å……æ–‡æ¡£ (supplement-æ–‡æ¡£)**: åŒ…å«é‡è¦ä¿¡æ¯çš„æ–‡æœ¬æ–‡ä»¶ï¼Œå¦‚æ³•å¾‹æ¡æ–‡ã€æ”¿ç­–ä¿¡æ¯ç­‰
                4. **æ— å…³æ–‡ä»¶ (irrelevant)**: ä¸è¡¨æ ¼å¡«å†™æ— å…³çš„æ–‡ä»¶

                ä»”ç»†æ£€æŸ¥ä¸è¦æŠŠè¡¥å……æ–‡ä»¶é”™è¯¯åˆ’åˆ†ä¸ºæ¨¡æ¿æ–‡ä»¶åä¹‹äº¦ç„¶ï¼Œè¡¥å……æ–‡ä»¶é‡Œé¢æ˜¯æœ‰æ•°æ®çš„ï¼Œæ¨¡æ¿æ–‡ä»¶é‡Œé¢æ˜¯ç©ºçš„ï¼Œæˆ–è€…åªæœ‰ä¸€ä¸¤ä¸ªä¾‹å­æ•°æ®
                æ³¨æ„ï¼šæ‰€æœ‰æ–‡ä»¶å·²è½¬æ¢ä¸ºtxtæ ¼å¼ï¼Œè¡¨æ ¼ä»¥HTMLä»£ç å½¢å¼å‘ˆç°ï¼Œè¯·æ ¹æ®å†…å®¹è€Œéæ–‡ä»¶åæˆ–åç¼€åˆ¤æ–­ã€‚

                å½“å‰åˆ†ææ–‡ä»¶:
                æ–‡ä»¶å: {source_path.name}
                æ–‡ä»¶è·¯å¾„: {file_path}
                æ–‡ä»¶å†…å®¹:
                {analysis_content}

                è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼å›å¤ï¼Œåªè¿”å›è¿™ä¸€ä¸ªæ–‡ä»¶çš„åˆ†ç±»ç»“æœï¼ˆä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ï¼‰ï¼Œä¸è¦å°†è¿”å›å†…å®¹åŒ…è£¹åœ¨```json```ä¸­ï¼š
                {{
                    "classification": "template" | "supplement-è¡¨æ ¼" | "supplement-æ–‡æ¡£" | "irrelevant"
                }}"""
                
                # Get LLM analysis for this file
                print("ğŸ“¤ æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œæ–‡ä»¶åˆ†ç±»...")
                analysis_response = invoke_model(model_name="deepseek-ai/DeepSeek-V3", messages=[SystemMessage(content=system_prompt)])

                # Parse JSON response for this file
                try:
                    # Extract JSON from response
                    response_content = analysis_response.strip()
                    print(f"ğŸ“¥ LLMåˆ†ç±»å“åº”: {response_content}")
                    
                    # Remove markdown code blocks if present
                    if response_content.startswith('```'):
                        response_content = response_content.split('\n', 1)[1]
                        response_content = response_content.rsplit('\n', 1)[0]
                    
                    file_classification = json.loads(response_content)
                    classification_type = file_classification.get("classification", "irrelevant")
                    
                    print(f"âœ… æ–‡ä»¶ {source_path.name} åˆ†ç±»ä¸º: {classification_type}")
                    return file_path, classification_type, source_path.name
                    
                except json.JSONDecodeError as e:
                    print(f"âŒ æ–‡ä»¶ {source_path.name} JSONè§£æé”™è¯¯: {e}")
                    print(f"LLMå“åº”: {analysis_response}")
                    # Fallback: mark as irrelevant for safety
                    return file_path, "irrelevant", source_path.name
                
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶å‡ºé”™ {file_path}: {e}")
                # Return irrelevant on error
                return file_path, "irrelevant", Path(file_path).name
        
        # Use ThreadPoolExecutor for parallel processing
        max_workers = min(len(new_files_to_process), 5)  # Limit to 5 concurrent requests
        print(f"ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç†æ–‡ä»¶ï¼Œä½¿ç”¨ {max_workers} ä¸ªå·¥ä½œçº¿ç¨‹")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all file analysis tasks
            future_to_file = {
                executor.submit(analyze_single_file, file_path): file_path 
                for file_path in new_files_to_process
            }
            
            # Process completed tasks as they finish
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    file_path_result, classification_type, file_name = future.result()
                    
                    # Add to appropriate category
                    if classification_type == "template":
                        classification_results["template"].append(file_path_result)
                    elif classification_type == "supplement-è¡¨æ ¼":
                        classification_results["supplement"]["è¡¨æ ¼"].append(file_path_result)
                    elif classification_type == "supplement-æ–‡æ¡£":
                        classification_results["supplement"]["æ–‡æ¡£"].append(file_path_result)
                    else:  # irrelevant or unknown
                        classification_results["irrelevant"].append(file_path_result)
                    
                    processed_files.append(file_name)
                    
                except Exception as e:
                    print(f"âŒ å¹¶è¡Œå¤„ç†æ–‡ä»¶ä»»åŠ¡å¤±è´¥ {file_path}: {e}")
                    # Add to irrelevant on error
                    classification_results["irrelevant"].append(file_path)
        
        print(f"ğŸ‰ å¹¶è¡Œæ–‡ä»¶åˆ†æå®Œæˆ:")
        print(f"  - æ¨¡æ¿æ–‡ä»¶: {len(classification_results['template'])} ä¸ª")
        print(f"  - è¡¥å……è¡¨æ ¼: {len(classification_results['supplement']['è¡¨æ ¼'])} ä¸ª")
        print(f"  - è¡¥å……æ–‡æ¡£: {len(classification_results['supplement']['æ–‡æ¡£'])} ä¸ª")
        print(f"  - æ— å…³æ–‡ä»¶: {len(classification_results['irrelevant'])} ä¸ª")
        print(f"  - æˆåŠŸå¤„ç†: {len(processed_files)} ä¸ªæ–‡ä»¶")
        
        if not processed_files and not classification_results["irrelevant"]:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶")
            print("âœ… _analyze_uploaded_files æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            return {
                "uploaded_template_files_path": [],
                "supplement_files_path": {"è¡¨æ ¼": [], "æ–‡æ¡£": []},
                "irrelevant_files_path": [],
                "all_files_irrelevant": True  # Flag for routing to text analysis
            }
        
        # Update state with classification results
        uploaded_template_files = classification_results.get("template", [])
        supplement_files = classification_results.get("supplement", {"è¡¨æ ¼": [], "æ–‡æ¡£": []})
        irrelevant_files = classification_results.get("irrelevant", [])
        
        # Create mapping of processed files to original files to track irrelevant originals
        irrelevant_original_files = []
        if irrelevant_files:
            original_files = state.get("original_files_path", [])
            processed_files = state.get("new_upload_files_processed_path", [])
            
            print("ğŸ” æ­£åœ¨æ˜ å°„æ— å…³æ–‡ä»¶å¯¹åº”çš„åŸå§‹æ–‡ä»¶...")
            
            # Create mapping based on filename (stem)
            for irrelevant_file in irrelevant_files:
                irrelevant_file_stem = Path(irrelevant_file).stem
                # Find the corresponding original file
                for original_file in original_files:
                    original_file_stem = Path(original_file).stem
                    if irrelevant_file_stem == original_file_stem:
                        irrelevant_original_files.append(original_file)
                        print(f"ğŸ“‹ æ˜ å°„æ— å…³æ–‡ä»¶: {Path(irrelevant_file).name} -> {Path(original_file).name}")
                        break
        
        # Check if all files are irrelevant
        # Safely handle the case where new_upload_files_processed_path might not exist in state
        new_files_processed_count = len(state.get("new_upload_files_processed_path", []))
        all_files_irrelevant = (
            len(uploaded_template_files) == 0 and 
            len(supplement_files.get("è¡¨æ ¼", [])) == 0 and 
            len(supplement_files.get("æ–‡æ¡£", [])) == 0 and
            len(irrelevant_files) == new_files_processed_count
        )
        
        if all_files_irrelevant:
            print("âš ï¸ æ‰€æœ‰æ–‡ä»¶éƒ½è¢«åˆ†ç±»ä¸ºæ— å…³æ–‡ä»¶")
            print("âœ… _analyze_uploaded_files æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            return {
                "uploaded_template_files_path": [],
                "supplement_files_path": {"è¡¨æ ¼": [], "æ–‡æ¡£": []},
                "irrelevant_files_path": irrelevant_files,
                "irrelevant_original_files_path": irrelevant_original_files,
                "all_files_irrelevant": True  # Flag for routing
            }
        else:
            # Some files are relevant, proceed with normal flow
            analysis_summary = f"""æ–‡ä»¶åˆ†æå®Œæˆ:
            æ¨¡æ¿æ–‡ä»¶: {len(uploaded_template_files)} ä¸ª
            è¡¥å……è¡¨æ ¼: {len(supplement_files.get("è¡¨æ ¼", []))} ä¸ª  
            è¡¥å……æ–‡æ¡£: {len(supplement_files.get("æ–‡æ¡£", []))} ä¸ª
            æ— å…³æ–‡ä»¶: {len(irrelevant_files)} ä¸ª"""
            
            print("âœ… æ–‡ä»¶åˆ†æå®Œæˆï¼Œå­˜åœ¨æœ‰æ•ˆæ–‡ä»¶")
            print("âœ… _analyze_uploaded_files æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            
            return {
                "uploaded_template_files_path": uploaded_template_files,
                "supplement_files_path": supplement_files,
                "irrelevant_files_path": irrelevant_files,
                "irrelevant_original_files_path": irrelevant_original_files,
                "all_files_irrelevant": False  # Flag for routing
            }
                
    def _route_after_analyze_uploaded_files(self, state: FileProcessState):
        """Route after analyzing uploaded files. Uses Send objects for all routing."""
        print("Debug: route_after_analyze_uploaded_files")
        
        # Check if all files are irrelevant - route to text analysis
        if state.get("all_files_irrelevant", False):
            # First clean up irrelevant files, then analyze text
            sends = []
            if state.get("irrelevant_files_path"):
                sends.append(Send("process_irrelevant", state))
            return sends if sends else [Send("summary_file_upload", state)]
        
        # Some files are relevant - process them in parallel
        sends = []
        if state.get("uploaded_template_files_path"):
            print("Debug: process_template")
            sends.append(Send("process_template", state))
        if state.get("supplement_files_path", {}).get("è¡¨æ ¼") or state.get("supplement_files_path", {}).get("æ–‡æ¡£"):
            print("Debug: process_supplement")
            sends.append(Send("process_supplement", state))
        if state.get("irrelevant_files_path"):
            print("Debug: process_irrelevant")
            sends.append(Send("process_irrelevant", state))

        # The parallel nodes will automatically converge, then continue to summary
        return sends if sends else [Send("summary_file_upload", state)]  # Fallback
    
    def _process_supplement(self, state: FileProcessState) -> FileProcessState:
        """This node will process the supplement files, it will analyze the supplement files and summarize the content of the files as well as stored the summary in data.json"""
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _process_supplement")
        print("=" * 50)
        
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        # Load existing data.json with better error handling
        data_json_path = Path("agents/data.json")
        try:
            with open(data_json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except FileNotFoundError:
            print("ğŸ“ data.jsonä¸å­˜åœ¨ï¼Œåˆ›å»ºç©ºçš„æ•°æ®ç»“æ„")
            data = {}
        except json.JSONDecodeError as e:
            print(f"âš ï¸ data.jsonæ ¼å¼é”™è¯¯: {e}")
            print("ğŸ“ å¤‡ä»½åŸæ–‡ä»¶å¹¶åˆ›å»ºæ–°çš„æ•°æ®ç»“æ„")
            # Backup the corrupted file
            backup_path = data_json_path.with_suffix('.json.backup')
            if data_json_path.exists():
                data_json_path.rename(backup_path)
                print(f"ğŸ“¦ åŸæ–‡ä»¶å·²å¤‡ä»½åˆ°: {backup_path}")
            data = {}
        
        # Use village_name from state as the location for all supplement files
        location = state["village_name"]
        
        table_files = state["supplement_files_path"]["è¡¨æ ¼"]
        document_files = state["supplement_files_path"]["æ–‡æ¡£"]
        
        print(f"ğŸ“Š éœ€è¦å¤„ç†çš„è¡¨æ ¼æ–‡ä»¶: {len(table_files)} ä¸ª")
        print(f"ğŸ“„ éœ€è¦å¤„ç†çš„æ–‡æ¡£æ–‡ä»¶: {len(document_files)} ä¸ª")
        
        # Collect new messages instead of directly modifying state
        new_messages = []
        
        def process_table_file(table_file: str) -> tuple[str, str, dict]:
            """Process a single table file and return (file_path, file_type, result_data)"""
            try:
                source_path = Path(table_file)
                print(f"ğŸ” è¡¨æ ¼æ–‡ä»¶è·¯å¾„: {source_path}")
                print(f"ğŸ” æ­£åœ¨å¤„ç†è¡¨æ ¼æ–‡ä»¶: {source_path.name}")
                
                
                # Use village_name as the location for this table file
                file_location = location
                

                print("ğŸ“¤ æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œè¡¨æ ¼åˆ†æ...")
                
                try:
                    file_name = source_path.name
                    print(f"ğŸ” è¡¨æ ¼æ–‡ä»¶å: {file_name}")
                    
                    # Find the corresponding original Excel file from the uploaded files
                    table_file_stem = Path(table_file).stem
                    original_files = state.get("original_files_path", [])
                    original_excel_file = None
                    
                    for original_file in original_files:
                        if Path(original_file).stem == table_file_stem:
                            original_excel_file = Path(original_file)
                            break
                    
                    if original_excel_file and original_excel_file.exists():
                        print(f"ğŸ” æ‰¾åˆ°åŸå§‹Excelæ–‡ä»¶: {original_excel_file}")
                        analysis_response = invoke_model_with_screenshot(model_name="Qwen/Qwen2.5-VL-72B-Instruct", file_path=original_excel_file)
                        print("ğŸ“¥ è¡¨æ ¼åˆ†æå“åº”æ¥æ”¶æˆåŠŸ")
                    else:
                        print(f"âš ï¸ æœªæ‰¾åˆ°å¯¹åº”çš„åŸå§‹Excelæ–‡ä»¶: {table_file_stem}")
                        raise FileNotFoundError(f"Original Excel file not found for {table_file_stem}")
                        
                except Exception as llm_error:
                    print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {llm_error}")
                    # Create fallback response  
                    analysis_response = f"è¡¨æ ¼æ–‡ä»¶åˆ†æå¤±è´¥: {str(llm_error)}ï¼Œæ–‡ä»¶å: {source_path.name}"
                
                # Create result data with location information
                # Note: file_path will be updated after moving to final destination
                result_data = {
                    "file_key": source_path.name.split(".")[0],
                    "location": file_location,
                    "new_entry": {
                        "summary": analysis_response,
                        "file_path": str(table_file),  # This will be updated after moving
                        "original_file_path": str(original_excel_file) if original_excel_file else "",  # This will be updated after moving
                        "timestamp": datetime.now().isoformat(),
                        "file_size": source_path.stat().st_size
                    },
                    "analysis_response": analysis_response
                }
                
                print(f"âœ… è¡¨æ ¼æ–‡ä»¶å·²åˆ†æ: {source_path.name} (ä½ç½®: {file_location})")
                
                # Reconstruct CSV with headers using the analyzed structure
                try:
                    reconstructed_csv_path = reconstruct_csv_with_headers(
                        analysis_response, source_path.name, original_excel_file, village_name=state["village_name"]
                    )
                    if reconstructed_csv_path:
                        result_data["reconstructed_csv_path"] = reconstructed_csv_path
                        print(f"ğŸ“Š CSVé‡æ„å®Œæˆ: {reconstructed_csv_path}")
                except Exception as csv_error:
                    print(f"âŒ CSVé‡æ„å¤±è´¥: {csv_error}")
                    result_data["reconstructed_csv_path"] = ""
                
                return table_file, "table", result_data
                
            except Exception as e:
                print(f"âŒ å¤„ç†è¡¨æ ¼æ–‡ä»¶å‡ºé”™ {table_file}: {e}")
                return table_file, "table", {
                    "file_key": Path(table_file).name,
                    "location": location,  # Use village_name as location on error
                    "new_entry": {
                        "summary": f"è¡¨æ ¼æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}",
                        "file_path": str(table_file),
                        "timestamp": datetime.now().isoformat(),
                        "file_size": 0
                    },
                    "analysis_response": f"è¡¨æ ¼æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}"
                }

        def process_document_file(document_file: str) -> tuple[str, str, dict]:
            """Process a single document file and return (file_path, file_type, result_data)"""
            try:
                source_path = Path(document_file)
                print(f"ğŸ” æ­£åœ¨å¤„ç†æ–‡æ¡£æ–‡ä»¶: {source_path.name}")
                
                file_content = source_path.read_text(encoding='utf-8')
                # file_content = file_content[:2000] if len(file_content) > 2000 else file_content
                file_name = extract_filename(document_file)
                print(f"ğŸ” æ–‡æ¡£æ–‡ä»¶å: {file_name}")
                
                # Use village_name as the location for this document file
                file_location = location
                print(f"ğŸ“ æ–‡æ¡£æ–‡ä»¶ä½¿ç”¨ä½ç½®: {file_location}")
                
                system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡æ¡£åˆ†æä¸“å®¶ï¼Œå…·å¤‡æ³•å¾‹ä¸æ”¿ç­–è§£è¯»èƒ½åŠ›ã€‚ä½ çš„ä»»åŠ¡æ˜¯é˜…è¯»ç”¨æˆ·æä¾›çš„ HTML æ ¼å¼æ–‡ä»¶ï¼Œå¹¶ä»ä¸­æå–å‡ºæœ€é‡è¦çš„ 1-2 æ¡å…³é”®ä¿¡æ¯è¿›è¡Œæ€»ç»“ï¼Œæ— éœ€æå–å…¨éƒ¨å†…å®¹ã€‚

è¯·éµå¾ªä»¥ä¸‹è¦æ±‚ï¼š

1. å¿½ç•¥æ‰€æœ‰ HTML æ ‡ç­¾ï¼ˆå¦‚ <p>ã€<div>ã€<table> ç­‰ï¼‰ï¼Œåªå…³æ³¨æ–‡æœ¬å†…å®¹ï¼›

2. ä»æ–‡ä»¶ä¸­æå–é‡è¦çš„é¡¹æ ¸å¿ƒæ”¿ç­–ä¿¡æ¯ï¼ˆä¾‹å¦‚è¡¥è´´é‡‘é¢ã€é€‚ç”¨å¯¹è±¡ã€å®¡æ‰¹æµç¨‹ç­‰ï¼‰ï¼Œæˆ–è€…å…¶ä»–ä½ è§‰å¾—é‡è¦çš„ä¿¡æ¯ï¼›

3. å¯¹æå–çš„ä¿¡æ¯è¿›è¡Œç»“æ„åŒ–æ€»ç»“ï¼Œè¯­è¨€æ­£å¼ã€é€»è¾‘æ¸…æ™°ã€ç®€æ´æ˜äº†ï¼›

4. è¾“å‡ºæ ¼å¼ä¸ºä¸¥æ ¼çš„ JSONï¼Œä½†ä¸è¦åŒ…è£¹åœ¨```jsonä¸­ï¼Œç›´æ¥è¿”å›jsonæ ¼å¼å³å¯ï¼š
   {{
     "æ–‡ä»¶å": "å†…å®¹æ€»ç»“"
   }}

5. è‹¥æä¾›å¤šä¸ªæ–‡ä»¶ï¼Œéœ€åˆ†åˆ«å¤„ç†å¹¶åˆå¹¶è¾“å‡ºä¸ºä¸€ä¸ª JSON å¯¹è±¡ï¼›

6. è¾“å‡ºè¯­è¨€åº”ä¸è¾“å…¥æ–‡æ¡£ä¿æŒä¸€è‡´ï¼ˆè‹¥æ–‡æ¡£ä¸ºä¸­æ–‡ï¼Œåˆ™è¾“å‡ºä¸­æ–‡ï¼‰ï¼›

7. è¾“å‡ºæ–‡ä»¶åå’Œæä¾›çš„æ–‡ä»¶åä¸€è‡´ï¼Œä¸è®¸æœ‰ä»»ä½•æ›´æ”¹

è¯·æ ¹æ®ä¸Šè¿°è¦æ±‚ï¼Œå¯¹æä¾›çš„ HTML æ–‡ä»¶å†…å®¹è¿›è¡Œåˆ†æå¹¶è¿”å›ç»“æœã€‚

æ–‡ä»¶å†…å®¹:
{file_content}
""".format(file_name=file_name, file_content=file_content)

                print("ğŸ“¤ æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œæ–‡æ¡£åˆ†æ...")
                print("ç¡®è®¤æ–‡æ¡£åˆ†ææç¤ºè¯ï¼š\n", system_prompt)
                
                try:
                    analysis_response = invoke_model(model_name="Pro/deepseek-ai/DeepSeek-V3", messages=[SystemMessage(content=system_prompt)])
                    print("ğŸ“¥ æ–‡æ¡£åˆ†æå“åº”æ¥æ”¶æˆåŠŸ")
                    analysis_response_dict = json.loads(analysis_response)
                    keys = list(analysis_response_dict.keys())
                    old_key = keys[0]
                    new_key = file_name
                    analysis_response_dict[new_key] = analysis_response_dict.pop(old_key)
                    analysis_response = json.dumps(analysis_response_dict, ensure_ascii=False)
                    print("ğŸ“¥ æ–‡æ¡£åˆ†æå“åº”è½¬æ¢æˆåŠŸ:", analysis_response)
                except Exception as llm_error:
                    print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {llm_error}")
                    # Create fallback response
                    analysis_response = f"æ–‡æ¡£æ–‡ä»¶åˆ†æå¤±è´¥: {str(llm_error)}ï¼Œæ–‡ä»¶å: {source_path.name}"

                # Create result data with location information
                # Note: file_path will be updated after moving to final destination
                result_data = {
                    "file_key": source_path.name,
                    "location": file_location,  # Single location
                    "new_entry": {
                        "summary": analysis_response,
                        "file_path": str(document_file),  # This will be updated after moving
                        "original_file_path": str(source_path),  # This will be updated after moving
                        "timestamp": datetime.now().isoformat(),
                        "file_size": source_path.stat().st_size
                    },
                    "analysis_response": analysis_response
                }
                
                print(f"âœ… æ–‡æ¡£æ–‡ä»¶å·²åˆ†æ: {source_path.name} (ä½ç½®: {file_location})")
                return document_file, "document", result_data
                
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡æ¡£æ–‡ä»¶å‡ºé”™ {document_file}: {e}")
                return document_file, "document", {
                    "file_key": Path(document_file).name,
                    "location": location,  # Use village_name as location on error
                    "new_entry": {
                        "summary": f"æ–‡æ¡£æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}",
                        "file_path": str(document_file),
                        "timestamp": datetime.now().isoformat(),
                        "file_size": 0
                    },
                    "analysis_response": f"æ–‡æ¡£æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}"
                }

        # Use ThreadPoolExecutor for parallel processing
        all_files = [(file, "table") for file in table_files] + [(file, "document") for file in document_files]
        total_files = len(all_files)
        
        if total_files == 0:
            print("âš ï¸ æ²¡æœ‰æ–‡ä»¶éœ€è¦å¤„ç†")
            print("âœ… _process_supplement æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            return {}
        
        max_workers = min(total_files, 5)  # Limit to 4 concurrent requests for supplement processing
        print(f"ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç†è¡¥å……æ–‡ä»¶ï¼Œä½¿ç”¨ {max_workers} ä¸ªå·¥ä½œçº¿ç¨‹")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all file processing tasks
            future_to_file = {}
            for file_path, file_type in all_files:
                if file_type == "table":
                    future = executor.submit(process_table_file, file_path)
                    # Implement the logic for 
                else:  # document
                    future = executor.submit(process_document_file, file_path)
                future_to_file[future] = (file_path, file_type)
            
            # Process completed tasks as they finish
            for future in as_completed(future_to_file):
                file_path, file_type = future_to_file[future]
                try:
                    file_path, processed_file_type, result_data = future.result()
                    
                    # Add to new_messages
                    new_messages.append(AIMessage(content=result_data["analysis_response"]))
                    
                    # Update data.json structure with location-based storage
                    file_key = result_data["file_key"]
                    new_entry = result_data["new_entry"]
                    
                    # Both table and document files now use single location
                    file_location = result_data["location"]
                    # Ensure location structure exists in data
                    data = ensure_location_structure(data, file_location)
                    
                    if processed_file_type == "table":
                        if file_key in data[file_location]["è¡¨æ ¼"]:
                            print(f"âš ï¸ è¡¨æ ¼æ–‡ä»¶ {file_key} å·²å­˜åœ¨äº {file_location}ï¼Œå°†æ›´æ–°å…¶å†…å®¹")
                            # Preserve any additional fields that might exist
                            existing_entry = data[file_location]["è¡¨æ ¼"][file_key]
                            for key, value in existing_entry.items():
                                if key not in new_entry:
                                    new_entry[key] = value
                        else:
                            print(f"ğŸ“ æ·»åŠ æ–°çš„è¡¨æ ¼æ–‡ä»¶: {file_key} åˆ° {file_location}")
                        data[file_location]["è¡¨æ ¼"][file_key] = new_entry
                    else:  # document - now also uses single location
                        if file_key in data[file_location]["æ–‡æ¡£"]:
                            print(f"âš ï¸ æ–‡æ¡£æ–‡ä»¶ {file_key} å·²å­˜åœ¨äº {file_location}ï¼Œå°†æ›´æ–°å…¶å†…å®¹")
                            # Preserve any additional fields that might exist
                            existing_entry = data[file_location]["æ–‡æ¡£"][file_key]
                            for key, value in existing_entry.items():
                                if key not in new_entry:
                                    new_entry[key] = value
                        else:
                            print(f"ğŸ“ æ·»åŠ æ–°çš„æ–‡æ¡£æ–‡ä»¶: {file_key} åˆ° {file_location}")
                        data[file_location]["æ–‡æ¡£"][file_key] = new_entry
                    
                except Exception as e:
                    print(f"âŒ å¹¶è¡Œå¤„ç†æ–‡ä»¶ä»»åŠ¡å¤±è´¥ {file_path}: {e}")
                    # Create fallback entry
                    fallback_response = f"æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}"
                    new_messages.append(AIMessage(content=fallback_response))
        
        print(f"ğŸ‰ å¹¶è¡Œæ–‡ä»¶å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {total_files} ä¸ªæ–‡ä»¶")
        
        # Move supplement files to their final destinations and update data.json with new paths
        original_files = state.get("original_files_path", [])
        
        # Track moved files to update data.json paths
        moved_files_info = {}
        
        # Move table files to their final destination
        for table_file in table_files:
            # Find corresponding original file
            table_file_stem = Path(table_file).stem
            corresponding_original_file = ""
            
            for original_file in original_files:
                if Path(original_file).stem == table_file_stem:
                    corresponding_original_file = original_file
                    break
            
            try:
                move_result = move_supplement_files_to_final_destination(
                    table_file, corresponding_original_file, "table", village_name=state["village_name"]
                )
                print(f"âœ… è¡¨æ ¼æ–‡ä»¶å·²ç§»åŠ¨åˆ°æœ€ç»ˆä½ç½®: {Path(table_file).name}")
                
                # Store moved file info for later data.json update
                moved_files_info[Path(table_file).name] = {
                    "new_processed_path": move_result["processed_supplement_path"],
                    "new_original_path": move_result["original_supplement_path"],
                    "new_screen_shot_path": move_result.get("screen_shot_path", "")  # Use get to avoid KeyError
                }
            except Exception as e:
                print(f"âŒ ç§»åŠ¨è¡¨æ ¼æ–‡ä»¶å¤±è´¥ {table_file}: {e}")
        
        # Move document files to their final destination
        for document_file in document_files:
            # Find corresponding original file
            document_file_stem = Path(document_file).stem
            corresponding_original_file = ""
            
            for original_file in original_files:
                if Path(original_file).stem == document_file_stem:
                    corresponding_original_file = original_file
                    break
            
            try:
                move_result = move_supplement_files_to_final_destination(
                    document_file, corresponding_original_file, "document", village_name=state["village_name"]
                )
                print(f"âœ… æ–‡æ¡£æ–‡ä»¶å·²ç§»åŠ¨åˆ°æœ€ç»ˆä½ç½®: {Path(document_file).name}")
                
                # Store moved file info for later data.json update
                moved_files_info[Path(document_file).name] = {
                    "new_processed_path": move_result["processed_supplement_path"],
                    "new_original_path": move_result["original_supplement_path"]
                }
            except Exception as e:
                print(f"âŒ ç§»åŠ¨æ–‡æ¡£æ–‡ä»¶å¤±è´¥ {document_file}: {e}")
        
        # Update data.json entries with new file paths
        for location in data.keys():
            if isinstance(data[location], dict):
                # Update table file paths
                for file_key in data[location].get("è¡¨æ ¼", {}):
                    if file_key in moved_files_info:
                        if moved_files_info[file_key]["new_processed_path"]:
                            data[location]["è¡¨æ ¼"][file_key]["file_path"] = moved_files_info[file_key]["new_processed_path"]
                        if moved_files_info[file_key]["new_original_path"]:
                            data[location]["è¡¨æ ¼"][file_key]["original_file_path"] = moved_files_info[file_key]["new_original_path"]
                
                # Update document file paths
                for file_key in data[location].get("æ–‡æ¡£", {}):
                    if file_key in moved_files_info:
                        if moved_files_info[file_key]["new_processed_path"]:
                            data[location]["æ–‡æ¡£"][file_key]["file_path"] = moved_files_info[file_key]["new_processed_path"]
                        if moved_files_info[file_key]["new_original_path"]:
                            data[location]["æ–‡æ¡£"][file_key]["original_file_path"] = moved_files_info[file_key]["new_original_path"]
        
        # Save updated data.json with atomic write
        try:
            # Write to a temporary file first to prevent corruption
            temp_path = data_json_path.with_suffix('.json.tmp')
            with open(temp_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
            
            # Atomic rename to replace the original file
            temp_path.replace(data_json_path)
            
            # Count total files across all locations
            total_table_files = sum(len(data[location]["è¡¨æ ¼"]) for location in data.keys() if isinstance(data[location], dict))
            total_document_files = sum(len(data[location]["æ–‡æ¡£"]) for location in data.keys() if isinstance(data[location], dict))
            
            print(f"âœ… å·²æ›´æ–° data.jsonï¼Œè¡¨æ ¼æ–‡ä»¶ {total_table_files} ä¸ªï¼Œæ–‡æ¡£æ–‡ä»¶ {total_document_files} ä¸ª")
            
            # Log the files that were processed in this batch
            if table_files:
                print(f"ğŸ“Š æœ¬æ‰¹æ¬¡å¤„ç†çš„è¡¨æ ¼æ–‡ä»¶: {[Path(f).name for f in table_files]}")
            if document_files:
                print(f"ğŸ“„ æœ¬æ‰¹æ¬¡å¤„ç†çš„æ–‡æ¡£æ–‡ä»¶: {[Path(f).name for f in document_files]}")
            
            # Log current distribution by location
            print("ğŸ“ å½“å‰æ•°æ®åˆ†å¸ƒ:")
            for location in data.keys():
                if isinstance(data[location], dict):
                    table_count = len(data[location]["è¡¨æ ¼"])
                    doc_count = len(data[location]["æ–‡æ¡£"])
                    print(f"  {location}: è¡¨æ ¼ {table_count} ä¸ª, æ–‡æ¡£ {doc_count} ä¸ª")
                
        except Exception as e:
            print(f"âŒ ä¿å­˜ data.json æ—¶å‡ºé”™: {e}")
            # Clean up temp file if it exists
            temp_path = data_json_path.with_suffix('.json.tmp')
            if temp_path.exists():
                try:
                    temp_path.unlink()
                    print("ğŸ—‘ï¸ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
                except Exception as cleanup_error:
                    print(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {cleanup_error}")
        
        print("âœ… _process_supplement æ‰§è¡Œå®Œæˆ")
        print("=" * 50)
        
        # Return empty dict since we don't need to update state with messages
        return {}
    
        
    def _process_irrelevant(self, state: FileProcessState) -> FileProcessState:
        """This node will process the irrelevant files, it will delete the irrelevant files (both processed and original) from the staging area"""
        
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _process_irrelevant")
        print("=" * 50)
        
        irrelevant_files = state["irrelevant_files_path"]
        irrelevant_original_files = state.get("irrelevant_original_files_path", [])
        
        print(f"ğŸ—‘ï¸ éœ€è¦åˆ é™¤çš„æ— å…³å¤„ç†æ–‡ä»¶æ•°é‡: {len(irrelevant_files)}")
        print(f"ğŸ—‘ï¸ éœ€è¦åˆ é™¤çš„æ— å…³åŸå§‹æ–‡ä»¶æ•°é‡: {len(irrelevant_original_files)}")
        
        # Combine all files to delete
        all_files_to_delete = irrelevant_files + irrelevant_original_files
        
        if all_files_to_delete:
            delete_result = delete_files_from_staging_area(all_files_to_delete)
            
            deleted_count = len(delete_result["deleted_files"])
            failed_count = len(delete_result["failed_deletes"])
            
            print(f"ğŸ“Š åˆ é™¤ç»“æœ: æˆåŠŸ {deleted_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª (æ€»è®¡ {len(all_files_to_delete)} ä¸ªæ–‡ä»¶)")
            
            if delete_result["failed_deletes"]:
                print("âŒ åˆ é™¤å¤±è´¥çš„æ–‡ä»¶:")
                for failed_file in delete_result["failed_deletes"]:
                    print(f"  - {failed_file}")
        else:
            print("âš ï¸ æ²¡æœ‰æ— å…³æ–‡ä»¶éœ€è¦åˆ é™¤")
        
        print("âœ… _process_irrelevant æ‰§è¡Œå®Œæˆ")
        print("=" * 50)
        
        return {}  # Return empty dict since this node doesn't need to update any state keys

    
    def _process_template(self, state: FileProcessState) -> FileProcessState:
        """This node will process the template files, it will analyze the template files and determine if it's a valid template"""
        
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _process_template")
        print("=" * 50)
        
        template_files = state["uploaded_template_files_path"]
        print(f"ğŸ“‹ éœ€è¦å¤„ç†çš„æ¨¡æ¿æ–‡ä»¶æ•°é‡: {len(template_files)}")
        
        # If multiple templates, ask user to choose
        if len(template_files) > 1:
            print("âš ï¸ æ£€æµ‹åˆ°å¤šä¸ªæ¨¡æ¿æ–‡ä»¶ï¼Œéœ€è¦ç”¨æˆ·é€‰æ‹©")
            template_names = [Path(f).name for f in template_files]
            template_list = "\n".join([f"  {i+1}. {name}" for i, name in enumerate(template_names)])
            question = f"""æ£€æµ‹åˆ°å¤šä¸ªæ¨¡æ¿æ–‡ä»¶ï¼Œè¯·é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡æ¿ï¼š

ğŸ“‹ å¯ç”¨æ¨¡æ¿ï¼š
{template_list}

è¯·è¾“å…¥åºå·ï¼ˆå¦‚ï¼š1ï¼‰é€‰æ‹©æ¨¡æ¿ï¼š"""
            
            try:
                print("ğŸ¤ æ­£åœ¨è¯·æ±‚ç”¨æˆ·ç¡®è®¤æ¨¡æ¿é€‰æ‹©...")
                user_choice = self.request_user_clarification.invoke(
                    input = {"question": question,
                             "context": "ç³»ç»Ÿéœ€è¦ç¡®å®šä½¿ç”¨å“ªä¸ªæ¨¡æ¿æ–‡ä»¶è¿›è¡Œåç»­å¤„ç†"}
                    )
                
                # Parse user choice
                try:
                    choice_index = int(user_choice.strip()) - 1
                    if 0 <= choice_index < len(template_files):
                        selected_template = template_files[choice_index]
                        # Remove non-selected templates
                        rejected_templates = [f for i, f in enumerate(template_files) if i != choice_index]
                        
                        # Delete rejected template files (both processed and original)
                        original_files = state.get("original_files_path", [])
                        for rejected_file in rejected_templates:
                            try:
                                # Delete processed template file
                                Path(rejected_file).unlink()
                                print(f"ğŸ—‘ï¸ å·²åˆ é™¤æœªé€‰ä¸­çš„å¤„ç†æ¨¡æ¿: {Path(rejected_file).name}")
                                
                                # Find and delete corresponding original file
                                rejected_file_stem = Path(rejected_file).stem
                                for original_file in original_files:
                                    original_file_path = Path(original_file)
                                    if original_file_path.stem == rejected_file_stem:
                                        try:
                                            original_file_path.unlink()
                                            print(f"ğŸ—‘ï¸ å·²åˆ é™¤æœªé€‰ä¸­çš„åŸå§‹æ¨¡æ¿: {original_file_path.name}")
                                            break
                                        except Exception as orig_error:
                                            print(f"âŒ åˆ é™¤åŸå§‹æ¨¡æ¿æ–‡ä»¶å‡ºé”™: {orig_error}")
                                
                            except Exception as e:
                                print(f"âŒ åˆ é™¤æ¨¡æ¿æ–‡ä»¶å‡ºé”™: {e}")
                        
                        # Update state to only include selected template
                        template_files = [selected_template]
                        print(f"âœ… ç”¨æˆ·é€‰æ‹©äº†æ¨¡æ¿: {Path(selected_template).name}")
                        
                    else:
                        print("âŒ æ— æ•ˆçš„é€‰æ‹©ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡æ¿")
                        selected_template = template_files[0]
                        template_files = [selected_template]
                        
                except ValueError:
                    print("âŒ è¾“å…¥æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡æ¿")
                    selected_template = template_files[0]
                    template_files = [selected_template]
                    
            except Exception as e:
                print(f"âŒ ç”¨æˆ·é€‰æ‹©å‡ºé”™: {e}")
                selected_template = template_files[0]
                template_files = [selected_template]
        
        # Analyze the selected template for complexity
        template_file = template_files[0]
        print(f"ğŸ” æ­£åœ¨åˆ†ææ¨¡æ¿å¤æ‚åº¦: {Path(template_file).name}")
        
        try:
            source_path = Path(template_file)
            template_content = source_path.read_text(encoding='utf-8')
            
            # Create prompt to determine if template is complex or simple
            system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªè¡¨æ ¼ç»“æ„åˆ†æä¸“å®¶ï¼Œéœ€è¦åˆ¤æ–­è¿™ä¸ªè¡¨æ ¼æ¨¡æ¿æ˜¯å¤æ‚æ¨¡æ¿è¿˜æ˜¯ç®€å•æ¨¡æ¿ã€‚

            åˆ¤æ–­æ ‡å‡†ï¼š
            - **å¤æ‚æ¨¡æ¿**: è¡¨æ ¼åŒæ—¶åŒ…å«è¡Œè¡¨å¤´å’Œåˆ—è¡¨å¤´ï¼Œå³æ—¢æœ‰è¡Œæ ‡é¢˜åˆæœ‰åˆ—æ ‡é¢˜çš„äºŒç»´è¡¨æ ¼ç»“æ„
            - **ç®€å•æ¨¡æ¿**: è¡¨æ ¼åªåŒ…å«åˆ—è¡¨å¤´æˆ–è€…åªåŒ…å«è¡Œè¡¨å¤´ï¼Œä½†æ˜¯å¯ä»¥æ˜¯å¤šçº§è¡¨å¤´ï¼Œæ¯è¡Œæ˜¯ç‹¬ç«‹çš„æ•°æ®è®°å½•

            æ¨¡æ¿å†…å®¹ï¼ˆHTMLæ ¼å¼ï¼‰ï¼š
            {template_content}

            è¯·ä»”ç»†åˆ†æè¡¨æ ¼ç»“æ„ï¼Œç„¶ååªå›å¤ä»¥ä¸‹é€‰é¡¹ä¹‹ä¸€ï¼š
            [Complex] - å¦‚æœæ˜¯å¤æ‚æ¨¡æ¿ï¼ˆåŒ…å«è¡Œè¡¨å¤´å’Œåˆ—è¡¨å¤´ï¼‰
            [Simple] - å¦‚æœæ˜¯ç®€å•æ¨¡æ¿ï¼ˆåªåŒ…å«åˆ—è¡¨å¤´ï¼‰"""
            

            print("ğŸ“¤ æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œæ¨¡æ¿å¤æ‚åº¦åˆ†æ...")
            
            analysis_response = invoke_model(model_name="Pro/deepseek-ai/DeepSeek-V3", messages=[SystemMessage(content=system_prompt)])
            
            # Extract the classification from the response
            if "[Complex]" in analysis_response:
                template_type = "[Complex]"
            elif "[Simple]" in analysis_response:
                template_type = "[Simple]"
            else:
                template_type = "[Simple]"  # Default fallback
            
            # å°†æ¨¡æ¿æ–‡ä»¶ï¼ˆåŒ…æ‹¬åŸå§‹æ–‡ä»¶ï¼‰ç§»åŠ¨åˆ°æœ€ç»ˆä½ç½®
            # Find corresponding original file
            original_files = state.get("original_files_path", [])
            template_file_stem = Path(template_file).stem
            corresponding_original_file = ""
            
            for original_file in original_files:
                if Path(original_file).stem == template_file_stem:
                    corresponding_original_file = original_file
                    break
            
            # Move template files to final destination using session ID
            # Extract session ID from one of the file paths
            session_id = state["session_id"]  # Default session ID
            if template_file:
                # Extract session ID from the file path: conversations/session_id/user_uploaded_files/...
                template_path_parts = Path(template_file).parts
                if len(template_path_parts) >= 3 and template_path_parts[0] == "conversations":
                    session_id = template_path_parts[1]
            
            move_result = move_template_files_to_final_destination(
                template_file, corresponding_original_file, session_id
            )
            final_template_path = move_result["processed_template_path"]
            final_original_template_path = move_result["original_template_path"]
            print(f"ğŸ“ æ¨¡æ¿åŸå§‹æ–‡ä»¶å·²ç§»åŠ¨åˆ°: {final_original_template_path}")
            print(f"ğŸ“ æ¨¡æ¿å¤„ç†æ–‡ä»¶å·²ç§»åŠ¨åˆ°: {final_template_path}")
            if move_result["original_template_path"]:
                print(f"ğŸ“ æ¨¡æ¿åŸå§‹æ–‡ä»¶å·²ç§»åŠ¨åˆ°: {move_result['original_template_path']}")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°å¯¹åº”çš„åŸå§‹æ¨¡æ¿æ–‡ä»¶")

            print(f"ğŸ“¥ æ¨¡æ¿åˆ†æç»“æœ: {template_type}")
            print("âœ… _process_template æ‰§è¡Œå®Œæˆ")
            print("=" * 50)

            return {"template_complexity": template_type,
                    "uploaded_template_files_path": [final_template_path]
                    }

        except Exception as e:
            print(f"âŒ æ¨¡æ¿åˆ†æLLMè°ƒç”¨å‡ºé”™: {e}")
            # Default to Simple if analysis fails
            template_type = "[Simple]"
            print("âš ï¸ æ¨¡æ¿åˆ†æå¤±è´¥ï¼Œé»˜è®¤ä¸ºç®€å•æ¨¡æ¿")
            
            # Still try to move the template file (including original) even if LLM analysis fails
            original_files = state.get("original_files_path", [])
            template_file_stem = Path(template_file).stem
            corresponding_original_file = ""
            
            for original_file in original_files:
                if Path(original_file).stem == template_file_stem:
                    corresponding_original_file = original_file
                    break
            
            # Extract session ID from file path
            session_id = state["session_id"]  # Default session ID
            if template_file:
                template_path_parts = Path(template_file).parts
                if len(template_path_parts) >= 3 and template_path_parts[0] == "conversations":
                    session_id = template_path_parts[1]
            
            move_result = move_template_files_to_final_destination(
                template_file, corresponding_original_file, session_id
            )
            final_template_path = move_result["processed_template_path"]
            
            if move_result["original_template_path"]:
                print(f"ğŸ“ æ¨¡æ¿åŸå§‹æ–‡ä»¶å·²ç§»åŠ¨åˆ°: {move_result['original_template_path']}")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°å¯¹åº”çš„åŸå§‹æ¨¡æ¿æ–‡ä»¶")
            
            print("âœ… _process_template æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            
            return {
                "template_complexity": template_type,
                "uploaded_template_files_path": [final_template_path]
            }

    def _summary_file_upload(self, state: FileProcessState) -> FileProcessState:
        """Summary node for file upload process"""
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _summary_file_upload")
        print("=" * 50)
        
        # Log the final state summary
        print("ğŸ“Š æ–‡ä»¶å¤„ç†æ€»ç»“:")
        print(f"  - ä¸Šä¼ æ–‡ä»¶æ€»æ•°: {len(state.get('upload_files_path', []))}")
        print(f"  - æ–°ä¸Šä¼ æ–‡ä»¶æ•°: {len(state.get('new_upload_files_path', []))}")
        print(f"  - æ¨¡æ¿æ–‡ä»¶æ•°: {len(state.get('uploaded_template_files_path', []))}")
        print(f"  - è¡¥å……è¡¨æ ¼æ–‡ä»¶æ•°: {len(state.get('supplement_files_path', {}).get('è¡¨æ ¼', []))}")
        print(f"  - è¡¥å……æ–‡æ¡£æ–‡ä»¶æ•°: {len(state.get('supplement_files_path', {}).get('æ–‡æ¡£', []))}")
        print(f"  - æ— å…³æ–‡ä»¶æ•°: {len(state.get('irrelevant_files_path', []))}")
        print(f"  - æ¨¡æ¿å¤æ‚åº¦: {state.get('template_complexity', 'N/A')}")
        
        print("âœ… _summary_file_upload æ‰§è¡Œå®Œæˆ")
        print("=" * 50)
        
        return {}


    def run_file_process_agent(self, session_id: str = "1", upload_files_path: list[str] = [], village_name: str = "") -> FileProcessState:
        """Driver to run the process file agent"""
        print("\nğŸš€ å¼€å§‹è¿è¡Œ FileProcessAgent")
        print("=" * 60)

        initial_state = self._create_initial_state(session_id = session_id, upload_files_path = upload_files_path, village_name = village_name)
        config = {"configurable": {"thread_id": session_id}}

        print(f"ğŸ“‹ ä¼šè¯ID: {session_id}")
        print(f"ğŸ“ åˆå§‹çŠ¶æ€å·²åˆ›å»º")
        print("ğŸ”„ æ­£åœ¨æ‰§è¡Œæ–‡ä»¶å¤„ç†å·¥ä½œæµ...")

        try:
            final_state = self.graph.invoke(initial_state, config=config)

            print("\nğŸ‰ FileProcessAgent æ‰§è¡Œå®Œæˆï¼")
            print("=" * 60)
            print("ğŸ“Š æœ€ç»ˆç»“æœ:")
            print(f"- ä¸Šä¼ æ–‡ä»¶æ•°é‡: {len(final_state.get('upload_files_path', []))}")
            print(f"- æ–°ä¸Šä¼ æ–‡ä»¶æ•°é‡: {len(final_state.get('new_upload_files_path', []))}")
            print(f"- æ–°ä¸Šä¼ æ–‡ä»¶å·²å¤„ç†æ•°é‡: {len(final_state.get('new_upload_files_processed_path', []))}")
            print(f"- åŸå§‹æ–‡ä»¶æ•°é‡: {len(final_state.get('original_files_path', []))}")

            return final_state
        
        except Exception as e:
            print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return initial_state
