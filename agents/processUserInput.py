import sys
from pathlib import Path

# Add root project directory to sys.path
sys.path.append(str(Path(__file__).resolve().parent.parent))



from typing import Dict, List, Optional, Any, TypedDict, Annotated
from datetime import datetime
from utils.modelRelated import invoke_model
from utils.file_process import (detect_and_process_file_paths)
from agents.fileProcessAgent import FileProcessAgent

import uuid
import json
import os

from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed

from langgraph.graph import StateGraph, END, START
from langgraph.constants import Send
from langgraph.graph.message import add_messages
# from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command, interrupt
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI

load_dotenv()


class ProcessUserInputState(TypedDict):
    process_user_input_messages: Annotated[list[BaseMessage], add_messages]
    user_input: str
    upload_files_path: list[str] # Store all uploaded files
    text_input_validation: str  # Store validation result [Valid] or [Invalid]
    previous_AI_messages: list[BaseMessage]
    summary_message: str  # Add the missing field
    template_file_path: str
    template_complexity: str
    session_id: str
    current_node: str
    next_node: str
    village_name: str
    
class ProcessUserInputAgent:

    @tool
    def request_user_clarification(question: str, context: str = "") -> str:
        """
        è¯¢é—®ç”¨æˆ·æ¾„æ¸…ï¼Œå’Œç”¨æˆ·ç¡®è®¤ï¼Œæˆ–è€…è¯¢é—®ç”¨æˆ·è¡¥å……ä¿¡æ¯ï¼Œå½“ä½ ä¸ç¡®å®šçš„æ—¶å€™è¯·è¯¢é—®ç”¨æˆ·

        å‚æ•°ï¼š
            question: é—®é¢˜
            context: å¯é€‰è¡¥å……å†…å®¹ï¼Œè§£é‡Šä¸ºç”šæ¶é­”ä½ éœ€è¦ä¸€ä¸‹ä¿¡æ¯
        """
        print("\n" + "="*60)
        print("ğŸ¤” éœ€è¦æ‚¨çš„ç¡®è®¤")
        print("="*60)
        print(f"ğŸ“‹ {question}")
        if context:
            print(f"ğŸ’¡ {context}")
        print("="*60)
        
        user_response = input("ğŸ‘¤ è¯·è¾“å…¥æ‚¨çš„é€‰æ‹©: ").strip()
        
        print(f"âœ… æ‚¨çš„é€‰æ‹©: {user_response}")
        print("="*60 + "\n")
        
        return user_response
    
    tools = [request_user_clarification]



    def __init__(self):
        self.memory = MemorySaver()
        self.graph = self._build_graph().compile(checkpointer=self.memory)


    def _build_graph(self) -> StateGraph:
        """This function will build the graph for the process user input agent"""
        graph = StateGraph(ProcessUserInputState)
        graph.add_node("collect_user_input", self._collect_user_input)
        graph.add_node("file_process_agent", self._file_process_agent)
        graph.add_node("analyze_text_input", self._analyze_text_input)
        graph.add_node("clarification_tool_node", ToolNode(self.tools, messages_key = "process_user_input_messages"))
        graph.add_node("summary_user_input", self._summary_user_input)
        graph.add_node("decide_next_node", self._decide_next_node)
        graph.add_node("combine_summary_and_decide_next_node", self._combine_summary_and_decide_next_node)
        
        graph.add_edge(START, "collect_user_input")

        graph.add_conditional_edges(
            "collect_user_input",
            self._route_after_collect_user_input,
            {
                "file_process_agent": "file_process_agent",
                "analyze_text_input": "analyze_text_input",
            }
        )

        graph.add_conditional_edges("file_process_agent", self._route_after_file_process_agent)
        graph.add_conditional_edges("analyze_text_input", self._route_after_analyze_text_input)
        graph.add_edge("summary_user_input", "combine_summary_and_decide_next_node")
        graph.add_edge("decide_next_node", "combine_summary_and_decide_next_node")
        graph.add_edge("combine_summary_and_decide_next_node", END)
        return graph



    def create_initial_state(self, session_id: str, previous_AI_messages = None, 
                             current_node: str = "", village_name: str = "") -> ProcessUserInputState:
        """This function initializes the state of the process user input agent"""
        
        # Handle both single BaseMessage and list[BaseMessage] input
        processed_messages = None
        if previous_AI_messages is not None:
            if isinstance(previous_AI_messages, list):
                processed_messages = previous_AI_messages
                print(f"ğŸ” åˆå§‹åŒ–: æ¥æ”¶åˆ°æ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å« {len(previous_AI_messages)} æ¡æ¶ˆæ¯")
            else:
                # It's a single message, convert to list
                processed_messages = [previous_AI_messages]
                print(f"ğŸ” åˆå§‹åŒ–: æ¥æ”¶åˆ°å•æ¡æ¶ˆæ¯ï¼Œå·²è½¬æ¢ä¸ºåˆ—è¡¨")
        else:
            print(f"ğŸ” åˆå§‹åŒ–: æ²¡æœ‰æ¥æ”¶åˆ°previous_AI_messages")
        
        return {
            "process_user_input_messages": [],
            "user_input": "",
            "upload_files_path": [],
            "text_input_validation": None,
            "previous_AI_messages": processed_messages,
            "summary_message": "",
            "template_complexity": "",
            "template_file_path": "",
            "session_id": session_id,
            "current_node": current_node,
            "next_node": "collect_user_input",
            "village_name": village_name
        }


    def _collect_user_input(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """This is the node where we get user's input"""
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _collect_user_input")
        print("=" * 50)
        print("âŒ¨ï¸ ç­‰å¾…ç”¨æˆ·è¾“å…¥...")
        
        user_input = interrupt("ç”¨æˆ·ï¼š")
        
        print(f"ğŸ“¥ æ¥æ”¶åˆ°ç”¨æˆ·è¾“å…¥: {user_input[:100]}{'...' if len(user_input) > 100 else ''}")
        user_upload_files = detect_and_process_file_paths(user_input)
        print(f"ğŸ” æ£€æµ‹åˆ°çš„æ–‡ä»¶: {user_upload_files}")
        print("âœ… _collect_user_input æ‰§è¡Œå®Œæˆ")
        print("=" * 50)
        
        return {
            "process_user_input_messages": [HumanMessage(content=user_input)],
            "user_input": user_input,
            "upload_files_path": user_upload_files
        }



    def _route_after_collect_user_input(self, state: ProcessUserInputState) -> str:
        """This node act as a safety check node, it will analyze the user's input and determine if it's a valid input,
        based on the LLM's previous response, at the same time it will route the agent to the correct node"""
        
        upload_files_path = state["upload_files_path"]
        if upload_files_path:
            # Files detected - route to file_upload 
            # Note: We cannot modify state in routing functions, so file_upload node will re-detect files
            return "file_process_agent"
        
        # User didn't upload any new files, we will analyze the text input
        return "analyze_text_input"


    def _file_process_agent(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """This node will route to the file process agent"""
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _file_process_agent")
        print("=" * 50)
        
        file_process_agent = FileProcessAgent()
        file_process_agent_final_state = file_process_agent.run_file_process_agent(
            session_id=state["session_id"],
            upload_files_path=state["upload_files_path"],
            village_name=state["village_name"]
        )
        
        # Handle template file path - convert list to string if necessary
        template_files_list = file_process_agent_final_state.get("uploaded_template_files_path", [])
        if isinstance(template_files_list, list) and len(template_files_list) > 0:
            template_file_path = template_files_list[0]  # Take the first template file
        else:
            template_file_path = ""
            
        template_complexity = file_process_agent_final_state.get("template_complexity", "")
        print(f"ğŸ” æ¨¡æ¿æ–‡ä»¶è·¯å¾„: {template_file_path}")
        print(f"ğŸ” æ¨¡æ¿å¤æ‚åº¦: {template_complexity}")

        return {"template_file_path": template_file_path,
                "template_complexity": template_complexity}

    def _route_after_file_process_agent(self, state: ProcessUserInputState) -> str:
        sends = []
        sends.append(Send("decide_next_node", state))
        sends.append(Send("summary_user_input", state))
        return sends

    def _analyze_text_input(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """This node performs a safety check on user text input when all uploaded files are irrelevant.
        It validates if the user input contains meaningful table/Excel-related content.
        Returns [Valid] or [Invalid] based on the analysis."""
        
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _analyze_text_input")
        print("=" * 50)
        
        user_input = state["user_input"]
        print(f"ğŸ“ æ­£åœ¨åˆ†æç”¨æˆ·æ–‡æœ¬è¾“å…¥: {user_input[:100]}{'...' if len(user_input) > 100 else ''}")
        
        if not user_input or user_input.strip() == "":
            print("âŒ ç”¨æˆ·è¾“å…¥ä¸ºç©º")
            print("âœ… _analyze_text_input æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            return {
                "text_input_validation": "[Invalid]",
                "process_user_input_messages": [SystemMessage(content="âŒ ç”¨æˆ·è¾“å…¥ä¸ºç©ºï¼ŒéªŒè¯å¤±è´¥")]
            }
        
        # Create validation prompt for text input safety check
        # Get the previous AI message content safely
        previous_ai_content = ""
        try:
            if state.get("previous_AI_messages"):
                previous_ai_messages = state["previous_AI_messages"]
                print(f"ğŸ” previous_AI_messages ç±»å‹: {type(previous_ai_messages)}")
                
                # Handle both single message and list of messages
                if isinstance(previous_ai_messages, list):
                    if len(previous_ai_messages) > 0:
                        latest_message = previous_ai_messages[-1]
                        if hasattr(latest_message, 'content'):
                            previous_ai_content = latest_message.content
                        else:
                            previous_ai_content = str(latest_message)
                        print(f"ğŸ“ ä»æ¶ˆæ¯åˆ—è¡¨æå–å†…å®¹ï¼Œé•¿åº¦: {len(previous_ai_content)}")
                    else:
                        print("âš ï¸ æ¶ˆæ¯åˆ—è¡¨ä¸ºç©º")
                else:
                    # It's a single message object
                    if hasattr(previous_ai_messages, 'content'):
                        previous_ai_content = previous_ai_messages.content
                    else:
                        previous_ai_content = str(previous_ai_messages)
                    print(f"ğŸ“ ä»å•ä¸ªæ¶ˆæ¯æå–å†…å®¹ï¼Œé•¿åº¦: {len(previous_ai_content)}")
            else:
                print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°previous_AI_messages")
                
        except Exception as e:
            print(f"âŒ æå–previous_AI_messageså†…å®¹æ—¶å‡ºé”™: {e}")
            previous_ai_content = ""
            
        system_prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è¾“å…¥éªŒè¯ä¸“å®¶ï¼Œä»»åŠ¡æ˜¯åˆ¤æ–­ç”¨æˆ·çš„æ–‡æœ¬è¾“å…¥æ˜¯å¦ä¸**è¡¨æ ¼ç”Ÿæˆæˆ– Excel å¤„ç†ç›¸å…³**ï¼Œå¹¶ä¸”æ˜¯å¦åœ¨å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡ä¸­å…·æœ‰å®é™…æ„ä¹‰ã€‚

ä½ å°†è·å¾—ä»¥ä¸‹ä¸¤éƒ¨åˆ†ä¿¡æ¯ï¼š
- ä¸Šä¸€è½® AI çš„å›å¤ï¼ˆç”¨äºåˆ¤æ–­ä¸Šä¸‹æ–‡æ˜¯å¦è¿è´¯ï¼‰
- å½“å‰ç”¨æˆ·çš„è¾“å…¥å†…å®¹

è¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†è¿›è¡Œåˆ¤æ–­ï¼š

ã€æœ‰æ•ˆè¾“å…¥ [Valid]ã€‘æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶å³å¯è§†ä¸ºæœ‰æ•ˆï¼š
- æ˜ç¡®æåˆ°ç”Ÿæˆè¡¨æ ¼ã€å¡«å†™è¡¨æ ¼ã€Excel å¤„ç†ã€æ•°æ®æ•´ç†ç­‰ç›¸å…³æ“ä½œ
- æå‡ºå…³äºè¡¨æ ¼å­—æ®µã€æ•°æ®æ ¼å¼ã€æ¨¡æ¿ç»“æ„ç­‰æ–¹é¢çš„éœ€æ±‚æˆ–æé—®
- æä¾›è¡¨æ ¼ç›¸å…³çš„æ•°æ®å†…å®¹ã€å­—æ®µè¯´æ˜æˆ–è§„åˆ™
- å¯¹ä¸Šä¸€è½® AI çš„å›å¤ä½œå‡ºæœ‰æ„ä¹‰çš„å»¶ç»­æˆ–å›åº”ï¼ˆå³ä½¿æœªç›´æ¥æåˆ°è¡¨æ ¼ï¼‰
- å³ä½¿å­˜åœ¨é”™åˆ«å­—ã€è¯­ç—…ã€æ‹¼å†™é”™è¯¯ï¼Œåªè¦è¯­ä¹‰æ¸…æ™°åˆç†ï¼Œä¹Ÿè§†ä¸ºæœ‰æ•ˆ

ã€æ— æ•ˆè¾“å…¥ [Invalid]ã€‘ç¬¦åˆä»¥ä¸‹ä»»ä¸€æƒ…å†µå³è§†ä¸ºæ— æ•ˆï¼š
- å†…å®¹ä¸è¡¨æ ¼/Excel å®Œå…¨æ— å…³ï¼ˆå¦‚é—²èŠã€æƒ…ç»ªè¡¨è¾¾ã€ä¸ä¸Šä¸‹æ–‡è·³è„±ï¼‰
- æ˜æ˜¾ä¸ºæµ‹è¯•æ–‡æœ¬ã€éšæœºå­—ç¬¦æˆ–ç³»ç»Ÿè°ƒè¯•è¾“å…¥ï¼ˆå¦‚ "123"ã€"æµ‹è¯•ä¸€ä¸‹"ã€"å“ˆå•Šå•Šå•Š" ç­‰ï¼‰
- ä»…åŒ…å«ç©ºç™½ã€è¡¨æƒ…ç¬¦å·ã€æ ‡ç‚¹ç¬¦å·ç­‰æ— å®é™…å†…å®¹

ã€è¾“å‡ºè¦æ±‚ã€‘
è¯·ä½ æ ¹æ®ä¸Šè¿°æ ‡å‡†ï¼Œ**ä»…è¾“å‡ºä»¥ä¸‹ä¸¤ç§ç»“æœä¹‹ä¸€**ï¼ˆä¸æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ï¼‰ï¼š
- [Valid]
- [Invalid]

ã€ä¸Šä¸€è½® AI çš„å›å¤ã€‘
{previous_ai_content}
"""


        try:
            print("ğŸ“¤ æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œæ–‡æœ¬è¾“å…¥éªŒè¯...")
            # Get LLM validation
            print("ç³»ç»Ÿæç¤ºè¯éªŒè¯ç”¨æˆ·è¾“å…¥: \n" + system_prompt)
            user_input = "ç”¨æˆ·è¾“å…¥ï¼š" + user_input
            print("analyze_text_inputæ—¶è°ƒç”¨æ¨¡å‹çš„è¾“å…¥: \n" + user_input)              
            validation_response = invoke_model(model_name="Pro/deepseek-ai/DeepSeek-V3", messages=[SystemMessage(content=system_prompt), HumanMessage(content=user_input)])
            # validation_response = self.llm_s.invoke([SystemMessage(content=system_prompt)])
            
            print(f"ğŸ“¥ éªŒè¯å“åº”: {validation_response}")
            
            if "[Valid]" in validation_response:
                validation_result = "[Valid]"
                status_message = "ç”¨æˆ·è¾“å…¥éªŒè¯é€šè¿‡ - å†…å®¹ä¸è¡¨æ ¼ç›¸å…³ä¸”æœ‰æ„ä¹‰"
            elif "[Invalid]" in validation_response:
                validation_result = "[Invalid]"
                status_message = "ç”¨æˆ·è¾“å…¥éªŒè¯å¤±è´¥ - å†…å®¹ä¸è¡¨æ ¼æ— å…³æˆ–æ— æ„ä¹‰"
            else:
                # Default to Invalid for safety
                validation_result = "[Invalid]"
                status_message = "ç”¨æˆ·è¾“å…¥éªŒè¯å¤±è´¥ - æ— æ³•ç¡®å®šè¾“å…¥æœ‰æ•ˆæ€§ï¼Œé»˜è®¤ä¸ºæ— æ•ˆ"
                print(f"âš ï¸ æ— æ³•è§£æéªŒè¯ç»“æœï¼ŒLLMå“åº”: {validation_response}")
            
            print(f"ğŸ“Š éªŒè¯ç»“æœ: {validation_result}")
            print(f"ğŸ“‹ çŠ¶æ€è¯´æ˜: {status_message}")
            
            # Create validation summary
            summary_message = f"""æ–‡æœ¬è¾“å…¥å®‰å…¨æ£€æŸ¥å®Œæˆ:
            
            **ç”¨æˆ·è¾“å…¥**: {user_input[:100]}{'...' if len(user_input) > 100 else ''}
            **éªŒè¯ç»“æœ**: {validation_result}
            **çŠ¶æ€**: {status_message}"""
            
            print("âœ… _analyze_text_input æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            
            return {
                "text_input_validation": validation_result,
                "process_user_input_messages": [SystemMessage(content=summary_message)]
            }
                
        except Exception as e:
            print(f"âŒ éªŒè¯æ–‡æœ¬è¾“å…¥æ—¶å‡ºé”™: {e}")
            
            # Default to Invalid for safety when there's an error
            error_message = f"""âŒ æ–‡æœ¬è¾“å…¥éªŒè¯å‡ºé”™: {e}
            
            ğŸ“„ **ç”¨æˆ·è¾“å…¥**: {user_input[:100]}{'...' if len(user_input) > 100 else ''}
            ğŸ”’ **å®‰å…¨æªæ–½**: é»˜è®¤æ ‡è®°ä¸ºæ— æ•ˆè¾“å…¥"""
            
            print("âœ… _analyze_text_input æ‰§è¡Œå®Œæˆ (å‡ºé”™)")
            print("=" * 50)
            
            return {
                "text_input_validation": "[Invalid]",
                "process_user_input_messages": [SystemMessage(content=error_message)]
            }



    def _route_after_analyze_text_input(self, state: ProcessUserInputState) -> str:
        """Route after text input validation based on [Valid] or [Invalid] result."""
        sends = []
        validation_result = state.get("text_input_validation", "[Invalid]")
        
        if validation_result == "[Valid]":
            sends.append(Send("decide_next_node", state))
            sends.append(Send("summary_user_input", state))
        else:
            # Text input is invalid, route back to collect user input
            sends.append(Send("collect_user_input", state))
        return sends
        


    def _decide_next_node(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """è¿™ä¸ªèŠ‚ç‚¹è°ƒç”¨å¤§æ¨¡å‹æ¥å†³å®šä¸‹ä¸€æ­¥çš„è·¯ç”±"""
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _decide_next_node")
        print("=" * 50)
        
        # First check if template complexity is available
        template_complexity = state.get("template_complexity", "")
        print(f"ğŸ” æ¨¡æ¿å¤æ‚åº¦: {template_complexity}")
        
        if "[Complex]" in template_complexity:
            route_decision = "complex_template"
            print("ğŸ“ åŸºäºæ¨¡æ¿å¤æ‚åº¦è·¯ç”±åˆ°: complex_template")
        elif "[Simple]" in template_complexity:
            route_decision = "simple_template"
            print("ğŸ“ åŸºäºæ¨¡æ¿å¤æ‚åº¦è·¯ç”±åˆ°: simple_template")
        else:
            # If no template complexity, determine based on user input context
            user_input = state.get("user_input", "")
            
            # Get previous AI messages for context
            previous_ai_content = ""
            if state.get("previous_AI_messages"):
                previous_ai_messages = state["previous_AI_messages"]
                if isinstance(previous_ai_messages, list) and len(previous_ai_messages) > 0:
                    latest_message = previous_ai_messages[-1]
                    if hasattr(latest_message, 'content'):
                        previous_ai_content = latest_message.content
                    else:
                        previous_ai_content = str(latest_message)
                elif not isinstance(previous_ai_messages, list):
                    if hasattr(previous_ai_messages, 'content'):
                        previous_ai_content = previous_ai_messages.content
                    else:
                        previous_ai_content = str(previous_ai_messages)
            
            system_prompt = f"""ä½ æ˜¯ä¸€ä½æ™ºèƒ½å·¥ä½œæµè·¯ç”±å†³ç­–ä¸“å®¶ï¼Œè´Ÿè´£æ ¹æ®ç”¨æˆ·è¾“å…¥å’Œå¯¹è¯ä¸Šä¸‹æ–‡ï¼Œå’Œå½“å‰èŠ‚ç‚¹ï¼Œç²¾ç¡®åˆ¤æ–­ä¸‹ä¸€æ­¥åº”è¯¥æ‰§è¡Œçš„èŠ‚ç‚¹ã€‚

## ğŸ“‹ è·¯ç”±å†³ç­–è§„åˆ™

### 1. å½“å‰èŠ‚ç‚¹ä¸ºinitial_collect_user_inputæ—¶
- **åˆ¤æ–­é€»è¾‘**ï¼šåˆ†æç”¨æˆ·è¾“å…¥æ˜¯å¦åŒ…å«æ˜ç¡®çš„è¡¨æ ¼ç”Ÿæˆæ„å›¾
- **è·¯ç”±è§„åˆ™**ï¼š
  - **åŒ…å«æ˜ç¡®è¡¨æ ¼ç”Ÿæˆæ„å›¾**ï¼ˆå¦‚"ç”Ÿæˆè¡¨æ ¼"ã€"åˆ›å»ºæ¨¡æ¿"ã€"å¡«å……è¡¨æ ¼"ã€"åˆ¶ä½œæŠ¥è¡¨"ç­‰ï¼‰â†’ `chat_with_user_to_determine_template`
  - **ä»…æä¾›æ–‡ä»¶è·¯å¾„**æˆ–**æ¨¡ç³Šéœ€æ±‚**æˆ–**æ— æ˜ç¡®æ„å›¾** â†’ `initial_collect_user_input`ï¼ˆç»§ç»­æ”¶é›†ç”¨æˆ·è¾“å…¥ï¼‰
- **å…³é”®è¯ç¤ºä¾‹**ï¼š
  - âœ… è§¦å‘è·¯ç”±ï¼šç”Ÿæˆã€åˆ›å»ºã€åˆ¶ä½œã€å¡«å……ã€è¡¨æ ¼ã€æ¨¡æ¿ã€æŠ¥è¡¨ã€ç»Ÿè®¡ã€æ•´ç†æ•°æ®
  - âŒ ç»§ç»­æ”¶é›†ï¼šä»…æ–‡ä»¶è·¯å¾„ã€ç®€å•é—®å€™ã€ä¸æ˜ç¡®æè¿°

### 2. å½“å‰èŠ‚ç‚¹ä¸ºdesign_excel_templateæ—¶
- **è§¦å‘æ¡ä»¶**ï¼šç”¨æˆ·è¾“å…¥æ¶‰åŠè¡¨æ ¼ç”Ÿæˆã€æ¨¡æ¿è®¾è®¡ã€å­—æ®µå®šä¹‰ç­‰éœ€æ±‚
- **ç”¨æˆ·åé¦ˆåˆ¤æ–­**ï¼š
  - æ»¡æ„/ç¡®è®¤ â†’ `generate_html_template`ï¼ˆå¼€å§‹ç”ŸæˆHTMLæ¨¡æ¿ï¼‰
  - ä¿®æ”¹å»ºè®®/ä¸æ»¡æ„ â†’ `design_excel_template`ï¼ˆé‡æ–°è®¾è®¡è¡¨æ ¼ç»“æ„ï¼‰

### 3. å½“å‰èŠ‚ç‚¹ä¸ºrecall_relative_filesæ—¶
- **è§¦å‘æ¡ä»¶**ï¼šéœ€è¦æŸ¥æ‰¾æˆ–ç¡®è®¤ç›¸å…³æ–‡ä»¶
- **ç”¨æˆ·åé¦ˆåˆ¤æ–­**ï¼š
  - æ–‡ä»¶ç›¸å…³/ç¡®è®¤ä½¿ç”¨ â†’ `determine_the_mapping_of_headers`ï¼ˆè¿›è¡Œå­—æ®µæ˜ å°„ï¼‰
  - æ–‡ä»¶ä¸ç›¸å…³/éœ€è¦é‡æ–°æŸ¥æ‰¾ â†’ `recall_relative_files`ï¼ˆé‡æ–°å¬å›æ–‡ä»¶ï¼‰

### 4. å½“å‰èŠ‚ç‚¹ä¸ºmodify_generated_tableæ—¶
- **è§¦å‘æ¡ä»¶**ï¼šç”¨æˆ·æƒ³è¦ä¿®æ”¹ç”Ÿæˆçš„è¡¨æ ¼ï¼Œå¹¶æå‡ºç›¸åº”çš„éœ€æ±‚
- **ç”¨æˆ·åé¦ˆåˆ¤æ–­**ï¼š
  - éœ€è¦å¯¹è¡¨æ ¼æ·»åŠ æ–°çš„è¡¨å¤´ â†’ `reconstruct_table_structure`ï¼ˆé‡æ–°ä¿®æ”¹è¡¨æ ¼ç»“æ„ï¼‰
  - éœ€è¦å¯¹è¡¨æ ¼è¿›è¡Œåˆ é™¤æˆ–è€…è¿‡æ»¤ â†’ `filter_generated_table`ï¼ˆé‡æ–°è¿‡æ»¤è¡¨æ ¼ï¼‰

## å½“å‰èŠ‚ç‚¹
{state["current_node"]}

## ğŸ“Š ä¸Šä¸‹æ–‡ä¿¡æ¯
**ä¸Šä¸€è½®AIå›å¤å†…å®¹**ï¼š
{previous_ai_content}

**ç”¨æˆ·å½“å‰è¾“å…¥**ï¼š
{user_input}

## ğŸ¯ åˆ¤æ–­è¦ç‚¹
- **æ„å›¾è¯†åˆ«**ï¼šç”¨æˆ·æ˜¯å¦æ˜ç¡®è¡¨è¾¾äº†è¡¨æ ¼ç”Ÿæˆéœ€æ±‚ï¼Ÿ
- **å†…å®¹åˆ†æ**ï¼šä»…æ–‡ä»¶è·¯å¾„ vs æ˜ç¡®çš„è¡¨æ ¼å¤„ç†æŒ‡ä»¤
- **ä¸Šä¸‹æ–‡ç†è§£**ï¼šç»“åˆå½“å‰èŠ‚ç‚¹çŠ¶æ€åšå‡ºåˆç†åˆ¤æ–­

## ğŸ“ å†³ç­–ç¤ºä¾‹
**å½“å‰èŠ‚ç‚¹ä¸ºinitial_collect_user_inputæ—¶ï¼š**
- ç”¨æˆ·è¾“å…¥ï¼š"å¸®æˆ‘ç”Ÿæˆä¸€ä¸ªå…šå‘˜ä¿¡æ¯è¡¨æ ¼" â†’ `chat_with_user_to_determine_template`
- ç”¨æˆ·è¾“å…¥ï¼š"d:\files\å…šå‘˜ä¿¡æ¯è¡¨.xlsx" â†’ `initial_collect_user_input`
- ç”¨æˆ·è¾“å…¥ï¼š"ä½ å¥½" â†’ `initial_collect_user_input`
- ç”¨æˆ·è¾“å…¥ï¼š"æˆ‘è¦åˆ¶ä½œä¸€ä¸ªç»Ÿè®¡æŠ¥è¡¨" â†’ `chat_with_user_to_determine_template`

## ğŸ“¤ è¾“å‡ºè¦æ±‚
**ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œä¸å¾—åŒ…å«ä»»ä½•è§£é‡Šæˆ–é¢å¤–æ–‡å­—**ï¼š
ä»…è¿”å›èŠ‚ç‚¹åç§°ï¼Œå¦‚ï¼š`design_excel_template`"""
            
            try:
                print("å½“å‰èŠ‚ç‚¹ä¸ºï¼š" + state["current_node"])
                print("ç”¨æˆ·è¾“å…¥ä¸ºï¼š" + user_input)
                print("ç³»ç»Ÿæç¤ºä¸ºï¼š" + system_prompt)
                response = invoke_model(model_name="Pro/deepseek-ai/DeepSeek-V3", messages=[SystemMessage(content=system_prompt), HumanMessage(content=user_input)])
                route_decision = response.strip()
                print(f"ğŸ“ åŸºäºLLMå†³ç­–è·¯ç”±åˆ°: {route_decision}")
            except Exception as e:
                print(f"âŒ LLMè·¯ç”±å†³ç­–å¤±è´¥: {e}")
                route_decision = "design_excel_template"  # é»˜è®¤è·¯ç”±
                print(f"ğŸ“ ä½¿ç”¨é»˜è®¤è·¯ç”±: {route_decision}")
        
        print("âœ… _decide_next_node æ‰§è¡Œå®Œæˆ")
        print("=" * 50)
        
        return {"next_node": route_decision}
    
    def _summary_user_input(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """Summary node that consolidates all information from this round."""
        
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _summary_user_input")
        print("=" * 50)
        
        print(f"ğŸ”„ å¼€å§‹æ€»ç»“ç”¨æˆ·è¾“å…¥ï¼Œå½“å‰æ¶ˆæ¯æ•°: {len(state.get('process_user_input_messages', []))}")
        
        # Extract content from all messages in this processing round
        process_user_input_messages_content = ("\n").join([item.content for item in state["process_user_input_messages"]])
        print(f"ğŸ“ å¤„ç†çš„æ¶ˆæ¯å†…å®¹é•¿åº¦: {len(process_user_input_messages_content)} å­—ç¬¦")
        
        system_prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç”¨æˆ·è¾“å…¥åˆ†æä¸“å®¶ï¼Œä»»åŠ¡æ˜¯æ ¹æ®å½“å‰è½®æ¬¡çš„å†å²å¯¹è¯å†…å®¹ï¼Œæ€»ç»“ç”¨æˆ·åœ¨ä¿¡æ¯æ”¶é›†è¿‡ç¨‹ä¸­çš„æ‰€æœ‰æœ‰æ•ˆè¾“å…¥ã€‚

ã€ä½ çš„ç›®æ ‡ã€‘
- æå–æœ¬è½®å¯¹è¯ä¸­ç”¨æˆ·æä¾›çš„æ‰€æœ‰æœ‰ä»·å€¼ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š
  - æ–‡ä»¶ä¸Šä¼ ï¼ˆå¦‚æ•°æ®æ–‡ä»¶ã€æ¨¡æ¿æ–‡ä»¶ç­‰ï¼‰ï¼›
  - æ–‡æœ¬è¾“å…¥ï¼ˆå¦‚å¡«å†™è¯´æ˜ã€æ”¿ç­–ä¿¡æ¯ã€è®¡ç®—è§„åˆ™ç­‰ï¼‰ï¼›
  - å¯¹å¬å›æ–‡ä»¶çš„åˆ¤æ–­ï¼ˆä¾‹å¦‚ç”¨æˆ·ç¡®è®¤æŸäº›æ–‡ä»¶æ˜¯å¦ç›¸å…³ï¼‰ï¼›
- æ³¨æ„ï¼šæœ‰æ—¶ä½ è¢«ä½œä¸º"ç¡®è®¤èŠ‚ç‚¹"è°ƒç”¨ï¼Œä»»åŠ¡æ˜¯è®©ç”¨æˆ·åˆ¤æ–­æ–‡ä»¶æ˜¯å¦ç›¸å…³ï¼Œæ­¤æ—¶ä½ éœ€è¦æ€»ç»“çš„æ˜¯"ç”¨æˆ·çš„åˆ¤æ–­ç»“æœ"ï¼Œè€Œä¸æ˜¯æ–‡ä»¶æœ¬èº«ã€‚
- è¯·åŸºäºä¸Šä¸‹æ–‡çµæ´»åˆ¤æ–­å“ªäº›å†…å®¹æ„æˆæœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚
- æ€»ç»“ä¸­è¯·ä¸è¦åŒ…å«ç”¨æˆ·ä¸Šä¼ çš„æ— å…³ä¿¡æ¯å†…å®¹ï¼Œä»¥åŠæœ‰æ•ˆæ€§éªŒè¯
- ä½†æ˜¯ä¸€å®šä¸è¦å¿½ç•¥æ›²è§£ç”¨æˆ·çš„æ„å›¾

ã€è¾“å‡ºæ ¼å¼ã€‘
ä»…è¿”å›ä»¥ä¸‹ JSON å¯¹è±¡ï¼Œä¸å¾—åŒ…å«ä»»ä½•é¢å¤–è§£é‡Šæˆ–æ–‡æœ¬,ä¸è¦åŒ…è£¹åœ¨```jsonä¸­ï¼Œç›´æ¥è¿”å›jsonæ ¼å¼å³å¯ï¼š
{{
  "summary": "å¯¹æœ¬è½®ç”¨æˆ·æä¾›çš„ä¿¡æ¯è¿›è¡Œæ€»ç»“"
}}
"""

        try:
            user_input = "ã€å†å²å¯¹è¯ã€‘\n" + process_user_input_messages_content
            print("ğŸ“¤ æ­£åœ¨è°ƒç”¨LLMç”Ÿæˆæ€»ç»“...")
            response = invoke_model(model_name="Pro/deepseek-ai/DeepSeek-V3", messages=[SystemMessage(content=system_prompt), HumanMessage(content=user_input)])
            print(f"ğŸ“¥ LLMæ€»ç»“å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
            
            # Clean the response to handle markdown code blocks and malformed JSON
            cleaned_response = response.strip()
            
            # Remove markdown code blocks if present
            if '```json' in cleaned_response:
                print("ğŸ” æ£€æµ‹åˆ°markdownä»£ç å—ï¼Œæ­£åœ¨æ¸…ç†...")
                # Extract content between ```json and ```
                start_marker = '```json'
                end_marker = '```'
                start_index = cleaned_response.find(start_marker)
                if start_index != -1:
                    start_index += len(start_marker)
                    end_index = cleaned_response.find(end_marker, start_index)
                    if end_index != -1:
                        cleaned_response = cleaned_response[start_index:end_index].strip()
                    else:
                        # If no closing ```, take everything after ```json
                        cleaned_response = cleaned_response[start_index:].strip()
            elif '```' in cleaned_response:
                print("ğŸ” æ£€æµ‹åˆ°é€šç”¨ä»£ç å—ï¼Œæ­£åœ¨æ¸…ç†...")
                # Handle generic ``` blocks
                parts = cleaned_response.split('```')
                if len(parts) >= 3:
                    # Take the middle part (index 1)
                    cleaned_response = parts[1].strip()
            
            # If there are multiple JSON objects, take the first valid one
            if '}{' in cleaned_response:
                print("âš ï¸ æ£€æµ‹åˆ°å¤šä¸ªJSONå¯¹è±¡ï¼Œå–ç¬¬ä¸€ä¸ª")
                cleaned_response = cleaned_response.split('}{')[0] + '}'
            
            print(f"ğŸ” æ¸…ç†åçš„å“åº”: {cleaned_response}")
            
            response_json = json.loads(cleaned_response)
            final_response = json.dumps(response_json, ensure_ascii=False)
            
            print(f"âœ… æ€»ç»“ç”ŸæˆæˆåŠŸ")
            print(f"ğŸ“Š æœ€ç»ˆå“åº”: {final_response}")
            print("âœ… _summary_user_input æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            
            return {"summary_message": final_response}
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æé”™è¯¯: {e}")
            print(f"âŒ åŸå§‹å“åº”: {repr(response)}")
            # Fallback response
            fallback_response = {
                "summary": "ç”¨æˆ·æœ¬è½®æä¾›äº†æ–‡ä»¶ä¿¡æ¯ï¼Œä½†è§£æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯"
            }
            final_fallback = json.dumps(fallback_response, ensure_ascii=False)
            print(f"ğŸ”„ ä½¿ç”¨å¤‡ç”¨å“åº”: {final_fallback}")
            print("âœ… _summary_user_input æ‰§è¡Œå®Œæˆ (å¤‡ç”¨)")
            print("=" * 50)
            return {"summary_message": final_fallback}

    def _combine_summary_and_decide_next_node(self, state: ProcessUserInputState) -> ProcessUserInputState:
        """Combine summary and decide next node results"""
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _combine_summary_and_decide_next_node")
        print("=" * 50)
        
        summary_message = state.get("summary_message", "")
        next_node = state.get("next_node", "")
        
        print(f"ğŸ“ æ€»ç»“æ¶ˆæ¯: {summary_message}")
        print(f"ğŸ”„ ä¸‹ä¸€èŠ‚ç‚¹: {next_node}")
        
        # Parse the summary message to add next_node information
        try:
            if summary_message:
                summary_json = json.loads(summary_message)
                summary_json["next_node"] = next_node
                combined_summary = json.dumps(summary_json, ensure_ascii=False)
            else:
                # If no summary message, create a basic one
                combined_summary = json.dumps({
                    "summary": "ç”¨æˆ·è¾“å…¥å¤„ç†å®Œæˆ",
                    "next_node": next_node
                }, ensure_ascii=False)
        except json.JSONDecodeError as e:
            print(f"âŒ è§£æsummary_messageæ—¶å‡ºé”™: {e}")
            # Fallback to basic structure
            combined_summary = json.dumps({
                "summary": "ç”¨æˆ·è¾“å…¥å¤„ç†å®Œæˆï¼Œä½†è§£æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯",
                "next_node": next_node
            }, ensure_ascii=False)
        
        print(f"ğŸ“Š åˆå¹¶åçš„æ€»ç»“: {combined_summary}")
        print("âœ… _combine_summary_and_decide_next_node æ‰§è¡Œå®Œæˆ")
        print("=" * 50)
        
        return {"summary_message": combined_summary}

    def run_process_user_input_agent(self, session_id: str = "1", previous_AI_messages: BaseMessage = None, 
                                     current_node: str = "", village_name: str = "") -> List:
        """This function runs the process user input agent using invoke method instead of streaming"""
        print("\nğŸš€ å¼€å§‹è¿è¡Œ ProcessUserInputAgent")
        print("=" * 60)
        
        initial_state = self.create_initial_state(session_id=session_id, previous_AI_messages=previous_AI_messages, 
                                                  current_node=current_node, village_name=village_name)
        config = {"configurable": {"thread_id": session_id}}
        
        print(f"ğŸ“‹ ä¼šè¯ID: {session_id}")
        print(f"ğŸ“ åˆå§‹çŠ¶æ€å·²åˆ›å»º")
        print("ğŸ”„ æ­£åœ¨æ‰§è¡Œç”¨æˆ·è¾“å…¥å¤„ç†å·¥ä½œæµ...")
        
        try:
            # Use invoke instead of stream for simpler execution
            while True:
                final_state = self.graph.invoke(initial_state, config=config)
                if "__interrupt__" in final_state:
                    interrupt_value = final_state["__interrupt__"][0].value
                    print(f"ğŸ’¬ æ™ºèƒ½ä½“: {interrupt_value}")
                    user_response = input("ğŸ‘¤ è¯·è¾“å…¥æ‚¨çš„å›å¤: ")
                    initial_state = Command(resume=user_response)
                    continue

                print("ğŸ‰æ‰§è¡Œå®Œæ¯•")
                summary_message = final_state.get("summary_message", "")
                template_file = final_state.get("template_file_path", "")
                print(f"ğŸ” è¿”å›ä¿¡æ¯æµ‹è¯•summary: {summary_message}")
                print(f"ğŸ” è¿”å›ä¿¡æ¯æµ‹è¯•template: {template_file}")
                combined_message = [summary_message, template_file]
                print(f"ğŸ” è¿”å›ä¿¡æ¯æµ‹è¯•combined: {combined_message}")
                return combined_message
            
        except Exception as e:
            print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            # Return empty results on error
            error_summary = json.dumps({
                "summary": f"å¤„ç†ç”¨æˆ·è¾“å…¥æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "next_node": "design_excel_template"
            }, ensure_ascii=False)
            return [error_summary, ""]



# Langgraph studio to export the compiled graph
agent = ProcessUserInputAgent()
graph = agent.graph


if __name__ == "__main__":
    agent = ProcessUserInputAgent()
    # save_graph_visualization(agent.graph, "process_user_input_graph.png")
    agent.run_process_user_input_agent(current_node="design_excel_template")