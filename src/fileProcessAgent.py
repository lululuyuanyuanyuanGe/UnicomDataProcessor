import sys
from pathlib import Path
import os
import stat

# Add root project directory to sys.path
sys.path.append(str(Path(__file__).resolve().parent.parent))

from dotenv import load_dotenv
load_dotenv()


from typing import Dict, TypedDict
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import shutil
from utils.modelRelated import invoke_model4extract_excel_headers
from utils.file_process import (    delete_files_from_staging_area,
                                    detect_and_process_file_paths)
from utils.process_links import download_and_rename_from_dict
from src.similarity_calculation import TableSimilarityCalculator
from utils.table_processing_helpers import (
    extract_headers_from_response, 
    extract_headers_from_txt_content,
    parse_llm_table_response,
    generate_fallback_table_name,
    create_table_description
)

import json
import re

from langgraph.graph import StateGraph, END, START
from langgraph.types import Send


class FileProcessState(TypedDict):
    session_id: str
    upload_files_path: list[dict] # Store all uploaded files with timestamps: [{"path": "file_path", "timestamp": "iso_timestamp", "file_id": "unique_id"}]
    new_upload_files_path: list[dict] # Track the new uploaded files in this round with timestamps and file_id
    new_upload_files_processed_path: list[dict] # Store the processed new uploaded files with timestamps and file_id
    original_files_path: list[dict] # Store the original files in original_file subfolder with timestamps
    table_files_path: list[dict]  # Store table files with timestamps and file_id
    table_headers2embed: list[str]  # Change from str to list[str] to handle multiple tables
    table_header_embeddings: list[float]
    processed_table_results: list[dict]  # Store results from concurrent per-file processing
    replacement_info: dict  # Store info about files being replaced: {file_path: (clean_name, old_file_path)}
    village_name: str


class FileProcessAgent:

    def __init__(self):
        # Thread lock for safe JSON file updates
        self._json_lock = threading.Lock()
        self.graph = self._build_graph().compile()

    def _build_graph(self):
        graph = StateGraph(FileProcessState)

        graph.add_node("file_upload", self._file_upload)
        graph.add_node("process_table_and_similarity", self._process_table_and_similarity)
        graph.add_node("summary_file_upload", self._summary_file_upload)

        graph.add_edge(START, "file_upload")
        graph.add_edge("file_upload", "process_table_and_similarity")
        graph.add_edge("process_table_and_similarity", "summary_file_upload")
        graph.add_edge("summary_file_upload", END)

        return graph

    def _create_initial_state(self, session_id: str = "1", upload_files_data: dict[str, str] = {}, village_name: str = "") -> FileProcessState:
        # Convert input file paths to dictionary format with timestamps
        # upload_files_data format: {file_path: file_id}
        current_timestamp = datetime.now().isoformat()
        upload_files_with_timestamps = [
            {"path": file_path, "timestamp": current_timestamp, "file_id": file_id} 
            for file_path, file_id in upload_files_data.items()
        ]
        
        return {
            "session_id": session_id,
            "upload_files_path": upload_files_with_timestamps,
            "new_upload_files_path": [],
            "new_upload_files_processed_path": [],
            "original_files_path": [],
            "uploaded_template_files_path": [],
            "table_files_path": [],
            "table_headers2embed": [],  # Changed from str to list[str]
            "table_header_embeddings": [],
            "processed_table_results": [],  # New field for concurrent processing results
            "replacement_info": {},  # Store replacement information
            "template_complexity": "",
            "village_name": village_name
        }

    def _file_upload(self, state: FileProcessState) -> FileProcessState:
        """This node will upload user's file to our system with concurrent processing"""
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _file_upload")
        print("=" * 50)
        
        print("ğŸ“ æ­£åœ¨æ£€æµ‹ç”¨æˆ·è¾“å…¥ä¸­çš„æ–‡ä»¶è·¯å¾„...")
        uploaded_files_path = state["upload_files_path"]
        print(f"ğŸ“‹ æ£€æµ‹åˆ° {len(uploaded_files_path)} ä¸ªæ–‡ä»¶")
        
        if not uploaded_files_path:
            print("âš ï¸ æ²¡æœ‰æ–‡ä»¶éœ€è¦ä¸Šä¼ ")
            print("âœ… _file_upload æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            return {
                "new_upload_files_path": [],
                "new_upload_files_processed_path": [],
                "table_files_path": []
            }
        
        # Extract file paths from the dictionary structure
        file_paths = [file_entry["path"] for file_entry in uploaded_files_path]
        
        # Create staging area (temp folder)
        project_root = Path.cwd()
        temp_dir = project_root / "temp"
        temp_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸš€ å¼€å§‹å¹¶å‘å¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶...")
        
        # Process files concurrently for LLM analysis
        processed_files = []
        max_workers = min(len(file_paths), 5)  # Limit concurrent processing
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all file processing tasks
            future_to_file = {
                executor.submit(self._process_single_file_for_llm, file_path): file_path
                for file_path in file_paths
            }
            
            # Collect results as they complete
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    processed_file_path = future.result()
                    if processed_file_path:
                        processed_files.append(processed_file_path)
                        print(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {Path(file_path).name} -> {Path(processed_file_path).name}")
                    else:
                        print(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {Path(file_path).name}")
                except Exception as e:
                    print(f"âŒ å¹¶å‘å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ {file_path}: {e}")
        
        # Create processed files with timestamps and preserve file_id
        current_timestamp = datetime.now().isoformat()
        processed_files_with_timestamps = []
        for file_path in processed_files:
            # Find the corresponding file_id from the original uploaded files
            file_id = None
            for uploaded_file_entry in uploaded_files_path:
                # Match by filename stem since processed files may have different extensions
                uploaded_file_stem = Path(uploaded_file_entry["path"]).stem
                processed_file_stem = Path(file_path).stem
                if uploaded_file_stem == processed_file_stem:
                    file_id = uploaded_file_entry.get("file_id")
                    break
            
            processed_files_with_timestamps.append({
                "path": file_path,
                "timestamp": current_timestamp,
                "file_id": file_id
            })
        
        print(f"ğŸ‰ æ–‡ä»¶ä¸Šä¼ å¤„ç†å®Œæˆ:")
        print(f"  - è¾“å…¥æ–‡ä»¶æ•°: {len(file_paths)}")
        print(f"  - æˆåŠŸå¤„ç†: {len(processed_files)} ä¸ª")
        print(f"  - å¤±è´¥å¤„ç†: {len(file_paths) - len(processed_files)} ä¸ª")
        print("âœ… _file_upload æ‰§è¡Œå®Œæˆ")
        print("=" * 50)

        return {
            "new_upload_files_path": uploaded_files_path,  # Keep original input files 
            "new_upload_files_processed_path": processed_files_with_timestamps,  # LLM-ready processed files
            "table_files_path": processed_files_with_timestamps  # Treat all processed files as table files
        }

    def _process_single_file_for_llm(self, file_path: str) -> str | None:
        """
        Process a single file for LLM analysis capability.
        
        Args:
            file_path: Path to the file to process
            
        Returns:
            str: Path to the processed file in temp folder, or None if failed
        """
        try:
            from utils.file_process import process_file_for_LLM_capability
            return process_file_for_LLM_capability(file_path)
        except Exception as e:
            print(f"âŒ å•æ–‡ä»¶LLMå¤„ç†å¤±è´¥ {file_path}: {e}")
            return None


    
    def _process_table_and_similarity(self, state: FileProcessState) -> FileProcessState:
        """Process all table files concurrently, each going through full pipeline"""
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _process_table_and_similarity (å¹¶å‘å¤„ç†)")
        print("=" * 50)
        
        table_files_with_timestamps = state["table_files_path"]
        
        print(f"ğŸ“Š éœ€è¦å¹¶å‘å¤„ç†çš„è¡¨æ ¼æ–‡ä»¶: {len(table_files_with_timestamps)} ä¸ª")
        
        if not table_files_with_timestamps:
            print("âš ï¸ æ²¡æœ‰è¡¨æ ¼æ–‡ä»¶éœ€è¦å¤„ç†")
            print("âœ… _process_table_and_similarity æ‰§è¡Œå®Œæˆ")
            print("=" * 50)
            return {"processed_table_results": [], "table_headers2embed": []}
        
        # Use ThreadPoolExecutor for concurrent per-file processing
        max_workers = min(len(table_files_with_timestamps), 5)  # Limit to avoid overwhelming resources
        all_results = []
        
        print(f"ğŸš€ å¼€å§‹å¹¶å‘å¤„ç†ï¼Œä½¿ç”¨ {max_workers} ä¸ªå·¥ä½œçº¿ç¨‹")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit pipeline for each file
            future_to_file = {
                executor.submit(self.process_table_pipeline, file_entry, state): file_entry 
                for file_entry in table_files_with_timestamps
            }
            
            # Collect results as they complete
            for future in as_completed(future_to_file):
                file_entry = future_to_file[future]
                try:
                    result = future.result()
                    all_results.append(result)
                    print(f"âœ… å®Œæˆå¤„ç†: {result.get('chinese_table_name', 'Unknown')}")
                except Exception as e:
                    print(f"âŒ å¤„ç†å¤±è´¥ {file_entry['path']}: {e}")
                    # Add failed entry to results
                    all_results.append({
                        "file_path": file_entry.get("path", ""),
                        "chinese_table_name": f"å¤„ç†å¤±è´¥_{Path(file_entry['path']).stem}",
                        "success": False,
                        "error": str(e)
                    })
        
        # Generate summary data
        successful_results = [r for r in all_results if r.get("success", False)]
        table_descriptions = [r["table_description"] for r in successful_results if r.get("table_description")]
        
        print(f"\nğŸ“Š å¹¶å‘å¤„ç†æ€»ç»“:")
        print(f"  - æ€»æ–‡ä»¶æ•°: {len(table_files_with_timestamps)}")
        print(f"  - æˆåŠŸå¤„ç†: {len(successful_results)}")
        print(f"  - å¤±è´¥å¤„ç†: {len(all_results) - len(successful_results)}")
        print(f"  - ç”Ÿæˆè¡¨æ ¼æè¿°: {len(table_descriptions)}")
        
        # Show successful table names
        if successful_results:
            print("âœ… æˆåŠŸå¤„ç†çš„è¡¨æ ¼:")
            for result in successful_results:
                chinese_name = result.get("chinese_table_name", "Unknown")
                header_count = len(result.get("headers", []))
                print(f"  - {chinese_name}: {header_count} ä¸ªè¡¨å¤´")
        
        print("âœ… _process_table_and_similarity æ‰§è¡Œå®Œæˆ")
        print("=" * 50)
        
        return {
            "processed_table_results": all_results,
            "table_headers2embed": table_descriptions  # List of table descriptions
        }


    # DISABLED: Uploaded files should not interact with data.json
    # They should only be stored in src/uploaded_files.json
    # def append_table_data_to_json(self, file_name: str, headers: list[str], full_response: str, village_name: str):
    #     """
    #     [DISABLED] Append table data to data.json file with proper structure
    #     Uploaded files now only use src/uploaded_files.json
    #     """
    #     pass
    
    def load_uploaded_files_json(self) -> Dict:
        """Load existing uploaded_files.json file with thread safety"""
        # Use absolute path to ensure we find the file regardless of working directory
        project_root = Path(__file__).resolve().parent.parent
        uploaded_files_json_path = project_root / "src" / "uploaded_files.json"
        
        with self._json_lock:
            try:
                if uploaded_files_json_path.exists():
                    with open(uploaded_files_json_path, 'r', encoding='utf-8') as f:
                        return json.load(f)
                else:
                    return []
            except (json.JSONDecodeError, FileNotFoundError) as e:
                print(f"âš ï¸ è¯»å–uploaded_files.jsonå¤±è´¥: {e}ï¼Œåˆ›å»ºæ–°çš„æ•°æ®ç»“æ„")
                return []

    def save_uploaded_files_json(self, data: Dict):
        """Save data to uploaded_files.json file with thread safety"""
        # Use absolute path to ensure we find the file regardless of working directory
        project_root = Path(__file__).resolve().parent.parent
        uploaded_files_json_path = project_root / "src" / "uploaded_files.json"
        
        with self._json_lock:
            try:
                with open(uploaded_files_json_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                print("âœ… æˆåŠŸæ›´æ–°uploaded_files.json")
            except Exception as e:
                print(f"âŒ ä¿å­˜uploaded_files.jsonå¤±è´¥: {e}")

    def save_original_file_to_uploads(self, file_path: str, chinese_name: str) -> str:
        """Move original file to uploaded_files/ directory with Chinese name, handling replacements"""
        try:
            source_path = Path(file_path)
            
            # Create uploaded_files directory if it doesn't exist
            uploads_dir = Path("uploaded_files")
            uploads_dir.mkdir(exist_ok=True)
            
            file_extension = source_path.suffix
            
            # ALWAYS use clean name without timestamp - no more timestamped files
            new_filename = f"{chinese_name}{file_extension}"
            dest_path = uploads_dir / new_filename
            
            # Check if file already exists and log replacement
            if dest_path.exists():
                print(f"ğŸ”„ æ›¿æ¢ç°æœ‰æ–‡ä»¶: {dest_path.name}")
            else:
                print(f"ğŸ“ ä¿å­˜æ–°æ–‡ä»¶: {dest_path.name}")
            
            # Copy/replace the file (shutil.copy will overwrite existing files)
            print(f"ğŸ“‹ Source path: {source_path}")
            os.chmod(source_path, stat.S_IWRITE)
            shutil.copy(source_path, dest_path)

            print(f"âœ… æ–‡ä»¶å·²ä¿å­˜åˆ°: {dest_path.name} (æ— æ—¶é—´æˆ³)")
            return str(dest_path)
            
        except Exception as e:
            print(f"âŒ ç§»åŠ¨æ–‡ä»¶åˆ°uploaded_fileså¤±è´¥: {e}")
            return file_path  # Return original path if move fails


    def update_uploaded_files_json(self, chinese_table_name: str, table_data: dict):
        """Update uploaded_files.json with table data (thread-safe)"""
        try:
            # Load existing data
            data = self.load_uploaded_files_json()
            
            # Find the index of the existing table entry
            # Priority: First try to find entry without file_id, then any matching entry
            index = None
            for i, item in enumerate(data):
                if item.get("excel_name") == chinese_table_name:
                    # If we find an entry without file_id, prefer to replace that one
                    if item.get("file_id") is None:
                        index = i
                        break
                    # If we haven't found a better match yet, use this one
                    elif index is None:
                        index = i
            
            if index is not None:
                # Update existing table data
                print(f"ğŸ”„ æ›¿æ¢ç°æœ‰è¡¨æ ¼æ•°æ®: {chinese_table_name} (ç´¢å¼•: {index})")
                data[index] = table_data
            else:
                # Add new table data
                print(f"ğŸ“Š æ·»åŠ æ–°è¡¨æ ¼æ•°æ®: {chinese_table_name}")
                data.append(table_data)
            
            # Save back to file
            self.save_uploaded_files_json(data)
            
            print(f"âœ… å·²æ›´æ–°uploaded_files.json - è¡¨æ ¼: {chinese_table_name}")
            
        except Exception as e:
            print(f"âŒ æ›´æ–°uploaded_files.jsonå¤±è´¥: {e}")

    def process_single_table_file(self, file_entry: dict, state: FileProcessState) -> dict:
        """Process one table file (header extraction and file management)"""
        file_path = file_entry["path"]
        file_timestamp = file_entry["timestamp"]
        file_id = file_entry.get("file_id")
        
        try:
            source_path = Path(file_path)
            print(f"ğŸ” æ­£åœ¨å¤„ç†å•ä¸ªè¡¨æ ¼æ–‡ä»¶: {source_path.name}")
            
            # Find corresponding original Excel file in temp folder
            table_file_stem = source_path.stem
            original_excel_file = None
            
            # Look for original Excel file in temp folder by exact name match
            temp_dir = Path("temp")
            if temp_dir.exists():
                # Search for original Excel file with same stem but different extensions
                for potential_file in temp_dir.glob(f"{table_file_stem}.*"):
                    if potential_file.suffix.lower() in {'.xlsx', '.xls', '.xlsm'}:
                        original_excel_file = potential_file
                        print(f"ğŸ” åœ¨tempæ–‡ä»¶å¤¹æ‰¾åˆ°åŸå§‹Excelæ–‡ä»¶: {original_excel_file.name}")
                        break
            
            # Fallback: check original_files_path from state (legacy)
            if not original_excel_file:
                original_files_with_timestamps = state.get("original_files_path", [])
                for original_file_entry in original_files_with_timestamps:
                    original_file_path = original_file_entry["path"]
                    if Path(original_file_path).stem == table_file_stem:
                        original_excel_file = Path(original_file_path)
                        print(f"ğŸ” åœ¨stateä¸­æ‰¾åˆ°åŸå§‹Excelæ–‡ä»¶: {original_excel_file.name}")
                        break
            
            headers = []
            analysis_response = ""
            
            # Extract headers and table name using LLM
            chinese_table_name = ""
            try:
                if original_excel_file and original_excel_file.exists():
                    print(f"ğŸ” æ‰¾åˆ°åŸå§‹Excelæ–‡ä»¶: {original_excel_file}")
                    # Use screenshot-based analysis to extract headers and table name
                    print("ğŸ“¤ æ­£åœ¨è°ƒç”¨LLMæå–è¡¨æ ¼ä¿¡æ¯...")
                    analysis_response = invoke_model4extract_excel_headers(
                        model_name="deepseek-ai/DeepSeek-V3", 
                        file_path=str(original_excel_file)
                    )
                    print("ğŸ“¥ è¡¨æ ¼ä¿¡æ¯æå–å“åº”æ¥æ”¶æˆåŠŸ")
                    
                    # Parse the LLM response to get table name and headers
                    llm_result = parse_llm_table_response(analysis_response)
                    
                    if llm_result["success"]:
                        # Use LLM provided table name and headers
                        chinese_table_name = llm_result["table_name"] or generate_fallback_table_name(source_path.name)
                        headers = llm_result["headers"]
                        print(f"âœ… LLMæå–æˆåŠŸ - è¡¨æ ¼å: {chinese_table_name}, è¡¨å¤´æ•°: {len(headers)}")
                    else:
                        # Fallback to old method
                        headers = extract_headers_from_response(analysis_response)
                        chinese_table_name = generate_fallback_table_name(source_path.name)
                        print(f"âš ï¸ LLMç»“æ„åŒ–æå–å¤±è´¥ï¼Œä½¿ç”¨åå¤‡æ–¹æ³• - è¡¨æ ¼å: {chinese_table_name}")
                    
                else:
                    print(f"âš ï¸ æœªæ‰¾åˆ°å¯¹åº”çš„åŸå§‹Excelæ–‡ä»¶: {table_file_stem}")
                    # Fallback: try to extract from txt content
                    file_content = source_path.read_text(encoding='utf-8')
                    headers = extract_headers_from_txt_content(file_content, source_path.name)
                    chinese_table_name = generate_fallback_table_name(source_path.name)
                    analysis_response = f"ä»æ–‡æœ¬å†…å®¹æå– - è¡¨æ ¼å: {chinese_table_name}, è¡¨å¤´: {headers}"
                    
            except Exception as llm_error:
                print(f"âŒ è¡¨æ ¼ä¿¡æ¯æå–å¤±è´¥: {llm_error}")
                # Fallback: try to extract from txt content
                try:
                    file_content = source_path.read_text(encoding='utf-8')
                    headers = extract_headers_from_txt_content(file_content, source_path.name)
                    chinese_table_name = generate_fallback_table_name(source_path.name)
                    analysis_response = f"æå–å¤±è´¥å›é€€ - è¡¨æ ¼å: {chinese_table_name}, è¡¨å¤´: {headers}"
                except Exception as e:
                    print(f"âŒ æ‰€æœ‰æå–æ–¹æ³•éƒ½å¤±è´¥: {e}")
                    headers = []
                    chinese_table_name = generate_fallback_table_name(source_path.name)
                    analysis_response = f"å®Œå…¨å¤±è´¥ - è¡¨æ ¼å: {chinese_table_name}, é”™è¯¯: {str(e)}"
            
            # Move original file to uploaded_files directory
            new_file_path = ""
            if original_excel_file:
                # Check if this is a replacement operation
                replacement_info = state.get("replacement_info", {})
                replacement_mode = False
                old_file_path = ""
                
                # Find if this file is being replaced
                for file_key, (clean_name, old_path) in replacement_info.items():
                    if Path(file_key).stem == table_file_stem:
                        original_file_key = file_key
                        replacement_mode = True
                        old_file_path = old_path
                        break
                
                new_file_path = self.save_original_file_to_uploads(
                    str(original_excel_file), 
                    chinese_table_name
                )
            
            # Generate table description for embedding
            table_description = create_table_description(chinese_table_name, headers)
            
            # Return processed file data
            return {
                "file_path": file_path,
                "timestamp": file_timestamp,
                "file_id": file_id,
                "chinese_table_name": chinese_table_name,
                "headers": headers,
                "original_file_path": new_file_path,
                "table_description": table_description,
                "analysis_response": analysis_response,
                "success": True
            }
            
        except Exception as e:
            print(f"âŒ å¤„ç†å•ä¸ªè¡¨æ ¼æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return {
                "file_path": file_path,
                "timestamp": file_entry.get("timestamp", ""),
                "file_id": file_entry.get("file_id"),
                "chinese_table_name": f"å¤„ç†å¤±è´¥_{Path(file_path).stem}",
                "headers": [],
                "original_file_path": "",
                "table_description": "",
                "analysis_response": f"å¤„ç†å¤±è´¥: {str(e)}",
                "success": False
            }

    def select_similarity_for_single_table(self, table_data: dict) -> dict:
        """Find similarity matches for a single processed table"""
        try:
            table_description = table_data["table_description"]
            chinese_table_name = table_data["chinese_table_name"]
            
            print(f"ğŸ” æ­£åœ¨è®¡ç®—ç›¸ä¼¼åº¦: {chinese_table_name}")
            
            if not table_description:
                print("âš ï¸ æ²¡æœ‰è¡¨æ ¼æè¿°ï¼Œè·³è¿‡ç›¸ä¼¼åº¦è®¡ç®—")
                table_data["similarity_match"] = {
                    "top_matches": [],
                    "best_match": None,
                    "similarity_scores": []
                }
                return table_data
            
            # Initialize similarity calculator
            calculator = TableSimilarityCalculator()
            
            # Find best matching tables
            results = calculator.get_best_matches(table_description, top_n=5)
            
            if results['success']:
                print(f"âœ… ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ: {chinese_table_name}")
                table_data["similarity_match"] = {
                    "top_matches": results.get('matches', []),
                    "best_match": results.get('top_match'),
                }
            else:
                print(f"âŒ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {results.get('error', 'æœªçŸ¥é”™è¯¯')}")
                table_data["similarity_match"] = {
                    "top_matches": [],
                    "best_match": None,
                    "error": results.get('error', 'ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥')
                }
                
            return table_data
            
        except Exception as e:
            print(f"âŒ ç›¸ä¼¼åº¦è®¡ç®—å¼‚å¸¸: {e}")
            table_data["similarity_match"] = {
                "top_matches": [],
                "best_match": None,
                "error": f"ç›¸ä¼¼åº¦è®¡ç®—å¼‚å¸¸: {str(e)}"
            }
            return table_data

    def process_table_pipeline(self, file_entry: dict, state: FileProcessState) -> dict:
        """Complete pipeline for one table file: process â†’ similarity â†’ save"""
        try:
            print(f"\nğŸš€ å¼€å§‹å¤„ç†è¡¨æ ¼ç®¡é“: {Path(file_entry['path']).name}")
            # Step 1: Process table file (header extraction)
            table_data = self.process_single_table_file(file_entry, state)
            
            if not table_data["success"]:
                print(f"âŒ è¡¨æ ¼å¤„ç†å¤±è´¥ï¼Œè·³è¿‡ç›¸ä¼¼åº¦è®¡ç®—")
                return table_data
            
            # Step 2: Calculate similarity
            complete_data = self.select_similarity_for_single_table(table_data)
            
            # Step 3: Save to uploaded_files.json
            chinese_table_name = complete_data["chinese_table_name"]
            
            # Prepare data structure for JSON
            json_entry = {
                "excel_name": chinese_table_name,
                "timestamp": complete_data["timestamp"],
                "file_id": complete_data.get("file_id"),
                "headers": complete_data["headers"],
                "original_file_path": complete_data["original_file_path"],
                "table_description": complete_data["table_description"],
                "similarity_match": complete_data["similarity_match"]
            }
            
            # Save to JSON
            self.update_uploaded_files_json(chinese_table_name, json_entry)
            
            print(f"âœ… å®Œæ•´ç®¡é“å¤„ç†å®Œæˆ: {chinese_table_name}")
            return complete_data
            
        except Exception as e:
            print(f"âŒ è¡¨æ ¼ç®¡é“å¤„ç†å¤±è´¥: {e}")
            return {
                "file_path": file_entry.get("path", ""),
                "timestamp": file_entry.get("timestamp", ""),
                "file_id": file_entry.get("file_id"),
                "chinese_table_name": f"ç®¡é“å¤±è´¥_{datetime.now().strftime('%H%M%S')}",
                "headers": [],
                "original_file_path": "",
                "table_description": "",
                "analysis_response": f"ç®¡é“å¤„ç†å¤±è´¥: {str(e)}",
                "similarity_match": {"error": str(e)},
                "success": False
            }

    
    def _summary_file_upload(self, state: FileProcessState) -> FileProcessState:
        """Summary node for file upload process"""
        
        
        print("\nğŸ” å¼€å§‹æ‰§è¡Œ: _summary_file_upload")
        print("=" * 50)
        
        # Log the final state summary
        print("ğŸ“Š æ–‡ä»¶å¤„ç†æ€»ç»“:")
        print(f"  - ä¸Šä¼ æ–‡ä»¶æ€»æ•°: {len(state.get('upload_files_path', []))}")
        print(f"  - æ–°ä¸Šä¼ æ–‡ä»¶æ•°: {len(state.get('new_upload_files_path', []))}")
        print(f"  - è¡¨æ ¼æ–‡ä»¶æ•°: {len(state.get('table_files_path', []))}")
        
        # Show concurrent processing results
        processed_results = state.get('processed_table_results', [])
        if processed_results:
            successful_results = [r for r in processed_results if r.get("success", False)]
            print(f"  - å¹¶å‘å¤„ç†è¡¨æ ¼æ•°: {len(processed_results)}")
            print(f"  - æˆåŠŸå¤„ç†: {len(successful_results)}")
            print(f"  - å¤±è´¥å¤„ç†: {len(processed_results) - len(successful_results)}")
            
            if successful_results:
                total_headers = sum(len(r.get("headers", [])) for r in successful_results)
                print(f"  - æ€»è¡¨å¤´æ•°: {total_headers}")
                
                print("ğŸ“‹ å¤„ç†æˆåŠŸçš„è¡¨æ ¼:")
                for result in successful_results:
                    chinese_name = result.get("chinese_table_name", "Unknown")
                    header_count = len(result.get("headers", []))
                    has_similarity = bool(result.get("similarity_match", {}).get("best_match"))
                    similarity_status = "âœ… æœ‰ç›¸ä¼¼åŒ¹é…" if has_similarity else "âš ï¸ æ— ç›¸ä¼¼åŒ¹é…"
                    print(f"    - {chinese_name}: {header_count} ä¸ªè¡¨å¤´, {similarity_status}")
        
        # Show table descriptions for embedding
        table_descriptions = state.get('table_headers2embed', [])
        if table_descriptions:
            print(f"  - ç”ŸæˆåµŒå…¥æè¿°æ•°: {len(table_descriptions)}")
            
        print("\nğŸ“ æ–‡ä»¶å­˜å‚¨ä¿¡æ¯:")
        print("  - åŸå§‹æ–‡ä»¶ä½ç½®: uploaded_files/ ç›®å½•")
        print("  - å¤„ç†ç»“æœå­˜å‚¨: src/uploaded_files.json")
        
        # Cleanup temp folder
        print("\nğŸ§¹ æ­£åœ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        temp_dir = Path("temp")
        if temp_dir.exists():
            try:
                # Remove all files and subdirectories in temp folder
                shutil.rmtree(str(temp_dir), ignore_errors=True)
                temp_dir.mkdir(parents=True, exist_ok=True)  # Recreate empty temp folder
                print("âœ… ä¸´æ—¶æ–‡ä»¶å¤¹å·²æ¸…ç†å®Œæˆ")
            except Exception as e:
                print(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤¹å¤±è´¥: {e}")
        else:
            print("âš ï¸ ä¸´æ—¶æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œè·³è¿‡æ¸…ç†")
        
        print("âœ… _summary_file_upload æ‰§è¡Œå®Œæˆ")
        print("=" * 50)
        
        return {**state}

    def run_file_process_agent(self, session_id: str = "1", upload_files_data: dict[str, str] = {}, village_name: str = "ChatBI") -> FileProcessState:
        """Driver to run the process file agent"""
        print("\nğŸš€ å¼€å§‹è¿è¡Œ FileProcessAgent")
        print("=" * 60)
        renamed_files = download_and_rename_from_dict(upload_files_data)
        initial_state = self._create_initial_state(session_id = session_id, upload_files_data = renamed_files, village_name = village_name)
        config = {"configurable": {"thread_id": session_id}}

        print(f"ğŸ“‹ ä¼šè¯ID: {session_id}")
        print(f"ğŸ“ åˆå§‹çŠ¶æ€å·²åˆ›å»º")
        print("ğŸ”„ æ­£åœ¨æ‰§è¡Œæ–‡ä»¶å¤„ç†å·¥ä½œæµ...")

        try:
            final_state = self.graph.invoke(initial_state, config=config)

            print("\nğŸ‰ FileProcessAgent æ‰§è¡Œå®Œæˆï¼")
            print("=" * 60)
            print("ğŸ“Š æœ€ç»ˆç»“æœ:")
            print(f"- ä¸Šä¼ æ–‡ä»¶æ•°é‡: {len(final_state.get('upload_files_path', []))}")
            print(f"- æ–°ä¸Šä¼ æ–‡ä»¶æ•°é‡: {len(final_state.get('new_upload_files_path', []))}")
            print(f"- æ–°ä¸Šä¼ æ–‡ä»¶å·²å¤„ç†æ•°é‡: {len(final_state.get('new_upload_files_processed_path', []))}")
            print(f"- åŸå§‹æ–‡ä»¶æ•°é‡: {len(final_state.get('original_files_path', []))}")
            print(f"- è¡¨æ ¼æ–‡ä»¶æ•°é‡: {len(final_state.get('table_files_path', []))}")

            return final_state
        
        except Exception as e:
            print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return initial_state
if __name__ == "__main__":
    # upload_files_path = input("è¯·è¾“å…¥ä¸Šä¼ æ–‡ä»¶è·¯å¾„: ")
    # upload_files_path_list = detect_and_process_file_paths(upload_files_path)
    # # Convert list to dict format with dummy file_ids for CLI usage
    # upload_files_data = {path: f"file_{i}" for i, path in enumerate(upload_files_path_list)}
    agent = FileProcessAgent()
    agent.run_file_process_agent(upload_files_data = {"http://58.144.196.118:5019/ai-index/atts/images/QA_20250805124450569_å…šå‘˜ä¿¡æ¯.xlsx": "file_001"})
